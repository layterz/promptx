{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-30 08:26:37.988\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpromptx\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mloading local app from /home/rjl/promptx/scratch/arxiv-reader\u001b[0m\n",
      "\u001b[32m2023-10-30 08:26:37.990\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpromptx\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mloaded environment variables from /home/rjl/promptx/scratch/arxiv-reader/.env\u001b[0m\n",
      "\u001b[32m2023-10-30 08:26:37.991\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpromptx\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mAPI KEY CQZm7\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mApp\u001b[0m\u001b[39m local \u001b[0m\u001b[33mpath\u001b[0m\u001b[39m=\u001b[0m\u001b[35m/home/rjl/promptx/scratch/\u001b[0m\u001b[95marxiv-reader\u001b[0m\u001b[1m>\u001b[0m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", None) \n",
    "\n",
    "from promptx import load\n",
    "\n",
    "load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import requests\n",
    "import uuid\n",
    "\n",
    "def load_pdf(filepath_or_url):\n",
    "    \"\"\"\n",
    "    Load content of a PDF from either a file path or a remote URL.\n",
    "    \n",
    "    :param filepath_or_url: File path or URL to fetch the PDF from.\n",
    "    :return: Content of the PDF as a string.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Handle remote URL\n",
    "    if filepath_or_url.startswith((\"http://\", \"https://\")):\n",
    "        response = requests.get(filepath_or_url)\n",
    "        response.raise_for_status()\n",
    "        id = str(uuid.uuid4())\n",
    "        filepath_or_url = f'./data/{id}.pdf'\n",
    "        with open(filepath_or_url, 'wb') as pdf:\n",
    "            pdf.write(response.content)\n",
    "    \n",
    "    with open(filepath_or_url, 'rb') as f:\n",
    "        pdf_reader = PyPDF2.PdfReader(f)\n",
    "        text_content = ''.join([page.extract_text() for page in pdf_reader.pages])\n",
    "    return text_content\n",
    "\n",
    "\n",
    "def batch(generator, bs=1, limit=None):\n",
    "    b = []\n",
    "    i = 0\n",
    "    for item in generator:\n",
    "        if limit and i > limit:\n",
    "            break\n",
    "        b.append(item)\n",
    "        if len(b) == bs:\n",
    "            yield b\n",
    "            b = []\n",
    "        i += bs\n",
    "    if b and (limit and i <= limit):  # Yield any remaining items in the batch\n",
    "        yield b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "from pydantic import Field\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from promptx.collection import Entity\n",
    "\n",
    "\n",
    "class Document(Entity):\n",
    "    title: str\n",
    "    abstract: str\n",
    "    url: str\n",
    "    body: str = Field(None, embed=False)\n",
    "\n",
    "\n",
    "def get_arxiv_urls():\n",
    "    response = requests.get('https://arxiv.org/list/cs.AI/recent')\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    urls = [f\"https://arxiv.org{a.attrs['href']}\" for a in soup.find_all('a', title='Abstract')]\n",
    "    return urls\n",
    "\n",
    "\n",
    "def extract_whitepaper_from_arxiv(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    title = soup.find('h1', class_='title').text.replace('Title:', '')\n",
    "    abstract = soup.find('blockquote', class_='abstract').text.replace('Abstract:', '')\n",
    "    url = soup.find('a', class_='download-pdf').attrs['href']\n",
    "    url = f\"https://arxiv.org{url}\"\n",
    "\n",
    "    return Document(\n",
    "        title=title,\n",
    "        abstract=abstract,\n",
    "        url=url,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pprint import pprint\n",
    "\n",
    "try:\n",
    "    urls = get_arxiv_urls()\n",
    "    url = random.choice(urls)\n",
    "    paper = extract_whitepaper_from_arxiv(url)\n",
    "except Exception as e:\n",
    "    print(f'Error loading {url}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Learning to Search Feasible and Infeasible Regions of Routing Problems with Flexible Neural k-Opt</td>\n",
       "      <td>\\n  In this paper, we present Neural k-Opt (NeuOpt), a novel learning-to-search\\n(L2S) solver for routing problems. It learns to perform flexible k-opt\\nexchanges based on a tailored action factorization method and a customized\\nrecurrent dual-stream decoder. As a pioneering work to circumvent the pure\\nfeasibility masking scheme and enable the autonomous exploration of both\\nfeasible and infeasible regions, we then propose the Guided Infeasible Region\\nExploration (GIRE) scheme, which supplements the NeuOpt policy network with\\nfeasibility-related features and leverages reward shaping to steer\\nreinforcement learning more effectively. Additionally, we equip NeuOpt with\\nDynamic Data Augmentation (D2A) for more diverse searches during inference.\\nExtensive experiments on the Traveling Salesman Problem (TSP) and Capacitated\\nVehicle Routing Problem (CVRP) demonstrate that our NeuOpt not only\\nsignificantly outstrips existing (masking-based) L2S solvers, but also\\nshowcases superiority over the learning-to-construct (L2C) and\\nlearning-to-predict (L2P) solvers. Notably, we offer fresh perspectives on how\\nneural solvers can handle VRP constraints. Our code is available:\\nthis https URL.\\n\\n</td>\n",
       "      <td>https://arxiv.org/pdf/2310.18264.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "\n",
       "                                                                                               title  \\\n",
       "\u001b[1;36m0\u001b[0m  Learning to Search Feasible and Infeasible Regions of Routing Problems with Flexible Neural k-Opt   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      abstract  \\\n",
       "\u001b[1;36m0\u001b[0m  \\n  In this paper, we present Neural k-Opt \u001b[1m(\u001b[0mNeuOpt\u001b[1m)\u001b[0m, a novel learning-to-search\\\u001b[1;35mn\u001b[0m\u001b[1m(\u001b[0mL2S\u001b[1m)\u001b[0m solver for routing problems. It learns to perform flexible k-opt\\nexchanges based on a tailored action factorization method and a customized\\nrecurrent dual-stream decoder. As a pioneering work to circumvent the pure\\nfeasibility masking scheme and enable the autonomous exploration of both\\nfeasible and infeasible regions, we then propose the Guided Infeasible Region\\nExploration \u001b[1m(\u001b[0mGIRE\u001b[1m)\u001b[0m scheme, which supplements the NeuOpt policy network with\\nfeasibility-related features and leverages reward shaping to steer\\nreinforcement learning more effectively. Additionally, we equip NeuOpt with\\nDynamic Data Augmentation \u001b[1m(\u001b[0mD2A\u001b[1m)\u001b[0m for more diverse searches during inference.\\nExtensive experiments on the Traveling Salesman Problem \u001b[1m(\u001b[0mTSP\u001b[1m)\u001b[0m and Capacitated\\nVehicle Routing Problem \u001b[1m(\u001b[0mCVRP\u001b[1m)\u001b[0m demonstrate that our NeuOpt not only\\nsignificantly outstrips existing \u001b[1m(\u001b[0mmasking-based\u001b[1m)\u001b[0m L2S solvers, but also\\nshowcases superiority over the learning-to-construct \u001b[1m(\u001b[0mL2C\u001b[1m)\u001b[0m and\\nlearning-to-predict \u001b[1m(\u001b[0mL2P\u001b[1m)\u001b[0m solvers. Notably, we offer fresh perspectives on how\\nneural solvers can handle VRP constraints. Our code is available:\\nthis https URL.\\n\\n\n",
       "\n",
       "                                    url  \n",
       "\u001b[1;36m0\u001b[0m  \u001b[4;94mhttps://arxiv.org/pdf/2310.18264.pdf\u001b[0m  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from promptx import store, query\n",
    "\n",
    "collection_name = 'arxiv'\n",
    "store(paper, collection=collection_name)\n",
    "query(collection=collection_name)[['title', 'abstract', 'url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m'f3eb72a4-d4ed-427f-bdcd-32b4461eb2dc'\u001b[0m,\n",
       "    \u001b[33mtype\u001b[0m=\u001b[32m'document'\u001b[0m,\n",
       "    \u001b[33mtitle\u001b[0m=\u001b[32m'Learning to Search Feasible and Infeasible Regions of Routing Problems with Flexible Neural k-Opt'\u001b[0m,\n",
       "    \u001b[33mabstract\u001b[0m=\u001b[32m'\\n  In this paper, we present Neural k-Opt \u001b[0m\u001b[32m(\u001b[0m\u001b[32mNeuOpt\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, a novel learning-to-search\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mL2S\u001b[0m\u001b[32m)\u001b[0m\u001b[32m solver for routing problems. It learns to perform flexible k-opt\\nexchanges based on a tailored action factorization method and a customized\\nrecurrent dual-stream decoder. As a pioneering work to circumvent the pure\\nfeasibility masking scheme and enable the autonomous exploration of both\\nfeasible and infeasible regions, we then propose the Guided Infeasible Region\\nExploration \u001b[0m\u001b[32m(\u001b[0m\u001b[32mGIRE\u001b[0m\u001b[32m)\u001b[0m\u001b[32m scheme, which supplements the NeuOpt policy network with\\nfeasibility-related features and leverages reward shaping to steer\\nreinforcement learning more effectively. Additionally, we equip NeuOpt with\\nDynamic Data Augmentation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mD2A\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for more diverse searches during inference.\\nExtensive experiments on the Traveling Salesman Problem \u001b[0m\u001b[32m(\u001b[0m\u001b[32mTSP\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and Capacitated\\nVehicle Routing Problem \u001b[0m\u001b[32m(\u001b[0m\u001b[32mCVRP\u001b[0m\u001b[32m)\u001b[0m\u001b[32m demonstrate that our NeuOpt not only\\nsignificantly outstrips existing \u001b[0m\u001b[32m(\u001b[0m\u001b[32mmasking-based\u001b[0m\u001b[32m)\u001b[0m\u001b[32m L2S solvers, but also\\nshowcases superiority over the learning-to-construct \u001b[0m\u001b[32m(\u001b[0m\u001b[32mL2C\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and\\nlearning-to-predict \u001b[0m\u001b[32m(\u001b[0m\u001b[32mL2P\u001b[0m\u001b[32m)\u001b[0m\u001b[32m solvers. Notably, we offer fresh perspectives on how\\nneural solvers can handle VRP constraints. Our code is available:\\nthis https URL.\\n\\n    '\u001b[0m,\n",
       "    \u001b[33murl\u001b[0m=\u001b[32m'https://arxiv.org/pdf/2310.18264.pdf'\u001b[0m,\n",
       "    \u001b[33mbody\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pdf with 96105 characters\n"
     ]
    }
   ],
   "source": [
    "pdf = load_pdf(paper.url)\n",
    "print(f'Loaded pdf with {len(pdf)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quote(Entity):\n",
    "    text: str\n",
    "    source: Document\n",
    "    start: int\n",
    "    end: int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes = []\n",
    "for chunk in batch(doc.sents, bs=10, limit=1000):\n",
    "    quotes += [\n",
    "        Quote(\n",
    "            text=sentence.text,\n",
    "            source=paper,\n",
    "            start=sentence.start_char,\n",
    "            end=sentence.end_char,\n",
    "        ) \n",
    "        for sentence in chunk\n",
    "    ]\n",
    "\n",
    "quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for chunk in batch(doc.sents, bs=10, limit=1000):\n",
    "    store(\n",
    "        *[\n",
    "            Quote(\n",
    "                text=sentence.text,\n",
    "                source=paper,\n",
    "                start=sentence.start_char,\n",
    "                end=sentence.end_char,\n",
    "            ) \n",
    "            for sentence in chunk\n",
    "        ], \n",
    "        collection=collection_name\n",
    "    )\n",
    "\n",
    "query(collection=collection_name).query('type == \"quote\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv-reader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
