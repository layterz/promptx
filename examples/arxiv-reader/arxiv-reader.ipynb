{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-30 09:18:16.766\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpromptx\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mloading local app from /home/rjl/promptx/examples/arxiv-reader\u001b[0m\n",
      "\u001b[32m2023-10-30 09:18:16.767\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpromptx\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mloaded environment variables from /home/rjl/promptx/examples/arxiv-reader/.env\u001b[0m\n",
      "\u001b[32m2023-10-30 09:18:16.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpromptx\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mAPI KEY CQZm7\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mApp\u001b[0m\u001b[39m local \u001b[0m\u001b[33mpath\u001b[0m\u001b[39m=\u001b[0m\u001b[35m/home/rjl/promptx/examples/\u001b[0m\u001b[95marxiv-reader\u001b[0m\u001b[1m>\u001b[0m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", None) \n",
    "\n",
    "from promptx import load\n",
    "\n",
    "load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import requests\n",
    "import uuid\n",
    "\n",
    "def load_pdf(filepath_or_url):\n",
    "    \"\"\"\n",
    "    Load content of a PDF from either a file path or a remote URL.\n",
    "    \n",
    "    :param filepath_or_url: File path or URL to fetch the PDF from.\n",
    "    :return: Content of the PDF as a string.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Handle remote URL\n",
    "    if filepath_or_url.startswith((\"http://\", \"https://\")):\n",
    "        response = requests.get(filepath_or_url)\n",
    "        response.raise_for_status()\n",
    "        id = str(uuid.uuid4())\n",
    "        filepath_or_url = f'./data/{id}.pdf'\n",
    "        with open(filepath_or_url, 'wb') as pdf:\n",
    "            pdf.write(response.content)\n",
    "    \n",
    "    with open(filepath_or_url, 'rb') as f:\n",
    "        pdf_reader = PyPDF2.PdfReader(f)\n",
    "        text_content = ''.join([page.extract_text() for page in pdf_reader.pages])\n",
    "    return text_content\n",
    "\n",
    "\n",
    "def batch(generator, bs=1, limit=None):\n",
    "    b = []\n",
    "    i = 0\n",
    "    for item in generator:\n",
    "        if limit and i > limit:\n",
    "            break\n",
    "        b.append(item)\n",
    "        if len(b) == bs:\n",
    "            yield b\n",
    "            b = []\n",
    "        i += bs\n",
    "    if b and (limit and i <= limit):  # Yield any remaining items in the batch\n",
    "        yield b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "from pydantic import Field\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from promptx.collection import Entity\n",
    "\n",
    "\n",
    "class Document(Entity):\n",
    "    title: str\n",
    "    abstract: str\n",
    "    url: str\n",
    "    body: str = Field(None, embed=False)\n",
    "\n",
    "\n",
    "def get_arxiv_urls():\n",
    "    response = requests.get('https://arxiv.org/list/cs.AI/recent')\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    urls = [f\"https://arxiv.org{a.attrs['href']}\" for a in soup.find_all('a', title='Abstract')]\n",
    "    return urls\n",
    "\n",
    "\n",
    "def extract_whitepaper_from_arxiv(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    title = soup.find('h1', class_='title').text.replace('Title:', '')\n",
    "    abstract = soup.find('blockquote', class_='abstract').text.replace('Abstract:', '')\n",
    "    url = soup.find('a', class_='download-pdf').attrs['href']\n",
    "    url = f\"https://arxiv.org{url}\"\n",
    "\n",
    "    return Document(\n",
    "        title=title,\n",
    "        abstract=abstract,\n",
    "        url=url,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pprint import pprint\n",
    "\n",
    "try:\n",
    "    urls = get_arxiv_urls()\n",
    "    url = random.choice(urls)\n",
    "    paper = extract_whitepaper_from_arxiv(url)\n",
    "except Exception as e:\n",
    "    print(f'Error loading {url}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Learning to Search Feasible and Infeasible Regions of Routing Problems with Flexible Neural k-Opt</td>\n",
       "      <td>\\n  In this paper, we present Neural k-Opt (NeuOpt), a novel learning-to-search\\n(L2S) solver for routing problems. It learns to perform flexible k-opt\\nexchanges based on a tailored action factorization method and a customized\\nrecurrent dual-stream decoder. As a pioneering work to circumvent the pure\\nfeasibility masking scheme and enable the autonomous exploration of both\\nfeasible and infeasible regions, we then propose the Guided Infeasible Region\\nExploration (GIRE) scheme, which supplements the NeuOpt policy network with\\nfeasibility-related features and leverages reward shaping to steer\\nreinforcement learning more effectively. Additionally, we equip NeuOpt with\\nDynamic Data Augmentation (D2A) for more diverse searches during inference.\\nExtensive experiments on the Traveling Salesman Problem (TSP) and Capacitated\\nVehicle Routing Problem (CVRP) demonstrate that our NeuOpt not only\\nsignificantly outstrips existing (masking-based) L2S solvers, but also\\nshowcases superiority over the learning-to-construct (L2C) and\\nlearning-to-predict (L2P) solvers. Notably, we offer fresh perspectives on how\\nneural solvers can handle VRP constraints. Our code is available:\\nthis https URL.\\n\\n</td>\n",
       "      <td>https://arxiv.org/pdf/2310.18264.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Image Clustering Conditioned on Text Criteria</td>\n",
       "      <td>\\nClassical clustering methods do not provide users with direct control of the clustering results, and the clustering results may not be consistent with the relevant criterion that a user has in mind. In this work, we present a new methodology for performing image clustering based on user-specified text criteria by leveraging modern vision-language models and large language models. We call our method Image Clustering Conditioned on Text Criteria (IC$|$TC), and it represents a different paradigm of image clustering. IC$|$TC requires a minimal and practical degree of human intervention and grants the user significant control over the clustering results in return. Our experiments show that IC$|$TC can effectively cluster images with various criteria, such as human action, physical location, or the person's mood, while significantly outperforming baselines.\\n</td>\n",
       "      <td>https://arxiv.org/pdf/2310.18297.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Innovation-to-Occupations Ontology: Linking Business Transformation Initiatives to Occupations and Skills</td>\n",
       "      <td>\\n  The fast adoption of new technologies forces companies to continuously adapt\\ntheir operations making it harder to predict workforce requirements. Several\\nrecent studies have attempted to predict the emergence of new roles and skills\\nin the labour market from online job ads. This paper aims to present a novel\\nontology linking business transformation initiatives to occupations and an\\napproach to automatically populating it by leveraging embeddings extracted from\\njob ads and Wikipedia pages on business transformation and emerging\\ntechnologies topics. To our knowledge, no previous research explicitly links\\nbusiness transformation initiatives, like the adoption of new technologies or\\nthe entry into new markets, to the roles needed. Our approach successfully\\nmatches occupations to transformation initiatives under ten different\\nscenarios, five linked to technology adoption and five related to business.\\nThis framework presents an innovative approach to guide enterprises and\\neducational institutions on the workforce requirements for specific business\\ntransformation initiatives.\\n\\n</td>\n",
       "      <td>https://arxiv.org/pdf/2310.17909.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is Scaling Learned Optimizers Worth It? Evaluating The Value of VeLO's 4000 TPU Months</td>\n",
       "      <td>\\n  We analyze VeLO (versatile learned optimizer), the largest scale attempt to\\ntrain a general purpose \"foundational\" optimizer to date. VeLO was trained on\\nthousands of machine learning tasks using over 4000 TPU months with the goal of\\nproducing an optimizer capable of generalizing to new problems while being\\nhyperparameter free, and outperforming industry standards such as Adam. We\\nindependently evaluate VeLO on the MLCommons optimizer benchmark suite. We find\\nthat, contrary to initial claims: (1) VeLO has a critical hyperparameter that\\nneeds problem-specific tuning, (2) VeLO does not necessarily outperform\\ncompetitors in quality of solution found, and (3) VeLO is not faster than\\ncompeting optimizers at reducing the training loss. These observations call\\ninto question VeLO's generality and the value of the investment in training it.\\n\\n</td>\n",
       "      <td>https://arxiv.org/pdf/2310.18191.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Moments for Perceptive Narration Analysis Through the Emotional Attachment of Audience to Discourse and Story</td>\n",
       "      <td>\\nIn this work, our goal is to develop a theoretical framework that can eventually be used for analyzing the effectiveness of visual stories such as feature films to comic books. To develop this theoretical framework, we introduce a new story element called moments. Our conjecture is that any linear story such as the story of a feature film can be decomposed into a set of moments that follow each other. Moments are defined as the perception of the actions, interactions, and expressions of all characters or a single character during a given time period. We categorize the moments into two major types: story moments and discourse moments. Each type of moment can further be classified into three types, which we call universal storytelling moments. We believe these universal moments foster or deteriorate the emotional attachment of the audience to a particular character or the story. We present a methodology to catalog the occurrences of these universal moments as they are found in the story. The cataloged moments can be represented using curves or color strips. Therefore, we can visualize a character's journey through the story as either a 3D curve or a color strip. We also demonstrated that both story and discourse moments can be transformed into one lump-sum attraction parameter. The attraction parameter in time provides a function that can be plotted graphically onto a timeline illustrating changes in the emotional attachment of audience to a character or the story. By inspecting these functions the story analyst can analytically decipher the moments in the story where the attachment is being established, maintained, strengthened, or conversely where it is languishing.\\n</td>\n",
       "      <td>https://arxiv.org/pdf/2310.18273.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "\n",
       "                                                                                                             title  \\\n",
       "\u001b[1;36m0\u001b[0m                Learning to Search Feasible and Infeasible Regions of Routing Problems with Flexible Neural k-Opt \n",
       "\u001b[1;36m1\u001b[0m                                                                    Image Clustering Conditioned on Text Criteria \n",
       "\u001b[1;36m2\u001b[0m    The Innovation-to-Occupations Ontology: Linking Business Transformation Initiatives to Occupations and Skills \n",
       "\u001b[1;36m3\u001b[0m                           Is Scaling Learned Optimizers Worth It? Evaluating The Value of VeLO's \u001b[1;36m4000\u001b[0m TPU Months \n",
       "\u001b[1;36m4\u001b[0m    Moments for Perceptive Narration Analysis Through the Emotional Attachment of Audience to Discourse and Story \n",
       "..                                                                                                             \u001b[33m...\u001b[0m \n",
       "\u001b[1;36m423\u001b[0m                                                                                                            NaN \n",
       "\u001b[1;36m424\u001b[0m                                                                                                            NaN \n",
       "\u001b[1;36m425\u001b[0m                                                                                                            NaN \n",
       "\u001b[1;36m426\u001b[0m                                                                                                            NaN \n",
       "\u001b[1;36m427\u001b[0m                                                                                                            NaN \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   abstract  \\\n",
       "\u001b[1;36m0\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \\n  In this paper, we present Neural k-Opt \u001b[1m(\u001b[0mNeuOpt\u001b[1m)\u001b[0m, a novel learning-to-search\\\u001b[1;35mn\u001b[0m\u001b[1m(\u001b[0mL2S\u001b[1m)\u001b[0m solver for routing problems. It learns to perform flexible k-opt\\nexchanges based on a tailored action factorization method and a customized\\nrecurrent dual-stream decoder. As a pioneering work to circumvent the pure\\nfeasibility masking scheme and enable the autonomous exploration of both\\nfeasible and infeasible regions, we then propose the Guided Infeasible Region\\nExploration \u001b[1m(\u001b[0mGIRE\u001b[1m)\u001b[0m scheme, which supplements the NeuOpt policy network with\\nfeasibility-related features and leverages reward shaping to steer\\nreinforcement learning more effectively. Additionally, we equip NeuOpt with\\nDynamic Data Augmentation \u001b[1m(\u001b[0mD2A\u001b[1m)\u001b[0m for more diverse searches during inference.\\nExtensive experiments on the Traveling Salesman Problem \u001b[1m(\u001b[0mTSP\u001b[1m)\u001b[0m and Capacitated\\nVehicle Routing Problem \u001b[1m(\u001b[0mCVRP\u001b[1m)\u001b[0m demonstrate that our NeuOpt not only\\nsignificantly outstrips existing \u001b[1m(\u001b[0mmasking-based\u001b[1m)\u001b[0m L2S solvers, but also\\nshowcases superiority over the learning-to-construct \u001b[1m(\u001b[0mL2C\u001b[1m)\u001b[0m and\\nlearning-to-predict \u001b[1m(\u001b[0mL2P\u001b[1m)\u001b[0m solvers. Notably, we offer fresh perspectives on how\\nneural solvers can handle VRP constraints. Our code is available:\\nthis https URL.\\n\\n\n",
       "\u001b[1;36m1\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\nClassical clustering methods do not provide users with direct control of the clustering results, and the clustering results may not be consistent with the relevant criterion that a user has in mind. In this work, we present a new methodology for performing image clustering based on user-specified text criteria by leveraging modern vision-language models and large language models. We call our method Image Clustering Conditioned on Text Criteria \u001b[1m(\u001b[0mIC$|$TC\u001b[1m)\u001b[0m, and it represents a different paradigm of image clustering. IC$|$TC requires a minimal and practical degree of human intervention and grants the user significant control over the clustering results in return. Our experiments show that IC$|$TC can effectively cluster images with various criteria, such as human action, physical location, or the person's mood, while significantly outperforming baselines.\\n\n",
       "\u001b[1;36m2\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n  The fast adoption of new technologies forces companies to continuously adapt\\ntheir operations making it harder to predict workforce requirements. Several\\nrecent studies have attempted to predict the emergence of new roles and skills\\nin the labour market from online job ads. This paper aims to present a novel\\nontology linking business transformation initiatives to occupations and an\\napproach to automatically populating it by leveraging embeddings extracted from\\njob ads and Wikipedia pages on business transformation and emerging\\ntechnologies topics. To our knowledge, no previous research explicitly links\\nbusiness transformation initiatives, like the adoption of new technologies or\\nthe entry into new markets, to the roles needed. Our approach successfully\\nmatches occupations to transformation initiatives under ten different\\nscenarios, five linked to technology adoption and five related to business.\\nThis framework presents an innovative approach to guide enterprises and\\neducational institutions on the workforce requirements for specific business\\ntransformation initiatives.\\n\\n\n",
       "\u001b[1;36m3\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\n  We analyze VeLO \u001b[1m(\u001b[0mversatile learned optimizer\u001b[1m)\u001b[0m, the largest scale attempt to\\ntrain a general purpose \u001b[32m\"foundational\"\u001b[0m optimizer to date. VeLO was trained on\\nthousands of machine learning tasks using over \u001b[1;36m4000\u001b[0m TPU months with the goal of\\nproducing an optimizer capable of generalizing to new problems while being\\nhyperparameter free, and outperforming industry standards such as Adam. We\\nindependently evaluate VeLO on the MLCommons optimizer benchmark suite. We find\\nthat, contrary to initial claims: \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m VeLO has a critical hyperparameter that\\nneeds problem-specific tuning, \u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m VeLO does not necessarily outperform\\ncompetitors in quality of solution found, and \u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m VeLO is not faster than\\ncompeting optimizers at reducing the training loss. These observations call\\ninto question VeLO's generality and the value of the investment in training it.\\n\\n\n",
       "\u001b[1;36m4\u001b[0m    \\nIn this work, our goal is to develop a theoretical framework that can eventually be used for analyzing the effectiveness of visual stories such as feature films to comic books. To develop this theoretical framework, we introduce a new story element called moments. Our conjecture is that any linear story such as the story of a feature film can be decomposed into a set of moments that follow each other. Moments are defined as the perception of the actions, interactions, and expressions of all characters or a single character during a given time period. We categorize the moments into two major types: story moments and discourse moments. Each type of moment can further be classified into three types, which we call universal storytelling moments. We believe these universal moments foster or deteriorate the emotional attachment of the audience to a particular character or the story. We present a methodology to catalog the occurrences of these universal moments as they are found in the story. The cataloged moments can be represented using curves or color strips. Therefore, we can visualize a character's journey through the story as either a 3D curve or a color strip. We also demonstrated that both story and discourse moments can be transformed into one lump-sum attraction parameter. The attraction parameter in time provides a function that can be plotted graphically onto a timeline illustrating changes in the emotional attachment of audience to a character or the story. By inspecting these functions the story analyst can analytically decipher the moments in the story where the attachment is being established, maintained, strengthened, or conversely where it is languishing.\\n\n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[33m...\u001b[0m\n",
       "\u001b[1;36m423\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     NaN\n",
       "\u001b[1;36m424\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     NaN\n",
       "\u001b[1;36m425\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     NaN\n",
       "\u001b[1;36m426\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     NaN\n",
       "\u001b[1;36m427\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     NaN\n",
       "\n",
       "                                      url  \n",
       "\u001b[1;36m0\u001b[0m    \u001b[4;94mhttps://arxiv.org/pdf/2310.18264.pdf\u001b[0m  \n",
       "\u001b[1;36m1\u001b[0m    \u001b[4;94mhttps://arxiv.org/pdf/2310.18297.pdf\u001b[0m  \n",
       "\u001b[1;36m2\u001b[0m    \u001b[4;94mhttps://arxiv.org/pdf/2310.17909.pdf\u001b[0m  \n",
       "\u001b[1;36m3\u001b[0m    \u001b[4;94mhttps://arxiv.org/pdf/2310.18191.pdf\u001b[0m  \n",
       "\u001b[1;36m4\u001b[0m    \u001b[4;94mhttps://arxiv.org/pdf/2310.18273.pdf\u001b[0m  \n",
       "..                                    \u001b[33m...\u001b[0m  \n",
       "\u001b[1;36m423\u001b[0m                                   NaN  \n",
       "\u001b[1;36m424\u001b[0m                                   NaN  \n",
       "\u001b[1;36m425\u001b[0m                                   NaN  \n",
       "\u001b[1;36m426\u001b[0m                                   NaN  \n",
       "\u001b[1;36m427\u001b[0m                                   NaN  \n",
       "\n",
       "\u001b[1m[\u001b[0m\u001b[1;36m428\u001b[0m rows x \u001b[1;36m3\u001b[0m columns\u001b[1m]\u001b[0m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from promptx import store, query\n",
    "\n",
    "collection_name = 'arxiv'\n",
    "store(paper, collection=collection_name)\n",
    "query(collection=collection_name)[['title', 'abstract', 'url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m'3a3b1607-1ca0-457f-9924-c43980373322'\u001b[0m,\n",
       "    \u001b[33mtype\u001b[0m=\u001b[32m'document'\u001b[0m,\n",
       "    \u001b[33mtitle\u001b[0m=\u001b[32m'Improving Intrinsic Exploration by Creating Stationary Objectives'\u001b[0m,\n",
       "    \u001b[33mabstract\u001b[0m=\u001b[32m\"\\nExploration bonuses in reinforcement learning guide long-horizon exploration by defining custom intrinsic objectives. Count-based methods use the frequency of state visits to derive an exploration bonus. In this paper, we identify that any intrinsic reward function derived from count-based methods is non-stationary and hence induces a difficult objective to optimize for the agent. The key contribution of our work lies in transforming the original non-stationary rewards into stationary rewards through an augmented state representation. For this purpose, we introduce the Stationary Objectives For Exploration \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSOFE\u001b[0m\u001b[32m)\u001b[0m\u001b[32m framework. SOFE requires identifying sufficient statistics for different exploration bonuses and finding an efficient encoding of these statistics to use as input to a deep network. SOFE is based on proposing state augmentations that expand the state space but hold the promise of simplifying the optimization of the agent's objective. Our experiments show that SOFE improves the agents' performance in challenging exploration problems, including sparse-reward tasks, pixel-based observations, 3D navigation, and procedurally generated environments.\\n    \"\u001b[0m,\n",
       "    \u001b[33murl\u001b[0m=\u001b[32m'https://arxiv.org/pdf/2310.18144.pdf'\u001b[0m,\n",
       "    \u001b[33mbody\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pdf with 47216 characters\n"
     ]
    }
   ],
   "source": [
    "pdf = load_pdf(paper.url)\n",
    "print(f'Loaded pdf with {len(pdf)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quote(Entity):\n",
    "    text: str\n",
    "    source: Document\n",
    "    start: int\n",
    "    end: int\n",
    "\n",
    "for chunk in batch(doc.sents, bs=10, limit=1000):\n",
    "    store(\n",
    "        *[\n",
    "            Quote(\n",
    "                text=sentence.text,\n",
    "                source=paper,\n",
    "                start=sentence.start_char,\n",
    "                end=sentence.end_char,\n",
    "            ) \n",
    "            for sentence in chunk\n",
    "        ], \n",
    "        collection=collection_name\n",
    "    )\n",
    "\n",
    "query(collection=collection_name).query('type == \"quote\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class ThoughtCategory(str, Enum):\n",
    "    fact = 'fact'\n",
    "    opinion = 'opinion'\n",
    "    idea = 'idea'\n",
    "    connection = 'connection'\n",
    "    belief = 'belief'\n",
    "\n",
    "\n",
    "class Thought(Entity):\n",
    "    value: str\n",
    "    category: ThoughtCategory\n",
    "    confidence: float\n",
    "    source: Entity = Field(None, generate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv-reader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
