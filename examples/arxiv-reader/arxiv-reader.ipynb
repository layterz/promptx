{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# arxiv reader\n",
    "\n",
    "This example demonstrates a technique for generating understanding over long documents - papers from the latest arxiv papers in the AI category in this case.\n",
    "\n",
    "The basic approach is to iterate over the document in chunks and ask the LLM to generate a synthetic dataset of thoughts about each chunk. The thoughts and the chunk are then embedded and can be recalled in subsequent iterations by querying the collection. Then we'll use the generated thoughts to ask questions about the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to define the data types that we'll be generating. Because we're embedding the generated objects we're using `Entity`, which is a thin wrapper on top of `pydantic.BaseModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-11-02 04:47:27.675\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpromptx\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mloading local app from /home/rjl/promptx/examples/arxiv-reader\u001b[0m\n",
      "\u001b[32m2023-11-02 04:47:27.680\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpromptx\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mloaded environment variables from /home/rjl/promptx/examples/arxiv-reader/.env\u001b[0m\n",
      "\u001b[32m2023-11-02 04:47:27.683\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpromptx\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mAPI KEY wMeGC\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "from pydantic import Field\n",
    "from promptx.collection import Entity\n",
    "\n",
    "\n",
    "class Document(Entity):\n",
    "    title: str\n",
    "    abstract: str\n",
    "    url: str\n",
    "\n",
    "class Quote(Entity):\n",
    "    text: str\n",
    "    source: Document\n",
    "    start: int\n",
    "    end: int\n",
    "\n",
    "class ThoughtCategory(str, Enum):\n",
    "    fact = 'fact'\n",
    "    opinion = 'opinion'\n",
    "    idea = 'idea'\n",
    "    connection = 'connection'\n",
    "    belief = 'belief'\n",
    "\n",
    "class Thought(Entity):\n",
    "    value: str\n",
    "    category: ThoughtCategory\n",
    "    confidence: float\n",
    "    source: Entity = Field(None, generate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to get the data from arxiv. We'll use `requests` to get the data ans `BeautifulSoup` to parse it into a `Document` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "from pydantic import Field\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def get_arxiv_urls():\n",
    "    response = requests.get('https://arxiv.org/list/cs.AI/recent')\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    urls = [f\"https://arxiv.org{a.attrs['href']}\" for a in soup.find_all('a', title='Abstract')]\n",
    "    return urls\n",
    "\n",
    "def extract_whitepaper_from_arxiv(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    title = soup.find('h1', class_='title').text.replace('Title:', '')\n",
    "    abstract = soup.find('blockquote', class_='abstract').text.replace('Abstract:', '')\n",
    "    url = soup.find('a', class_='download-pdf').attrs['href']\n",
    "    url = f\"https://arxiv.org{url}\"\n",
    "\n",
    "    return Document(\n",
    "        title=title,\n",
    "        abstract=abstract,\n",
    "        url=url,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use these functions to fetch the latest papers, select one at random, and extract the data from the HTML content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='b83c7abb-6801-4811-83a0-e6b74536ae3b' type='document' title='Minimally Modifying a Markov Game to Achieve Any Nash Equilibrium and Value' abstract='\\n  We study the game modification problem, where a benevolent game designer or a\\nmalevolent adversary modifies the reward function of a zero-sum Markov game so\\nthat a target deterministic or stochastic policy profile becomes the unique\\nMarkov perfect Nash equilibrium and has a value within a target range, in a way\\nthat minimizes the modification cost. We characterize the set of policy\\nprofiles that can be installed as the unique equilibrium of some game, and\\nestablish sufficient and necessary conditions for successful installation. We\\npropose an efficient algorithm, which solves a convex optimization problem with\\nlinear constraints and then performs random perturbation, to obtain a\\nmodification plan with a near-optimal cost.\\n\\n    ' url='https://arxiv.org/pdf/2311.00582.pdf'\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "try:\n",
    "    urls = get_arxiv_urls()\n",
    "    url = random.choice(urls)\n",
    "    paper = extract_whitepaper_from_arxiv(url)\n",
    "    print(paper)\n",
    "except Exception as e:\n",
    "    print(f'Error loading {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The document instance only has data about the paper and doesn't contain the actual text. Let's create a function to extract text from a PDF given a path or URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import requests\n",
    "import uuid\n",
    "\n",
    "def load_pdf(filepath_or_url):\n",
    "    \"\"\"\n",
    "    Load content of a PDF from either a file path or a remote URL.\n",
    "    \n",
    "    :param filepath_or_url: File path or URL to fetch the PDF from.\n",
    "    :return: Content of the PDF as a string.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Handle remote URL\n",
    "    if filepath_or_url.startswith((\"http://\", \"https://\")):\n",
    "        response = requests.get(filepath_or_url)\n",
    "        response.raise_for_status()\n",
    "        id = str(uuid.uuid4())\n",
    "        filepath_or_url = f'./data/{id}.pdf'\n",
    "        with open(filepath_or_url, 'wb') as pdf:\n",
    "            pdf.write(response.content)\n",
    "    \n",
    "    with open(filepath_or_url, 'rb') as f:\n",
    "        pdf_reader = PyPDF2.PdfReader(f)\n",
    "        text_content = ''.join([page.extract_text() for page in pdf_reader.pages])\n",
    "    return text_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pdf with 64217 characters\n"
     ]
    }
   ],
   "source": [
    "pdf = load_pdf(paper.url)\n",
    "print(f'Loaded pdf with {len(pdf)} characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the full text, but how do we split it into chunks that are small enough to be processed by the LLM? You could do this in a number of ways, but we'll use `spacy`, a popular NLP library, to split the text into sentences and then group them into chunks of 512 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a parsed `spacy` document we can split it into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "•δ‰\u001b[1;36m0.\u001b[0mThis follows from the first equality of \u001b[1m(\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1m)\u001b[0m\n",
       "RIJδ“x1, \u001b[1m(\u001b[0m\u001b[1;36m27\u001b[0m\u001b[1m)\u001b[0m\n",
       "otherwise both δandxwould be zero, contradicting a nonzero solution."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = doc.sents\n",
    "random.choice(list(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could iterate over each sentence individually, but that will be slow, expensive, and some sentences will be too short to convery meaning on their own. Instead, we'll group them into batches, or passages, and generate thoughts based on each passage of text. Before we do that, let's define a helper function to process the sentences in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(generator, bs=1, limit=None):\n",
    "    b = []\n",
    "    i = 0\n",
    "    for item in generator:\n",
    "        if limit and i > limit:\n",
    "            break\n",
    "        b.append(item)\n",
    "        if len(b) == bs:\n",
    "            yield b\n",
    "            b = []\n",
    "        i += bs\n",
    "    if b and (limit and i <= limit):  # Yield any remaining items in the batch\n",
    "        yield b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function yield's the generator in chunks defines by the batch size up to a total number of processed items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in batch(doc.sents, bs=5, limit=100):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the logic for thought generation. It's often tempting to overcomplicate prompts, but it can be difficult to know whether more information is actually helping. Often, less is more as it allows the model to focus more effectively.\n",
    "\n",
    "Let's start by simply generating a list of thoughts based on the current passage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptx import prompt\n",
    "\n",
    "def read(doc: list[str], bs=5, limit=None):\n",
    "    thoughts = []\n",
    "    for chunk in batch(doc, bs=bs, limit=limit):\n",
    "        print(f'Passage: {chunk}')\n",
    "        output = prompt(\n",
    "            '''\n",
    "            Given a passage from a document, generate a list of thoughts about the passage.\n",
    "            ''',\n",
    "            input=dict(\n",
    "                passage=chunk,\n",
    "            ),\n",
    "            output=[Thought],\n",
    "        ).objects\n",
    "\n",
    "        print(f'Thoughts: {[t.value for t in output]}')\n",
    "        thoughts += output\n",
    "    return thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thoughts = read([sentence.text for sentence in doc.sents], limit=100)\n",
    "[thought.value for thought in thoughts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if we replace the instructions with something like:\n",
    "\n",
    "```\n",
    "You are an AI researcher reading a whitepaper.\n",
    "Given a passage of text from the paper, generate a list of thoughts about the passage.\n",
    "```\n",
    "\n",
    "This produces very similar results, but is now far less useful because of how specific it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, let's try to improve the results by providing some more context in each prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptx import prompt\n",
    "\n",
    "def read(doc: list[str], bs=5, limit=None):\n",
    "    thoughts = []\n",
    "    recent_thoughts = []\n",
    "    previous_passage = None\n",
    "    for chunk in batch(doc, bs=bs, limit=limit):\n",
    "        print(f'Passage: {chunk}')\n",
    "        output = prompt(\n",
    "            '''\n",
    "            Given a passage from a document, generate a list of thoughts about the passage.\n",
    "            ''',\n",
    "            input=dict(\n",
    "                passage=chunk,\n",
    "                previous_passage=previous_passage,\n",
    "                recent_thoughts=[t.value for t in recent_thoughts],\n",
    "            ),\n",
    "            output=[Thought],\n",
    "        ).objects\n",
    "\n",
    "        print(f'Thoughts: {[t.value for t in output]}')\n",
    "        thoughts += output\n",
    "        previous_passage = chunk\n",
    "        recent_thoughts = (output + recent_thoughts)[:5]\n",
    "    return thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thoughts = read([sentence.text for sentence in doc.sents], limit=100)\n",
    "[thought.value for thought in thoughts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptx import prompt\n",
    "\n",
    "def read(doc: list[str], bs=5, limit=None):\n",
    "    thoughts = []\n",
    "    recent_thoughts = []\n",
    "    previous_passage = None\n",
    "    for chunk in batch(doc, bs=bs, limit=limit):\n",
    "        print(f'Passage: {chunk}')\n",
    "        output = prompt(\n",
    "            '''\n",
    "            Given a passage from a document, generate a list of thoughts about the passage.\n",
    "            ''',\n",
    "            input=dict(\n",
    "                passage=chunk,\n",
    "                previous_passage=previous_passage,\n",
    "            ),\n",
    "            output=[Thought],\n",
    "        ).objects\n",
    "\n",
    "        print(f'Thoughts: {[t.value for t in output]}')\n",
    "        thoughts += output\n",
    "        previous_passage = chunk\n",
    "        recent_thoughts = (output + recent_thoughts)[:5]\n",
    "    return thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thoughts = read([sentence.text for sentence in doc.sents], limit=100)\n",
    "[thought.value for thought in thoughts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptx import prompt\n",
    "\n",
    "def read(doc: list[str], bs=5, limit=None):\n",
    "    thoughts = []\n",
    "    recent_thoughts = []\n",
    "    previous_passage = None\n",
    "    for chunk in batch(doc, bs=bs, limit=limit):\n",
    "        print(f'Passage: {chunk}')\n",
    "        output = prompt(\n",
    "            '''\n",
    "            Given a passage from a document, generate a list of thoughts about the passage.\n",
    "            ''',\n",
    "            input=dict(\n",
    "                passage=chunk,\n",
    "                recent_thoughts=recent_thoughts,\n",
    "            ),\n",
    "            output=[Thought],\n",
    "        ).objects\n",
    "\n",
    "        print(f'Thoughts: {[t.value for t in output]}')\n",
    "        thoughts += output\n",
    "        previous_passage = chunk\n",
    "        recent_thoughts = (output + recent_thoughts)[:5]\n",
    "    return thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thoughts = read([sentence.text for sentence in doc.sents], limit=100)\n",
    "[thought.value for thought in thoughts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptx import delete_collection\n",
    "\n",
    "delete_collection('arxiv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptx import prompt, query, store\n",
    "\n",
    "def read(doc: list[str], bs=5, limit=None):\n",
    "    thoughts = []\n",
    "    recent_thoughts = []\n",
    "    previous_passage = None\n",
    "    for chunk in batch(doc, bs=bs, limit=limit):\n",
    "        print(f'Passage: {chunk}')\n",
    "        try:\n",
    "            recalled_thoughts = query(*chunk, collection='arxiv-thoughts', limit=3).objects\n",
    "        except Exception as e:\n",
    "            recalled_thoughts = []\n",
    "        \n",
    "        try:\n",
    "            recalled_quotes = query(*chunk, collection='arxiv-quotes', limit=3).objects\n",
    "        except Exception as e:\n",
    "            recalled_quotes = []\n",
    "\n",
    "        output = prompt(\n",
    "            '''\n",
    "            Given a passage from a document, generate a list of thoughts about the passage.\n",
    "            ''',\n",
    "            input=dict(\n",
    "                passage=chunk,\n",
    "                previous_passage=previous_passage,\n",
    "                recent_thoughts=[t.value for t in recent_thoughts],\n",
    "                recalled_thoughts=[t.value for t in recalled_thoughts],\n",
    "                recalled_quotes=[t.value for t in recalled_quotes],\n",
    "            ),\n",
    "            output=[Thought],\n",
    "        ).objects\n",
    "\n",
    "        print(f'Thoughts: {[t.value for t in output]}')\n",
    "        thoughts += output\n",
    "        previous_passage = chunk\n",
    "        recent_thoughts = (output + recent_thoughts)[:5]\n",
    "\n",
    "        store(*thoughts, collection='arxiv-thoughts')\n",
    "        store(*[Quote(text=text, source=paper, start=0, end=0) for text in chunk], collection='arxiv-qoutes')\n",
    "    return thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thoughts = read([sentence.text for sentence in doc.sents], limit=100)\n",
    "[thought.value for thought in thoughts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BREAK\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def batch(generator, bs=1, limit=None):\n",
    "    b = []\n",
    "    i = 0\n",
    "    for item in generator:\n",
    "        if limit and i > limit:\n",
    "            break\n",
    "        b.append(item)\n",
    "        if len(b) == bs:\n",
    "            yield b\n",
    "            b = []\n",
    "        i += bs\n",
    "    if b and (limit and i <= limit):  # Yield any remaining items in the batch\n",
    "        yield b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptx import store, query\n",
    "\n",
    "collection_name = 'arxiv'\n",
    "store(paper, collection=collection_name)\n",
    "query(collection=collection_name)[['title', 'abstract', 'url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptx import query\n",
    "\n",
    "paper = query(collection=collection_name).query('type == \"document\"').sample().first\n",
    "paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = load_pdf(paper.url)\n",
    "print(f'Loaded pdf with {len(pdf)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptx import store, query\n",
    "\n",
    "for chunk in batch(doc.sents, bs=10, limit=1000):\n",
    "    store(\n",
    "        *[\n",
    "            Quote(\n",
    "                text=sentence.text,\n",
    "                source=paper,\n",
    "                start=sentence.start_char,\n",
    "                end=sentence.end_char,\n",
    "            ) \n",
    "            for sentence in chunk\n",
    "        ], \n",
    "        collection=collection_name\n",
    "    )\n",
    "\n",
    "query(collection=collection_name).query('type == \"quote\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptx import prompt\n",
    "\n",
    "def read_document(doc, bs=5, limit=1000, recall_limit=3, recent_limit=5):\n",
    "    sentences = doc.sents\n",
    "    recent_thoughts = []\n",
    "    previous_passage = None\n",
    "    for chunk in batch(sentences, bs=bs, limit=limit):\n",
    "        passage = [sentence.text for sentence in chunk]\n",
    "        recalled_thoughts = query(*passage, collection=collection_name, limit=recall_limit).query('type == \"thought\"').objects\n",
    "        \n",
    "        thoughts = prompt(\n",
    "            '''\n",
    "            Given a passage of text and some context, generate some new thoughts about the text.\n",
    "            Make sure to not repeat any existing thoughts too closely.\n",
    "            ''',\n",
    "            input=dict(\n",
    "                context=dict(\n",
    "                    previous_passage=previous_passage,\n",
    "                    recent_thoughts=recent_thoughts,\n",
    "                    recalled_thoughts=recalled_thoughts,\n",
    "                ),\n",
    "                passage=passage,\n",
    "            ),\n",
    "            output=[Thought],\n",
    "        )\n",
    "\n",
    "        thoughts = [Thought(**{**dict(thought), 'source': paper}) for thought in thoughts.objects]\n",
    "        recent_thoughts = (thoughts + recent_thoughts)[:recent_limit]\n",
    "        previous_passage = passage\n",
    "        \n",
    "        print(f'Generated {len(thoughts)} thoughts')\n",
    "        print([thought.value for thought in thoughts])\n",
    "\n",
    "        store(*thoughts, collection=collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_document(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thoughts = query(collection=collection_name).query('type == \"thought\"')\n",
    "thoughts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv-reader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
