{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# arxiv reader\n",
    "\n",
    "This example demonstrates a technique for generating understanding over long documents - papers from the latest arxiv papers in the AI category in this case.\n",
    "\n",
    "The basic approach is to iterate over the document in chunks and ask the LLM to generate a synthetic dataset of thoughts about each chunk. The thoughts and the chunk are then embedded and can be recalled in subsequent iterations by querying the collection. Then we'll use the generated thoughts to ask questions about the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to define the data types that we'll be generating. Because we're embedding the generated objects we're using `Entity`, which is a thin wrapper on top of `pydantic.BaseModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-11-02 04:11:33.462\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpromptx\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mloading local app from /home/rjl/promptx/examples/arxiv-reader\u001b[0m\n",
      "\u001b[32m2023-11-02 04:11:33.465\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpromptx\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mloaded environment variables from /home/rjl/promptx/examples/arxiv-reader/.env\u001b[0m\n",
      "\u001b[32m2023-11-02 04:11:33.466\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpromptx\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mAPI KEY wMeGC\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "from pydantic import Field\n",
    "from promptx.collection import Entity\n",
    "\n",
    "\n",
    "class Document(Entity):\n",
    "    title: str\n",
    "    abstract: str\n",
    "    url: str\n",
    "\n",
    "class Quote(Entity):\n",
    "    text: str\n",
    "    source: Document\n",
    "    start: int\n",
    "    end: int\n",
    "\n",
    "class ThoughtCategory(str, Enum):\n",
    "    fact = 'fact'\n",
    "    opinion = 'opinion'\n",
    "    idea = 'idea'\n",
    "    connection = 'connection'\n",
    "    belief = 'belief'\n",
    "\n",
    "class Thought(Entity):\n",
    "    value: str\n",
    "    category: ThoughtCategory\n",
    "    confidence: float\n",
    "    source: Entity = Field(None, generate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to get the data from arxiv. We'll use `requests` to get the data ans `BeautifulSoup` to parse it into a `Document` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "from pydantic import Field\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def get_arxiv_urls():\n",
    "    response = requests.get('https://arxiv.org/list/cs.AI/recent')\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    urls = [f\"https://arxiv.org{a.attrs['href']}\" for a in soup.find_all('a', title='Abstract')]\n",
    "    return urls\n",
    "\n",
    "def extract_whitepaper_from_arxiv(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    title = soup.find('h1', class_='title').text.replace('Title:', '')\n",
    "    abstract = soup.find('blockquote', class_='abstract').text.replace('Abstract:', '')\n",
    "    url = soup.find('a', class_='download-pdf').attrs['href']\n",
    "    url = f\"https://arxiv.org{url}\"\n",
    "\n",
    "    return Document(\n",
    "        title=title,\n",
    "        abstract=abstract,\n",
    "        url=url,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use these functions to fetch the latest papers, select one at random, and extract the data from the HTML content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='2349300d-a5cc-4ca4-9ac4-7f14a36468a2' type='document' title='Minimally Modifying a Markov Game to Achieve Any Nash Equilibrium and Value' abstract='\\n  We study the game modification problem, where a benevolent game designer or a\\nmalevolent adversary modifies the reward function of a zero-sum Markov game so\\nthat a target deterministic or stochastic policy profile becomes the unique\\nMarkov perfect Nash equilibrium and has a value within a target range, in a way\\nthat minimizes the modification cost. We characterize the set of policy\\nprofiles that can be installed as the unique equilibrium of some game, and\\nestablish sufficient and necessary conditions for successful installation. We\\npropose an efficient algorithm, which solves a convex optimization problem with\\nlinear constraints and then performs random perturbation, to obtain a\\nmodification plan with a near-optimal cost.\\n\\n    ' url='https://arxiv.org/pdf/2311.00582.pdf'\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "try:\n",
    "    urls = get_arxiv_urls()\n",
    "    url = random.choice(urls)\n",
    "    paper = extract_whitepaper_from_arxiv(url)\n",
    "    print(paper)\n",
    "except Exception as e:\n",
    "    print(f'Error loading {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The document instance only has data about the paper and doesn't contain the actual text. Let's create a function to extract text from a PDF given a path or URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import requests\n",
    "import uuid\n",
    "\n",
    "def load_pdf(filepath_or_url):\n",
    "    \"\"\"\n",
    "    Load content of a PDF from either a file path or a remote URL.\n",
    "    \n",
    "    :param filepath_or_url: File path or URL to fetch the PDF from.\n",
    "    :return: Content of the PDF as a string.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Handle remote URL\n",
    "    if filepath_or_url.startswith((\"http://\", \"https://\")):\n",
    "        response = requests.get(filepath_or_url)\n",
    "        response.raise_for_status()\n",
    "        id = str(uuid.uuid4())\n",
    "        filepath_or_url = f'./data/{id}.pdf'\n",
    "        with open(filepath_or_url, 'wb') as pdf:\n",
    "            pdf.write(response.content)\n",
    "    \n",
    "    with open(filepath_or_url, 'rb') as f:\n",
    "        pdf_reader = PyPDF2.PdfReader(f)\n",
    "        text_content = ''.join([page.extract_text() for page in pdf_reader.pages])\n",
    "    return text_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pdf with 64217 characters\n"
     ]
    }
   ],
   "source": [
    "pdf = load_pdf(paper.url)\n",
    "print(f'Loaded pdf with {len(pdf)} characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the full text, but how do we split it into chunks that are small enough to be processed by the LLM? You could do this in a number of ways, but we'll use `spacy`, a popular NLP library, to split the text into sentences and then group them into chunks of 512 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a parsed `spacy` document we can split it into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "The stan-\n",
       "dard rock paper scissors game is a special case when\n",
       "the sizes are \u001b[1;36m3\u001b[0m, hence the name."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = doc.sents\n",
    "random.choice(list(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could iterate over each sentence individually, but that will be slow, expensive, and some sentences will be too short to convery meaning on their own. Instead, we'll group them into batches, or passages, and generate thoughts based on each passage of text. Before we do that, let's define a helper function to process the sentences in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(generator, bs=1, limit=None):\n",
    "    b = []\n",
    "    i = 0\n",
    "    for item in generator:\n",
    "        if limit and i > limit:\n",
    "            break\n",
    "        b.append(item)\n",
    "        if len(b) == bs:\n",
    "            yield b\n",
    "            b = []\n",
    "        i += bs\n",
    "    if b and (limit and i <= limit):  # Yield any remaining items in the batch\n",
    "        yield b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function yield's the generator in chunks defines by the batch size up to a total number of processed items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Minimally Modifying a Markov Game to Achieve Any Nash\n",
      "Equilibrium and Value\n",
      "Young Wu Jeremy McMahan Yiding Chen Yudong Chen Xiaojin Zhu Qiaomin Xie\n",
      "University of Wisconsin Madison\n",
      "Abstract\n",
      "We study the game modification problem,\n",
      "where a benevolent game designer or a malev-\n",
      "olent adversary modifies the reward function\n",
      "of a zero-sum Markov game so that a tar-\n",
      "get deterministic or stochastic policy pro-\n",
      "file becomes the unique Markov perfect Nash\n",
      "equilibrium and has a value within a target\n",
      "range, in a way that minimizes the modifica-\n",
      "tion cost., We characterize the set of policy\n",
      "profiles that can be installed as the unique\n",
      "equilibrium of some game, and establish suf-\n",
      "ficient and necessary conditions for success-\n",
      "ful installation., We propose an efficient al-\n",
      "gorithm, which solves a convex optimization\n",
      "problem with linear constraints and then per-\n",
      "forms random perturbation, to obtain a mod-\n",
      "ification plan with a near-optimal cost.\n",
      ", 1 Introduction\n",
      "Consider a two-player zero-sum Markov game G˝“\n",
      "pR˝, P˝qwith payoff matrices R˝and transition prob-\n",
      "ability matrices P˝. LetSbe the finite state space, Ai\n",
      "the finite set of actions for player iPt1,2u, and His\n",
      "the horizon., It is well known that such a game has at\n",
      "least one Markov Perfect (Nash) Equilibrium (MPE)1\n",
      "pp˝,q˝q, where p˝is the Markov policy for player 1\n",
      "andq˝for player 2 (Maskin and Tirole, 2001).]\n",
      "[Fur-\n",
      "thermore, all the MPEs of G˝have the same game\n",
      "value v˝, which is the expected payoff for player 1 and\n",
      "loss for player 2 at equilibrium.\n",
      ", 1In the special case where the Markov game has H“1\n",
      "stage, it reduces to a matrix normal form game; the Markov\n",
      "Perfect Equilibrium reduces to a Nash Equilibrium (NE).\n",
      ", Proceedings of the 27thInternational Conference on Artifi-\n",
      "cial Intelligence and Statistics (AISTATS) 2024, Valencia,\n",
      "Spain., PMLR: Volume TBD., Copyright 2024 by the au-\n",
      "thor(s).There may be reasons for a third party to prefer an\n",
      "outcome with a different MPE and/or game value.\n",
      "]\n",
      "[For instance, a benevolent third party may want to\n",
      "achieve fairness., Many games are unfair in that v˝‰0\n",
      "(an example, two-finger Morra, is given in the experi-\n",
      "ment section)., The third party can modify the payoffs\n",
      "R˝intoRsuch that the new game given to the players\n",
      "is fair with value v“0., Similarly, many games have\n",
      "non-intuitive MPEs, and players with bounded ratio-\n",
      "nality (e.g., average people) may fail to find them., For\n",
      "the benefit of such players, the third party may seek\n",
      "a new game whose MPE pp,qqis an intuitive strategy\n",
      "profile, such as uniform randomization among actions.\n",
      "]\n",
      "[In addition, one often desires an MPE consisting of\n",
      "stochastic policies (i.e., a mixed strategy equilibrium).\n",
      ", If actions represent resources (roads, advertisement\n",
      "slots, etc), the game designer might want all resources\n",
      "to be utilized; if actions represent customers, requests\n",
      "or demand, the designer might want all of them to be\n",
      "served; if a board/video game is concerned, the de-\n",
      "signer might want the agents to take diverse actions\n",
      "so that the game is more entertaining., Conversely, a\n",
      "malicious third party may want to trick the players\n",
      "into playing an MPE pp,qqof its choice., As most\n",
      "games have mixed equilibria, the players may get sus-\n",
      "picious if the modified game turns out to have a pure\n",
      "strategy MPE, whereas a mixed equilibrium is harder\n",
      "to detect., Furthermore, the adversary may want to\n",
      "control the game value vto favor one player over the\n",
      "other—this is the analogue of adversarial attacks in su-\n",
      "pervised learning.]\n"
     ]
    }
   ],
   "source": [
    "for chunk in batch(doc.sents, bs=5, limit=100):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the logic for thought generation. It's often tempting to overcomplicate prompts, but it can be difficult to know whether more information is actually helping. Often, less is more as it allows the model to focus more effectively.\n",
    "\n",
    "Let's start by simply generating a list of thoughts based on the current passage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptx import prompt\n",
    "\n",
    "def read(doc: list[str], bs=5, limit=None):\n",
    "    thoughts = []\n",
    "    for chunk in batch(doc, bs=bs, limit=limit):\n",
    "        print(f'Passage: {chunk}')\n",
    "        output = prompt(\n",
    "            '''\n",
    "            Given a passage from a document, generate a list of thoughts about the passage.\n",
    "            ''',\n",
    "            input=dict(\n",
    "                passage=chunk,\n",
    "            ),\n",
    "            output=[Thought],\n",
    "        ).objects\n",
    "\n",
    "        print(f'Thoughts: {[t.value for t in output]}')\n",
    "        thoughts += output\n",
    "    return thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage: ['Minimally Modifying a Markov Game to Achieve Any Nash\\nEquilibrium and Value\\nYoung Wu Jeremy McMahan Yiding Chen Yudong Chen Xiaojin Zhu Qiaomin Xie\\nUniversity of Wisconsin Madison\\nAbstract\\nWe study the game modification problem,\\nwhere a benevolent game designer or a malev-\\nolent adversary modifies the reward function\\nof a zero-sum Markov game so that a tar-\\nget deterministic or stochastic policy pro-\\nfile becomes the unique Markov perfect Nash\\nequilibrium and has a value within a target\\nrange, in a way that minimizes the modifica-\\ntion cost.', 'We characterize the set of policy\\nprofiles that can be installed as the unique\\nequilibrium of some game, and establish suf-\\nficient and necessary conditions for success-\\nful installation.', 'We propose an efficient al-\\ngorithm, which solves a convex optimization\\nproblem with linear constraints and then per-\\nforms random perturbation, to obtain a mod-\\nification plan with a near-optimal cost.\\n', '1 Introduction\\nConsider a two-player zero-sum Markov game G˝“\\npR˝, P˝qwith payoff matrices R˝and transition prob-\\nability matrices P˝. LetSbe the finite state space, Ai\\nthe finite set of actions for player iPt1,2u, and His\\nthe horizon.', 'It is well known that such a game has at\\nleast one Markov Perfect (Nash) Equilibrium (MPE)1\\npp˝,q˝q, where p˝is the Markov policy for player 1\\nandq˝for player 2 (Maskin and Tirole, 2001).']\n",
      "Thoughts: ['Minimally Modifying a Markov Game to Achieve Any Nash Equilibrium and Value', 'We study the game modification problem', 'A benevolent game designer or a malevolent adversary modifies the reward function of a zero-sum Markov game', 'A target deterministic or stochastic policy profile becomes the unique Markov perfect Nash equilibrium and has a value within a target range', 'Minimizes the modification cost', 'We characterize the set of policy profiles that can be installed as the unique equilibrium of some game', 'Establish sufficient and necessary conditions for successful installation', 'Propose an efficient algorithm', 'Solves a convex optimization problem with linear constraints and then performs random perturbation', 'Obtain a modification plan with a near-optimal cost', 'A two-player zero-sum Markov game G˝“ pR˝, P˝q with payoff matrices R˝ and transition probability matrices P˝', 'Let S be the finite state space, Ai the finite set of actions for player iPt1,2u, and His the horizon', 'Such a game has at least one Markov Perfect Nash Equilibrium (MPE)1 pp˝,q˝q, where p˝ is the Markov policy for player 1 and q˝ for player 2 (Maskin and Tirole, 2001)']\n",
      "Passage: ['Fur-\\nthermore, all the MPEs of G˝have the same game\\nvalue v˝, which is the expected payoff for player 1 and\\nloss for player 2 at equilibrium.\\n', '1In the special case where the Markov game has H“1\\nstage, it reduces to a matrix normal form game; the Markov\\nPerfect Equilibrium reduces to a Nash Equilibrium (NE).\\n', 'Proceedings of the 27thInternational Conference on Artifi-\\ncial Intelligence and Statistics (AISTATS) 2024, Valencia,\\nSpain.', 'PMLR: Volume TBD.', 'Copyright 2024 by the au-\\nthor(s).There may be reasons for a third party to prefer an\\noutcome with a different MPE and/or game value.\\n']\n",
      "Thoughts: ['All the MPEs of G˝have the same game value v˝', 'The Markov game reduces to a matrix normal form game in the special case where it has H“1 stage', 'The Markov Perfect Equilibrium reduces to a Nash Equilibrium (NE)', 'There may be reasons for a third party to prefer an outcome with a different MPE and/or game value']\n",
      "Passage: ['For instance, a benevolent third party may want to\\nachieve fairness.', 'Many games are unfair in that v˝‰0\\n(an example, two-finger Morra, is given in the experi-\\nment section).', 'The third party can modify the payoffs\\nR˝intoRsuch that the new game given to the players\\nis fair with value v“0.', 'Similarly, many games have\\nnon-intuitive MPEs, and players with bounded ratio-\\nnality (e.g., average people) may fail to find them.', 'For\\nthe benefit of such players, the third party may seek\\na new game whose MPE pp,qqis an intuitive strategy\\nprofile, such as uniform randomization among actions.\\n']\n",
      "Thoughts: ['Many games are unfair', 'The third party can modify the payoffs', 'The new game given to the players is fair with value v“0', 'Many games have non-intuitive MPEs', 'Players with bounded rationality may fail to find non-intuitive MPEs', 'The third party may seek a new game whose MPE is an intuitive strategy profile', 'The third party may seek uniform randomization among actions']\n",
      "Passage: ['In addition, one often desires an MPE consisting of\\nstochastic policies (i.e., a mixed strategy equilibrium).\\n', 'If actions represent resources (roads, advertisement\\nslots, etc), the game designer might want all resources\\nto be utilized; if actions represent customers, requests\\nor demand, the designer might want all of them to be\\nserved; if a board/video game is concerned, the de-\\nsigner might want the agents to take diverse actions\\nso that the game is more entertaining.', 'Conversely, a\\nmalicious third party may want to trick the players\\ninto playing an MPE pp,qqof its choice.', 'As most\\ngames have mixed equilibria, the players may get sus-\\npicious if the modified game turns out to have a pure\\nstrategy MPE, whereas a mixed equilibrium is harder\\nto detect.', 'Furthermore, the adversary may want to\\ncontrol the game value vto favor one player over the\\nother—this is the analogue of adversarial attacks in su-\\npervised learning.']\n",
      "Thoughts: ['One often desires an MPE consisting of stochastic policies (i.e., a mixed strategy equilibrium).', 'The game designer might want all resources to be utilized.', 'The game designer might want all customers, requests, or demand to be served.', 'The game designer might want the agents to take diverse actions to make the game more entertaining.', 'A malicious third party may want to trick the players into playing an MPE of its choice.', 'Players may get suspicious if the modified game has a pure strategy MPE instead of a mixed equilibrium.', 'An adversary may want to control the game value to favor one player over the other.', 'Adversarial attacks in supervised learning are analogous to an adversary controlling the game value.']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'Minimally Modifying a Markov Game to Achieve Any Nash Equilibrium and Value'\u001b[0m,\n",
       "    \u001b[32m'We study the game modification problem'\u001b[0m,\n",
       "    \u001b[32m'A benevolent game designer or a malevolent adversary modifies the reward function of a zero-sum Markov game'\u001b[0m,\n",
       "    \u001b[32m'A target deterministic or stochastic policy profile becomes the unique Markov perfect Nash equilibrium and has a value within a target range'\u001b[0m,\n",
       "    \u001b[32m'Minimizes the modification cost'\u001b[0m,\n",
       "    \u001b[32m'We characterize the set of policy profiles that can be installed as the unique equilibrium of some game'\u001b[0m,\n",
       "    \u001b[32m'Establish sufficient and necessary conditions for successful installation'\u001b[0m,\n",
       "    \u001b[32m'Propose an efficient algorithm'\u001b[0m,\n",
       "    \u001b[32m'Solves a convex optimization problem with linear constraints and then performs random perturbation'\u001b[0m,\n",
       "    \u001b[32m'Obtain a modification plan with a near-optimal cost'\u001b[0m,\n",
       "    \u001b[32m'A two-player zero-sum Markov game G˝“ pR˝, P˝q with payoff matrices R˝ and transition probability matrices P˝'\u001b[0m,\n",
       "    \u001b[32m'Let S be the finite state space, Ai the finite set of actions for player iPt1,2u, and His the horizon'\u001b[0m,\n",
       "    \u001b[32m'Such a game has at least one Markov Perfect Nash Equilibrium \u001b[0m\u001b[32m(\u001b[0m\u001b[32mMPE\u001b[0m\u001b[32m)\u001b[0m\u001b[32m1 pp˝,q˝q, where p˝ is the Markov policy for player 1 and q˝ for player 2 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mMaskin and Tirole, 2001\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "    \u001b[32m'All the MPEs of G˝have the same game value v˝'\u001b[0m,\n",
       "    \u001b[32m'The Markov game reduces to a matrix normal form game in the special case where it has H“1 stage'\u001b[0m,\n",
       "    \u001b[32m'The Markov Perfect Equilibrium reduces to a Nash Equilibrium \u001b[0m\u001b[32m(\u001b[0m\u001b[32mNE\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "    \u001b[32m'There may be reasons for a third party to prefer an outcome with a different MPE and/or game value'\u001b[0m,\n",
       "    \u001b[32m'Many games are unfair'\u001b[0m,\n",
       "    \u001b[32m'The third party can modify the payoffs'\u001b[0m,\n",
       "    \u001b[32m'The new game given to the players is fair with value v“0'\u001b[0m,\n",
       "    \u001b[32m'Many games have non-intuitive MPEs'\u001b[0m,\n",
       "    \u001b[32m'Players with bounded rationality may fail to find non-intuitive MPEs'\u001b[0m,\n",
       "    \u001b[32m'The third party may seek a new game whose MPE is an intuitive strategy profile'\u001b[0m,\n",
       "    \u001b[32m'The third party may seek uniform randomization among actions'\u001b[0m,\n",
       "    \u001b[32m'One often desires an MPE consisting of stochastic policies \u001b[0m\u001b[32m(\u001b[0m\u001b[32mi.e., a mixed strategy equilibrium\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.'\u001b[0m,\n",
       "    \u001b[32m'The game designer might want all resources to be utilized.'\u001b[0m,\n",
       "    \u001b[32m'The game designer might want all customers, requests, or demand to be served.'\u001b[0m,\n",
       "    \u001b[32m'The game designer might want the agents to take diverse actions to make the game more entertaining.'\u001b[0m,\n",
       "    \u001b[32m'A malicious third party may want to trick the players into playing an MPE of its choice.'\u001b[0m,\n",
       "    \u001b[32m'Players may get suspicious if the modified game has a pure strategy MPE instead of a mixed equilibrium.'\u001b[0m,\n",
       "    \u001b[32m'An adversary may want to control the game value to favor one player over the other.'\u001b[0m,\n",
       "    \u001b[32m'Adversarial attacks in supervised learning are analogous to an adversary controlling the game value.'\u001b[0m\n",
       "\u001b[1m]\u001b[0m"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thoughts = read([sentence.text for sentence in doc.sents], limit=100)\n",
    "[thought.value for thought in thoughts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if we replace the instructions with something like:\n",
    "\n",
    "```\n",
    "You are an AI researcher reading a whitepaper.\n",
    "Given a passage of text from the paper, generate a list of thoughts about the passage.\n",
    "```\n",
    "\n",
    "This produces very similar results, but is now far less useful because of how specific it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, let's try to improve the results by providing some more context in each prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptx import prompt\n",
    "\n",
    "def read(doc: list[str], bs=5, limit=None):\n",
    "    thoughts = []\n",
    "    recent_thoughts = []\n",
    "    previous_passage = None\n",
    "    for chunk in batch(doc, bs=bs, limit=limit):\n",
    "        print(f'Passage: {chunk}')\n",
    "        output = prompt(\n",
    "            '''\n",
    "            Given a passage from a document, generate a list of thoughts about the passage.\n",
    "            ''',\n",
    "            input=dict(\n",
    "                passage=chunk,\n",
    "                previous_passage=previous_passage,\n",
    "                recent_thoughts=[t.value for t in recent_thoughts],\n",
    "            ),\n",
    "            output=[Thought],\n",
    "        ).objects\n",
    "\n",
    "        print(f'Thoughts: {[t.value for t in output]}')\n",
    "        thoughts += output\n",
    "        previous_passage = chunk\n",
    "        recent_thoughts = (output + recent_thoughts)[:5]\n",
    "    return thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage: ['Minimally Modifying a Markov Game to Achieve Any Nash\\nEquilibrium and Value\\nYoung Wu Jeremy McMahan Yiding Chen Yudong Chen Xiaojin Zhu Qiaomin Xie\\nUniversity of Wisconsin Madison\\nAbstract\\nWe study the game modification problem,\\nwhere a benevolent game designer or a malev-\\nolent adversary modifies the reward function\\nof a zero-sum Markov game so that a tar-\\nget deterministic or stochastic policy pro-\\nfile becomes the unique Markov perfect Nash\\nequilibrium and has a value within a target\\nrange, in a way that minimizes the modifica-\\ntion cost.', 'We characterize the set of policy\\nprofiles that can be installed as the unique\\nequilibrium of some game, and establish suf-\\nficient and necessary conditions for success-\\nful installation.', 'We propose an efficient al-\\ngorithm, which solves a convex optimization\\nproblem with linear constraints and then per-\\nforms random perturbation, to obtain a mod-\\nification plan with a near-optimal cost.\\n', '1 Introduction\\nConsider a two-player zero-sum Markov game G˝“\\npR˝, P˝qwith payoff matrices R˝and transition prob-\\nability matrices P˝. LetSbe the finite state space, Ai\\nthe finite set of actions for player iPt1,2u, and His\\nthe horizon.', 'It is well known that such a game has at\\nleast one Markov Perfect (Nash) Equilibrium (MPE)1\\npp˝,q˝q, where p˝is the Markov policy for player 1\\nandq˝for player 2 (Maskin and Tirole, 2001).']\n",
      "Thoughts: ['Minimally Modifying a Markov Game to Achieve Any Nash Equilibrium and Value', 'The passage discusses the game modification problem', 'The passage proposes an efficient algorithm to solve the game modification problem', 'The passage mentions the existence of at least one Markov Perfect Equilibrium in a two-player zero-sum Markov game', 'The passage references the work of Maskin and Tirole (2001) in relation to Markov Perfect Equilibrium']\n",
      "Passage: ['Fur-\\nthermore, all the MPEs of G˝have the same game\\nvalue v˝, which is the expected payoff for player 1 and\\nloss for player 2 at equilibrium.\\n', '1In the special case where the Markov game has H“1\\nstage, it reduces to a matrix normal form game; the Markov\\nPerfect Equilibrium reduces to a Nash Equilibrium (NE).\\n', 'Proceedings of the 27thInternational Conference on Artifi-\\ncial Intelligence and Statistics (AISTATS) 2024, Valencia,\\nSpain.', 'PMLR: Volume TBD.', 'Copyright 2024 by the au-\\nthor(s).There may be reasons for a third party to prefer an\\noutcome with a different MPE and/or game value.\\n']\n",
      "Thoughts: ['All the MPEs of G˝have the same game value, which is the expected payoff for player 1 and loss for player 2 at equilibrium.', 'In the special case where the Markov game has H“1 stage, it reduces to a matrix normal form game; the Markov Perfect Equilibrium reduces to a Nash Equilibrium (NE).', 'There may be reasons for a third party to prefer an outcome with a different MPE and/or game value.', 'The passage discusses the game modification problem.', 'The passage proposes an efficient algorithm to solve the game modification problem.', 'The passage mentions the existence of at least one Markov Perfect Equilibrium in a two-player zero-sum Markov game.', 'The passage references the work of Maskin and Tirole (2001) in relation to Markov Perfect Equilibrium.']\n",
      "Passage: ['For instance, a benevolent third party may want to\\nachieve fairness.', 'Many games are unfair in that v˝‰0\\n(an example, two-finger Morra, is given in the experi-\\nment section).', 'The third party can modify the payoffs\\nR˝intoRsuch that the new game given to the players\\nis fair with value v“0.', 'Similarly, many games have\\nnon-intuitive MPEs, and players with bounded ratio-\\nnality (e.g., average people) may fail to find them.', 'For\\nthe benefit of such players, the third party may seek\\na new game whose MPE pp,qqis an intuitive strategy\\nprofile, such as uniform randomization among actions.\\n']\n",
      "Thoughts: ['A benevolent third party may want to achieve fairness.', 'Games can be unfair.', 'A third party can modify the payoffs to make the game fair.', 'Many games have non-intuitive MPEs.', 'Players may fail to find the MPE.', 'The third party may seek a new game with an intuitive strategy profile.', 'The MPEs of G˝have the same game value.', 'The Markov Perfect Equilibrium reduces to a Nash Equilibrium in some cases.', 'The third party may prefer an outcome with a different MPE and/or game value.', 'The passage discusses the game modification problem.', 'The passage proposes an efficient algorithm to solve the game modification problem.']\n",
      "Passage: ['In addition, one often desires an MPE consisting of\\nstochastic policies (i.e., a mixed strategy equilibrium).\\n', 'If actions represent resources (roads, advertisement\\nslots, etc), the game designer might want all resources\\nto be utilized; if actions represent customers, requests\\nor demand, the designer might want all of them to be\\nserved; if a board/video game is concerned, the de-\\nsigner might want the agents to take diverse actions\\nso that the game is more entertaining.', 'Conversely, a\\nmalicious third party may want to trick the players\\ninto playing an MPE pp,qqof its choice.', 'As most\\ngames have mixed equilibria, the players may get sus-\\npicious if the modified game turns out to have a pure\\nstrategy MPE, whereas a mixed equilibrium is harder\\nto detect.', 'Furthermore, the adversary may want to\\ncontrol the game value vto favor one player over the\\nother—this is the analogue of adversarial attacks in su-\\npervised learning.']\n",
      "Thoughts: ['The passage discusses the desire for an MPE consisting of stochastic policies.', 'The game designer might want all resources to be utilized and all customers or requests to be served.', 'The game designer might want the agents to take diverse actions to make the game more entertaining.', 'A malicious third party may want to trick the players into playing an MPE pp,qq of its choice.', 'Players may get suspicious if the modified game turns out to have a pure strategy MPE instead of a mixed equilibrium.', 'The adversary may want to control the game value v to favor one player over the other, similar to adversarial attacks in supervised learning.']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'Minimally Modifying a Markov Game to Achieve Any Nash Equilibrium and Value'\u001b[0m,\n",
       "    \u001b[32m'The passage discusses the game modification problem'\u001b[0m,\n",
       "    \u001b[32m'The passage proposes an efficient algorithm to solve the game modification problem'\u001b[0m,\n",
       "    \u001b[32m'The passage mentions the existence of at least one Markov Perfect Equilibrium in a two-player zero-sum Markov game'\u001b[0m,\n",
       "    \u001b[32m'The passage references the work of Maskin and Tirole \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2001\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in relation to Markov Perfect Equilibrium'\u001b[0m,\n",
       "    \u001b[32m'All the MPEs of G˝have the same game value, which is the expected payoff for player 1 and loss for player 2 at equilibrium.'\u001b[0m,\n",
       "    \u001b[32m'In the special case where the Markov game has H“1 stage, it reduces to a matrix normal form game; the Markov Perfect Equilibrium reduces to a Nash Equilibrium \u001b[0m\u001b[32m(\u001b[0m\u001b[32mNE\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.'\u001b[0m,\n",
       "    \u001b[32m'There may be reasons for a third party to prefer an outcome with a different MPE and/or game value.'\u001b[0m,\n",
       "    \u001b[32m'The passage discusses the game modification problem.'\u001b[0m,\n",
       "    \u001b[32m'The passage proposes an efficient algorithm to solve the game modification problem.'\u001b[0m,\n",
       "    \u001b[32m'The passage mentions the existence of at least one Markov Perfect Equilibrium in a two-player zero-sum Markov game.'\u001b[0m,\n",
       "    \u001b[32m'The passage references the work of Maskin and Tirole \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2001\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in relation to Markov Perfect Equilibrium.'\u001b[0m,\n",
       "    \u001b[32m'A benevolent third party may want to achieve fairness.'\u001b[0m,\n",
       "    \u001b[32m'Games can be unfair.'\u001b[0m,\n",
       "    \u001b[32m'A third party can modify the payoffs to make the game fair.'\u001b[0m,\n",
       "    \u001b[32m'Many games have non-intuitive MPEs.'\u001b[0m,\n",
       "    \u001b[32m'Players may fail to find the MPE.'\u001b[0m,\n",
       "    \u001b[32m'The third party may seek a new game with an intuitive strategy profile.'\u001b[0m,\n",
       "    \u001b[32m'The MPEs of G˝have the same game value.'\u001b[0m,\n",
       "    \u001b[32m'The Markov Perfect Equilibrium reduces to a Nash Equilibrium in some cases.'\u001b[0m,\n",
       "    \u001b[32m'The third party may prefer an outcome with a different MPE and/or game value.'\u001b[0m,\n",
       "    \u001b[32m'The passage discusses the game modification problem.'\u001b[0m,\n",
       "    \u001b[32m'The passage proposes an efficient algorithm to solve the game modification problem.'\u001b[0m,\n",
       "    \u001b[32m'The passage discusses the desire for an MPE consisting of stochastic policies.'\u001b[0m,\n",
       "    \u001b[32m'The game designer might want all resources to be utilized and all customers or requests to be served.'\u001b[0m,\n",
       "    \u001b[32m'The game designer might want the agents to take diverse actions to make the game more entertaining.'\u001b[0m,\n",
       "    \u001b[32m'A malicious third party may want to trick the players into playing an MPE pp,qq of its choice.'\u001b[0m,\n",
       "    \u001b[32m'Players may get suspicious if the modified game turns out to have a pure strategy MPE instead of a mixed equilibrium.'\u001b[0m,\n",
       "    \u001b[32m'The adversary may want to control the game value v to favor one player over the other, similar to adversarial attacks in supervised learning.'\u001b[0m\n",
       "\u001b[1m]\u001b[0m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thoughts = read([sentence.text for sentence in doc.sents], limit=100)\n",
    "[thought.value for thought in thoughts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptx import prompt\n",
    "\n",
    "def read(doc: list[str], bs=5, limit=None):\n",
    "    thoughts = []\n",
    "    recent_thoughts = []\n",
    "    previous_passage = None\n",
    "    for chunk in batch(doc, bs=bs, limit=limit):\n",
    "        print(f'Passage: {chunk}')\n",
    "        output = prompt(\n",
    "            '''\n",
    "            Given a passage from a document, generate a list of thoughts about the passage.\n",
    "            ''',\n",
    "            input=dict(\n",
    "                passage=chunk,\n",
    "                previous_passage=previous_passage,\n",
    "            ),\n",
    "            output=[Thought],\n",
    "        ).objects\n",
    "\n",
    "        print(f'Thoughts: {[t.value for t in output]}')\n",
    "        thoughts += output\n",
    "        previous_passage = chunk\n",
    "        recent_thoughts = (output + recent_thoughts)[:5]\n",
    "    return thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage: ['Minimally Modifying a Markov Game to Achieve Any Nash\\nEquilibrium and Value\\nYoung Wu Jeremy McMahan Yiding Chen Yudong Chen Xiaojin Zhu Qiaomin Xie\\nUniversity of Wisconsin Madison\\nAbstract\\nWe study the game modification problem,\\nwhere a benevolent game designer or a malev-\\nolent adversary modifies the reward function\\nof a zero-sum Markov game so that a tar-\\nget deterministic or stochastic policy pro-\\nfile becomes the unique Markov perfect Nash\\nequilibrium and has a value within a target\\nrange, in a way that minimizes the modifica-\\ntion cost.', 'We characterize the set of policy\\nprofiles that can be installed as the unique\\nequilibrium of some game, and establish suf-\\nficient and necessary conditions for success-\\nful installation.', 'We propose an efficient al-\\ngorithm, which solves a convex optimization\\nproblem with linear constraints and then per-\\nforms random perturbation, to obtain a mod-\\nification plan with a near-optimal cost.\\n', '1 Introduction\\nConsider a two-player zero-sum Markov game G˝“\\npR˝, P˝qwith payoff matrices R˝and transition prob-\\nability matrices P˝. LetSbe the finite state space, Ai\\nthe finite set of actions for player iPt1,2u, and His\\nthe horizon.', 'It is well known that such a game has at\\nleast one Markov Perfect (Nash) Equilibrium (MPE)1\\npp˝,q˝q, where p˝is the Markov policy for player 1\\nandq˝for player 2 (Maskin and Tirole, 2001).']\n",
      "Thoughts: ['Minimally Modifying a Markov Game to Achieve Any Nash Equilibrium and Value', 'We study the game modification problem, where a benevolent game designer or a malevolent adversary modifies the reward function of a zero-sum Markov game so that a target deterministic or stochastic policy profile becomes the unique Markov perfect Nash equilibrium and has a value within a target range, in a way that minimizes the modification cost.', 'We characterize the set of policy profiles that can be installed as the unique equilibrium of some game, and establish sufficient and necessary conditions for successful installation.', 'We propose an efficient algorithm, which solves a convex optimization problem with linear constraints and then performs random perturbation, to obtain a modification plan with a near-optimal cost.', 'Consider a two-player zero-sum Markov game G˝“pR˝, P˝qwith payoff matrices R˝and transition probability matrices P˝.', 'It is well known that such a game has at least one Markov Perfect (Nash) Equilibrium (MPE)pp˝,q˝q, where p˝is the Markov policy for player 1 andq˝for player 2 (Maskin and Tirole, 2001).']\n",
      "Passage: ['Fur-\\nthermore, all the MPEs of G˝have the same game\\nvalue v˝, which is the expected payoff for player 1 and\\nloss for player 2 at equilibrium.\\n', '1In the special case where the Markov game has H“1\\nstage, it reduces to a matrix normal form game; the Markov\\nPerfect Equilibrium reduces to a Nash Equilibrium (NE).\\n', 'Proceedings of the 27thInternational Conference on Artifi-\\ncial Intelligence and Statistics (AISTATS) 2024, Valencia,\\nSpain.', 'PMLR: Volume TBD.', 'Copyright 2024 by the au-\\nthor(s).There may be reasons for a third party to prefer an\\noutcome with a different MPE and/or game value.\\n']\n",
      "Thoughts: ['All the MPEs of G˝ have the same game value v˝', 'The Markov Perfect Equilibrium reduces to a Nash Equilibrium (NE) in the case where the Markov game has H\"1 stage', 'There may be reasons for a third party to prefer an outcome with a different MPE and/or game value', 'The passage is from the Proceedings of the 27th International Conference on Artificial Intelligence and Statistics (AISTATS) 2024 in Valencia, Spain', 'The passage is from PMLR: Volume TBD', 'The document is copyrighted by the author(s)', 'The document is about minimizing modifying a Markov game to achieve a target Nash equilibrium and value', 'The document proposes an efficient algorithm to solve the modification problem with near-optimal cost', 'The Markov game G˝ has a finite state space S, finite set of actions for player i, and horizon H', 'The Markov game G˝ has at least one Markov Perfect (Nash) Equilibrium (MPE)']\n",
      "Passage: ['For instance, a benevolent third party may want to\\nachieve fairness.', 'Many games are unfair in that v˝‰0\\n(an example, two-finger Morra, is given in the experi-\\nment section).', 'The third party can modify the payoffs\\nR˝intoRsuch that the new game given to the players\\nis fair with value v“0.', 'Similarly, many games have\\nnon-intuitive MPEs, and players with bounded ratio-\\nnality (e.g., average people) may fail to find them.', 'For\\nthe benefit of such players, the third party may seek\\na new game whose MPE pp,qqis an intuitive strategy\\nprofile, such as uniform randomization among actions.\\n']\n",
      "Thoughts: ['Many games are unfair', 'The third party can modify the payoffs', 'the new game given to the players\\nis fair with value v“0', 'many games have\\nnon-intuitive MPEs', 'players with bounded ratio-\\nnality (e.g., average people) may fail to find them', 'the third party may seek a new game whose MPE pp,qqis an intuitive strategy\\nprofile, such as uniform randomization among actions', 'all the MPEs of G˝have the same game\\nvalue v˝', 'the expected payoff for player 1 and\\nloss for player 2 at equilibrium', 'the Markov\\nPerfect Equilibrium reduces to a Nash Equilibrium (NE)', 'an outcome with a different MPE and/or game value']\n",
      "Passage: ['In addition, one often desires an MPE consisting of\\nstochastic policies (i.e., a mixed strategy equilibrium).\\n', 'If actions represent resources (roads, advertisement\\nslots, etc), the game designer might want all resources\\nto be utilized; if actions represent customers, requests\\nor demand, the designer might want all of them to be\\nserved; if a board/video game is concerned, the de-\\nsigner might want the agents to take diverse actions\\nso that the game is more entertaining.', 'Conversely, a\\nmalicious third party may want to trick the players\\ninto playing an MPE pp,qqof its choice.', 'As most\\ngames have mixed equilibria, the players may get sus-\\npicious if the modified game turns out to have a pure\\nstrategy MPE, whereas a mixed equilibrium is harder\\nto detect.', 'Furthermore, the adversary may want to\\ncontrol the game value vto favor one player over the\\nother—this is the analogue of adversarial attacks in su-\\npervised learning.']\n",
      "Thoughts: ['One often desires an MPE consisting of stochastic policies (i.e., a mixed strategy equilibrium).', 'Actions can represent resources (roads, advertisement slots, etc) or customers, requests, or demand.', 'Game designers might want all resources to be utilized or all customers/requests to be served.', 'In board/video games designers might want agents to take diverse actions for more entertainment.', 'A malicious third party may want to trick the players into playing an MPE pp, qq of its choice.', 'Modified games with pure strategy MPEs are easier to detect compared to games with mixed equilibria.', 'An adversary may want to control the game value v to favor one player over the other.', 'Adversarial attacks in supervised learning are analogous to controlling game values in MPEs.', 'A benevolent third party may want to achieve fairness in games.', 'Many games are unfair in that v >= 0.', \"The third party can modify the payoffs R into R' such that the new game given to the players is fair with value v = 0.\", 'Many games have non-intuitive MPEs, and players with bounded rationality may fail to find them.', 'A new game with an intuitive MPE, such as uniform randomization among actions, can benefit players with bounded rationality.']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'Minimally Modifying a Markov Game to Achieve Any Nash Equilibrium and Value'\u001b[0m,\n",
       "    \u001b[32m'We study the game modification problem, where a benevolent game designer or a malevolent adversary modifies the reward function of a zero-sum Markov game so that a target deterministic or stochastic policy profile becomes the unique Markov perfect Nash equilibrium and has a value within a target range, in a way that minimizes the modification cost.'\u001b[0m,\n",
       "    \u001b[32m'We characterize the set of policy profiles that can be installed as the unique equilibrium of some game, and establish sufficient and necessary conditions for successful installation.'\u001b[0m,\n",
       "    \u001b[32m'We propose an efficient algorithm, which solves a convex optimization problem with linear constraints and then performs random perturbation, to obtain a modification plan with a near-optimal cost.'\u001b[0m,\n",
       "    \u001b[32m'Consider a two-player zero-sum Markov game G˝“pR˝, P˝qwith payoff matrices R˝and transition probability matrices P˝.'\u001b[0m,\n",
       "    \u001b[32m'It is well known that such a game has at least one Markov Perfect \u001b[0m\u001b[32m(\u001b[0m\u001b[32mNash\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Equilibrium \u001b[0m\u001b[32m(\u001b[0m\u001b[32mMPE\u001b[0m\u001b[32m)\u001b[0m\u001b[32mpp˝,q˝q, where p˝is the Markov policy for player 1 andq˝for player 2 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mMaskin and Tirole, 2001\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.'\u001b[0m,\n",
       "    \u001b[32m'All the MPEs of G˝ have the same game value v˝'\u001b[0m,\n",
       "    \u001b[32m'The Markov Perfect Equilibrium reduces to a Nash Equilibrium \u001b[0m\u001b[32m(\u001b[0m\u001b[32mNE\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in the case where the Markov game has H\"1 stage'\u001b[0m,\n",
       "    \u001b[32m'There may be reasons for a third party to prefer an outcome with a different MPE and/or game value'\u001b[0m,\n",
       "    \u001b[32m'The passage is from the Proceedings of the 27th International Conference on Artificial Intelligence and Statistics \u001b[0m\u001b[32m(\u001b[0m\u001b[32mAISTATS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m 2024 in Valencia, Spain'\u001b[0m,\n",
       "    \u001b[32m'The passage is from PMLR: Volume TBD'\u001b[0m,\n",
       "    \u001b[32m'The document is copyrighted by the author\u001b[0m\u001b[32m(\u001b[0m\u001b[32ms\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "    \u001b[32m'The document is about minimizing modifying a Markov game to achieve a target Nash equilibrium and value'\u001b[0m,\n",
       "    \u001b[32m'The document proposes an efficient algorithm to solve the modification problem with near-optimal cost'\u001b[0m,\n",
       "    \u001b[32m'The Markov game G˝ has a finite state space S, finite set of actions for player i, and horizon H'\u001b[0m,\n",
       "    \u001b[32m'The Markov game G˝ has at least one Markov Perfect \u001b[0m\u001b[32m(\u001b[0m\u001b[32mNash\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Equilibrium \u001b[0m\u001b[32m(\u001b[0m\u001b[32mMPE\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "    \u001b[32m'Many games are unfair'\u001b[0m,\n",
       "    \u001b[32m'The third party can modify the payoffs'\u001b[0m,\n",
       "    \u001b[32m'the new game given to the players\\nis fair with value v“0'\u001b[0m,\n",
       "    \u001b[32m'many games have\\nnon-intuitive MPEs'\u001b[0m,\n",
       "    \u001b[32m'players with bounded ratio-\\nnality \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., average people\u001b[0m\u001b[32m)\u001b[0m\u001b[32m may fail to find them'\u001b[0m,\n",
       "    \u001b[32m'the third party may seek a new game whose MPE pp,qqis an intuitive strategy\\nprofile, such as uniform randomization among actions'\u001b[0m,\n",
       "    \u001b[32m'all the MPEs of G˝have the same game\\nvalue v˝'\u001b[0m,\n",
       "    \u001b[32m'the expected payoff for player 1 and\\nloss for player 2 at equilibrium'\u001b[0m,\n",
       "    \u001b[32m'the Markov\\nPerfect Equilibrium reduces to a Nash Equilibrium \u001b[0m\u001b[32m(\u001b[0m\u001b[32mNE\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "    \u001b[32m'an outcome with a different MPE and/or game value'\u001b[0m,\n",
       "    \u001b[32m'One often desires an MPE consisting of stochastic policies \u001b[0m\u001b[32m(\u001b[0m\u001b[32mi.e., a mixed strategy equilibrium\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.'\u001b[0m,\n",
       "    \u001b[32m'Actions can represent resources \u001b[0m\u001b[32m(\u001b[0m\u001b[32mroads, advertisement slots, etc\u001b[0m\u001b[32m)\u001b[0m\u001b[32m or customers, requests, or demand.'\u001b[0m,\n",
       "    \u001b[32m'Game designers might want all resources to be utilized or all customers/requests to be served.'\u001b[0m,\n",
       "    \u001b[32m'In board/video games designers might want agents to take diverse actions for more entertainment.'\u001b[0m,\n",
       "    \u001b[32m'A malicious third party may want to trick the players into playing an MPE pp, qq of its choice.'\u001b[0m,\n",
       "    \u001b[32m'Modified games with pure strategy MPEs are easier to detect compared to games with mixed equilibria.'\u001b[0m,\n",
       "    \u001b[32m'An adversary may want to control the game value v to favor one player over the other.'\u001b[0m,\n",
       "    \u001b[32m'Adversarial attacks in supervised learning are analogous to controlling game values in MPEs.'\u001b[0m,\n",
       "    \u001b[32m'A benevolent third party may want to achieve fairness in games.'\u001b[0m,\n",
       "    \u001b[32m'Many games are unfair in that v >= 0.'\u001b[0m,\n",
       "    \u001b[32m\"The third party can modify the payoffs R into R' such that the new game given to the players is fair with value v = 0.\"\u001b[0m,\n",
       "    \u001b[32m'Many games have non-intuitive MPEs, and players with bounded rationality may fail to find them.'\u001b[0m,\n",
       "    \u001b[32m'A new game with an intuitive MPE, such as uniform randomization among actions, can benefit players with bounded rationality.'\u001b[0m\n",
       "\u001b[1m]\u001b[0m"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thoughts = read([sentence.text for sentence in doc.sents], limit=100)\n",
    "[thought.value for thought in thoughts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptx import prompt\n",
    "\n",
    "def read(doc: list[str], bs=5, limit=None):\n",
    "    thoughts = []\n",
    "    recent_thoughts = []\n",
    "    previous_passage = None\n",
    "    for chunk in batch(doc, bs=bs, limit=limit):\n",
    "        print(f'Passage: {chunk}')\n",
    "        output = prompt(\n",
    "            '''\n",
    "            Given a passage from a document, generate a list of thoughts about the passage.\n",
    "            ''',\n",
    "            input=dict(\n",
    "                passage=chunk,\n",
    "                recent_thoughts=recent_thoughts,\n",
    "            ),\n",
    "            output=[Thought],\n",
    "        ).objects\n",
    "\n",
    "        print(f'Thoughts: {[t.value for t in output]}')\n",
    "        thoughts += output\n",
    "        previous_passage = chunk\n",
    "        recent_thoughts = (output + recent_thoughts)[:5]\n",
    "    return thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage: ['Minimally Modifying a Markov Game to Achieve Any Nash\\nEquilibrium and Value\\nYoung Wu Jeremy McMahan Yiding Chen Yudong Chen Xiaojin Zhu Qiaomin Xie\\nUniversity of Wisconsin Madison\\nAbstract\\nWe study the game modification problem,\\nwhere a benevolent game designer or a malev-\\nolent adversary modifies the reward function\\nof a zero-sum Markov game so that a tar-\\nget deterministic or stochastic policy pro-\\nfile becomes the unique Markov perfect Nash\\nequilibrium and has a value within a target\\nrange, in a way that minimizes the modifica-\\ntion cost.', 'We characterize the set of policy\\nprofiles that can be installed as the unique\\nequilibrium of some game, and establish suf-\\nficient and necessary conditions for success-\\nful installation.', 'We propose an efficient al-\\ngorithm, which solves a convex optimization\\nproblem with linear constraints and then per-\\nforms random perturbation, to obtain a mod-\\nification plan with a near-optimal cost.\\n', '1 Introduction\\nConsider a two-player zero-sum Markov game G˝“\\npR˝, P˝qwith payoff matrices R˝and transition prob-\\nability matrices P˝. LetSbe the finite state space, Ai\\nthe finite set of actions for player iPt1,2u, and His\\nthe horizon.', 'It is well known that such a game has at\\nleast one Markov Perfect (Nash) Equilibrium (MPE)1\\npp˝,q˝q, where p˝is the Markov policy for player 1\\nandq˝for player 2 (Maskin and Tirole, 2001).']\n",
      "Thoughts: ['Minimally Modifying a Markov Game to Achieve Any Nash Equilibrium and Value', 'We study the game modification problem,where a benevolent game designer or a malevolent adversary modifies the reward function of a zero-sum Markov game so that a target deterministic or stochastic policy profile becomes the unique Markov perfect Nash equilibrium and has a value within a target range, in a way that minimizes the modification cost.', 'We characterize the set of policy profiles that can be installed as the unique equilibrium of some game, and establish sufficient and necessary conditions for successful installation.', 'We propose an efficient algorithm, which solves a convex optimization problem with linear constraints and then performs random perturbation, to obtain a modification plan with a near-optimal cost.', 'Consider a two-player zero-sum Markov game G∼(S,A1,A2,H)', 'It is well known that such a game has at least one Markov Perfect (Nash) Equilibrium (MPE)(p,q), where p is the Markov policy for player 1 and q for player 2 (Maskin and Tirole, 2001).']\n",
      "Passage: ['Fur-\\nthermore, all the MPEs of G˝have the same game\\nvalue v˝, which is the expected payoff for player 1 and\\nloss for player 2 at equilibrium.\\n', '1In the special case where the Markov game has H“1\\nstage, it reduces to a matrix normal form game; the Markov\\nPerfect Equilibrium reduces to a Nash Equilibrium (NE).\\n', 'Proceedings of the 27thInternational Conference on Artifi-\\ncial Intelligence and Statistics (AISTATS) 2024, Valencia,\\nSpain.', 'PMLR: Volume TBD.', 'Copyright 2024 by the au-\\nthor(s).There may be reasons for a third party to prefer an\\noutcome with a different MPE and/or game value.\\n']\n",
      "Thoughts: ['All the MPEs of G˝have the same game value v˝, which is the expected payoff for player 1 and loss for player 2 at equilibrium.', 'The Markov Perfect Equilibrium reduces to a Nash Equilibrium (NE) in the special case where the Markov game has one stage.', 'There may be reasons for a third party to prefer an outcome with a different MPE and/or game value.', 'Minimally Modifying a Markov Game to Achieve Any Nash Equilibrium and Value', 'We study the game modification problem, where a benevolent game designer or a malevolent adversary modifies the reward function of a zero-sum Markov game so that a target deterministic or stochastic policy profile becomes the unique Markov perfect Nash equilibrium and has a value within a target range, in a way that minimizes the modification cost.', 'We characterize the set of policy profiles that can be installed as the unique equilibrium of some game, and establish sufficient and necessary conditions for successful installation.', 'We propose an efficient algorithm, which solves a convex optimization problem with linear constraints and then performs random perturbation, to obtain a modification plan with a near-optimal cost.', 'Consider a two-player zero-sum Markov game G∼(S,A1,A2,H)']\n",
      "Passage: ['For instance, a benevolent third party may want to\\nachieve fairness.', 'Many games are unfair in that v˝‰0\\n(an example, two-finger Morra, is given in the experi-\\nment section).', 'The third party can modify the payoffs\\nR˝intoRsuch that the new game given to the players\\nis fair with value v“0.', 'Similarly, many games have\\nnon-intuitive MPEs, and players with bounded ratio-\\nnality (e.g., average people) may fail to find them.', 'For\\nthe benefit of such players, the third party may seek\\na new game whose MPE pp,qqis an intuitive strategy\\nprofile, such as uniform randomization among actions.\\n']\n",
      "Thoughts: ['The passage discusses the concept of a third party modifying the payoffs of a game to achieve fairness.', 'The passage mentions that many games are unfair and gives an example called two-finger Morra.', 'The third party can modify the payoffs of the game to make it fair.', 'The passage suggests that players with bounded rationality may fail to find non-intuitive MPEs in games.', 'The third party may seek a new game with an intuitive strategy profile, such as uniform randomization among actions, for the benefit of players with bounded rationality.', 'All the MPEs of a certain game have the same game value.', 'The Markov Perfect Equilibrium reduces to a Nash Equilibrium in the special case where the Markov game has one stage.', 'A third party may prefer an outcome with a different MPE and/or game value.', 'The passage introduces the idea of minimally modifying a Markov game to achieve any Nash Equilibrium and value.', 'The passage describes the game modification problem and discusses the role of a benevolent game designer or a malevolent adversary in modifying the reward function of a zero-sum Markov game.']\n",
      "Passage: ['In addition, one often desires an MPE consisting of\\nstochastic policies (i.e., a mixed strategy equilibrium).\\n', 'If actions represent resources (roads, advertisement\\nslots, etc), the game designer might want all resources\\nto be utilized; if actions represent customers, requests\\nor demand, the designer might want all of them to be\\nserved; if a board/video game is concerned, the de-\\nsigner might want the agents to take diverse actions\\nso that the game is more entertaining.', 'Conversely, a\\nmalicious third party may want to trick the players\\ninto playing an MPE pp,qqof its choice.', 'As most\\ngames have mixed equilibria, the players may get sus-\\npicious if the modified game turns out to have a pure\\nstrategy MPE, whereas a mixed equilibrium is harder\\nto detect.', 'Furthermore, the adversary may want to\\ncontrol the game value vto favor one player over the\\nother—this is the analogue of adversarial attacks in su-\\npervised learning.']\n",
      "Thoughts: ['One often desires an MPE consisting of stochastic policies (i.e., a mixed strategy equilibrium).', 'Actions can represent resources or customers and the designer may want all of them to be utilized or served.', 'Diverse actions in a board/video game can make the game more entertaining.', 'A malicious third party may want to trick the players into playing an MPE of its choice.', 'Players may get suspicious if a modified game turns out to have a pure strategy MPE, whereas a mixed equilibrium is harder to detect.', 'The adversary may want to control the game value to favor one player over the other.', 'Modifying the payoffs of a game can achieve fairness.', 'Many games have mixed equilibria.', 'Bounded rationality may cause players to fail to find non-intuitive MPEs in games.', 'The third party may seek a new game with an intuitive strategy profile, such as uniform randomization among actions, for the benefit of players with bounded rationality.']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'Minimally Modifying a Markov Game to Achieve Any Nash Equilibrium and Value'\u001b[0m,\n",
       "    \u001b[32m'We study the game modification problem,where a benevolent game designer or a malevolent adversary modifies the reward function of a zero-sum Markov game so that a target deterministic or stochastic policy profile becomes the unique Markov perfect Nash equilibrium and has a value within a target range, in a way that minimizes the modification cost.'\u001b[0m,\n",
       "    \u001b[32m'We characterize the set of policy profiles that can be installed as the unique equilibrium of some game, and establish sufficient and necessary conditions for successful installation.'\u001b[0m,\n",
       "    \u001b[32m'We propose an efficient algorithm, which solves a convex optimization problem with linear constraints and then performs random perturbation, to obtain a modification plan with a near-optimal cost.'\u001b[0m,\n",
       "    \u001b[32m'Consider a two-player zero-sum Markov game G∼\u001b[0m\u001b[32m(\u001b[0m\u001b[32mS,A1,A2,H\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "    \u001b[32m'It is well known that such a game has at least one Markov Perfect \u001b[0m\u001b[32m(\u001b[0m\u001b[32mNash\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Equilibrium \u001b[0m\u001b[32m(\u001b[0m\u001b[32mMPE\u001b[0m\u001b[32m)\u001b[0m\u001b[32m(\u001b[0m\u001b[32mp,q\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, where p is the Markov policy for player 1 and q for player 2 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mMaskin and Tirole, 2001\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.'\u001b[0m,\n",
       "    \u001b[32m'All the MPEs of G˝have the same game value v˝, which is the expected payoff for player 1 and loss for player 2 at equilibrium.'\u001b[0m,\n",
       "    \u001b[32m'The Markov Perfect Equilibrium reduces to a Nash Equilibrium \u001b[0m\u001b[32m(\u001b[0m\u001b[32mNE\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in the special case where the Markov game has one stage.'\u001b[0m,\n",
       "    \u001b[32m'There may be reasons for a third party to prefer an outcome with a different MPE and/or game value.'\u001b[0m,\n",
       "    \u001b[32m'Minimally Modifying a Markov Game to Achieve Any Nash Equilibrium and Value'\u001b[0m,\n",
       "    \u001b[32m'We study the game modification problem, where a benevolent game designer or a malevolent adversary modifies the reward function of a zero-sum Markov game so that a target deterministic or stochastic policy profile becomes the unique Markov perfect Nash equilibrium and has a value within a target range, in a way that minimizes the modification cost.'\u001b[0m,\n",
       "    \u001b[32m'We characterize the set of policy profiles that can be installed as the unique equilibrium of some game, and establish sufficient and necessary conditions for successful installation.'\u001b[0m,\n",
       "    \u001b[32m'We propose an efficient algorithm, which solves a convex optimization problem with linear constraints and then performs random perturbation, to obtain a modification plan with a near-optimal cost.'\u001b[0m,\n",
       "    \u001b[32m'Consider a two-player zero-sum Markov game G∼\u001b[0m\u001b[32m(\u001b[0m\u001b[32mS,A1,A2,H\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m,\n",
       "    \u001b[32m'The passage discusses the concept of a third party modifying the payoffs of a game to achieve fairness.'\u001b[0m,\n",
       "    \u001b[32m'The passage mentions that many games are unfair and gives an example called two-finger Morra.'\u001b[0m,\n",
       "    \u001b[32m'The third party can modify the payoffs of the game to make it fair.'\u001b[0m,\n",
       "    \u001b[32m'The passage suggests that players with bounded rationality may fail to find non-intuitive MPEs in games.'\u001b[0m,\n",
       "    \u001b[32m'The third party may seek a new game with an intuitive strategy profile, such as uniform randomization among actions, for the benefit of players with bounded rationality.'\u001b[0m,\n",
       "    \u001b[32m'All the MPEs of a certain game have the same game value.'\u001b[0m,\n",
       "    \u001b[32m'The Markov Perfect Equilibrium reduces to a Nash Equilibrium in the special case where the Markov game has one stage.'\u001b[0m,\n",
       "    \u001b[32m'A third party may prefer an outcome with a different MPE and/or game value.'\u001b[0m,\n",
       "    \u001b[32m'The passage introduces the idea of minimally modifying a Markov game to achieve any Nash Equilibrium and value.'\u001b[0m,\n",
       "    \u001b[32m'The passage describes the game modification problem and discusses the role of a benevolent game designer or a malevolent adversary in modifying the reward function of a zero-sum Markov game.'\u001b[0m,\n",
       "    \u001b[32m'One often desires an MPE consisting of stochastic policies \u001b[0m\u001b[32m(\u001b[0m\u001b[32mi.e., a mixed strategy equilibrium\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.'\u001b[0m,\n",
       "    \u001b[32m'Actions can represent resources or customers and the designer may want all of them to be utilized or served.'\u001b[0m,\n",
       "    \u001b[32m'Diverse actions in a board/video game can make the game more entertaining.'\u001b[0m,\n",
       "    \u001b[32m'A malicious third party may want to trick the players into playing an MPE of its choice.'\u001b[0m,\n",
       "    \u001b[32m'Players may get suspicious if a modified game turns out to have a pure strategy MPE, whereas a mixed equilibrium is harder to detect.'\u001b[0m,\n",
       "    \u001b[32m'The adversary may want to control the game value to favor one player over the other.'\u001b[0m,\n",
       "    \u001b[32m'Modifying the payoffs of a game can achieve fairness.'\u001b[0m,\n",
       "    \u001b[32m'Many games have mixed equilibria.'\u001b[0m,\n",
       "    \u001b[32m'Bounded rationality may cause players to fail to find non-intuitive MPEs in games.'\u001b[0m,\n",
       "    \u001b[32m'The third party may seek a new game with an intuitive strategy profile, such as uniform randomization among actions, for the benefit of players with bounded rationality.'\u001b[0m\n",
       "\u001b[1m]\u001b[0m"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thoughts = read([sentence.text for sentence in doc.sents], limit=100)\n",
    "[thought.value for thought in thoughts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptx import delete_collection\n",
    "\n",
    "delete_collection('arxiv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptx import prompt, query, store\n",
    "\n",
    "def read(doc: list[str], bs=5, limit=None):\n",
    "    thoughts = []\n",
    "    recent_thoughts = []\n",
    "    previous_passage = None\n",
    "    for chunk in batch(doc, bs=bs, limit=limit):\n",
    "        print(f'Passage: {chunk}')\n",
    "        try:\n",
    "            recalled_thoughts = query(*chunk, collection='arxiv-thoughts', limit=3).objects\n",
    "        except Exception as e:\n",
    "            recalled_thoughts = []\n",
    "        \n",
    "        try:\n",
    "            recalled_quotes = query(*chunk, collection='arxiv-quotes', limit=3).objects\n",
    "        except Exception as e:\n",
    "            recalled_quotes = []\n",
    "\n",
    "        output = prompt(\n",
    "            '''\n",
    "            Given a passage from a document, generate a list of thoughts about the passage.\n",
    "            ''',\n",
    "            input=dict(\n",
    "                passage=chunk,\n",
    "                previous_passage=previous_passage,\n",
    "                recent_thoughts=[t.value for t in recent_thoughts],\n",
    "                recalled_thoughts=[t.value for t in recalled_thoughts],\n",
    "                recalled_quotes=[t.value for t in recalled_quotes],\n",
    "            ),\n",
    "            output=[Thought],\n",
    "        ).objects\n",
    "\n",
    "        print(f'Thoughts: {[t.value for t in output]}')\n",
    "        thoughts += output\n",
    "        previous_passage = chunk\n",
    "        recent_thoughts = (output + recent_thoughts)[:5]\n",
    "\n",
    "        store(*thoughts, collection='arxiv-thoughts')\n",
    "        store(*[Quote(text=text, source=paper, start=0, end=0) for text in chunk], collection='arxiv-qoutes')\n",
    "    return thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage: ['Minimally Modifying a Markov Game to Achieve Any Nash\\nEquilibrium and Value\\nYoung Wu Jeremy McMahan Yiding Chen Yudong Chen Xiaojin Zhu Qiaomin Xie\\nUniversity of Wisconsin Madison\\nAbstract\\nWe study the game modification problem,\\nwhere a benevolent game designer or a malev-\\nolent adversary modifies the reward function\\nof a zero-sum Markov game so that a tar-\\nget deterministic or stochastic policy pro-\\nfile becomes the unique Markov perfect Nash\\nequilibrium and has a value within a target\\nrange, in a way that minimizes the modifica-\\ntion cost.', 'We characterize the set of policy\\nprofiles that can be installed as the unique\\nequilibrium of some game, and establish suf-\\nficient and necessary conditions for success-\\nful installation.', 'We propose an efficient al-\\ngorithm, which solves a convex optimization\\nproblem with linear constraints and then per-\\nforms random perturbation, to obtain a mod-\\nification plan with a near-optimal cost.\\n', '1 Introduction\\nConsider a two-player zero-sum Markov game G˝“\\npR˝, P˝qwith payoff matrices R˝and transition prob-\\nability matrices P˝. LetSbe the finite state space, Ai\\nthe finite set of actions for player iPt1,2u, and His\\nthe horizon.', 'It is well known that such a game has at\\nleast one Markov Perfect (Nash) Equilibrium (MPE)1\\npp˝,q˝q, where p˝is the Markov policy for player 1\\nandq˝for player 2 (Maskin and Tirole, 2001).']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No collection found with name arxiv-thoughts",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/rjl/promptx/examples/arxiv-reader/arxiv-reader.ipynb Cell 32\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/rjl/promptx/examples/arxiv-reader/arxiv-reader.ipynb#Y104sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m thoughts \u001b[39m=\u001b[39m read([sentence\u001b[39m.\u001b[39;49mtext \u001b[39mfor\u001b[39;49;00m sentence \u001b[39min\u001b[39;49;00m doc\u001b[39m.\u001b[39;49msents], limit\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/rjl/promptx/examples/arxiv-reader/arxiv-reader.ipynb#Y104sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m [thought\u001b[39m.\u001b[39mvalue \u001b[39mfor\u001b[39;00m thought \u001b[39min\u001b[39;00m thoughts]\n",
      "\u001b[1;32m/home/rjl/promptx/examples/arxiv-reader/arxiv-reader.ipynb Cell 32\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/rjl/promptx/examples/arxiv-reader/arxiv-reader.ipynb#Y104sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m batch(doc, bs\u001b[39m=\u001b[39mbs, limit\u001b[39m=\u001b[39mlimit):\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/rjl/promptx/examples/arxiv-reader/arxiv-reader.ipynb#Y104sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mPassage: \u001b[39m\u001b[39m{\u001b[39;00mchunk\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/rjl/promptx/examples/arxiv-reader/arxiv-reader.ipynb#Y104sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     recalled_thoughts \u001b[39m=\u001b[39m query(\u001b[39m*\u001b[39;49mchunk, collection\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39marxiv-thoughts\u001b[39;49m\u001b[39m'\u001b[39;49m, limit\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\u001b[39m.\u001b[39mobjects\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/rjl/promptx/examples/arxiv-reader/arxiv-reader.ipynb#Y104sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     recalled_quotes \u001b[39m=\u001b[39m query(\u001b[39m*\u001b[39mchunk, collection\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39marxiv-quotes\u001b[39m\u001b[39m'\u001b[39m, limit\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mobjects\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/rjl/promptx/examples/arxiv-reader/arxiv-reader.ipynb#Y104sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     output \u001b[39m=\u001b[39m prompt(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/rjl/promptx/examples/arxiv-reader/arxiv-reader.ipynb#Y104sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m        \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/rjl/promptx/examples/arxiv-reader/arxiv-reader.ipynb#Y104sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m        Given a passage from a document, generate a list of thoughts about the passage.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/rjl/promptx/examples/arxiv-reader/arxiv-reader.ipynb#Y104sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m         output\u001b[39m=\u001b[39m[Thought],\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/rjl/promptx/examples/arxiv-reader/arxiv-reader.ipynb#Y104sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     )\u001b[39m.\u001b[39mobjects\n",
      "File \u001b[0;32m~/promptx/promptx/__init__.py:54\u001b[0m, in \u001b[0;36mquery\u001b[0;34m(field, where, collection, *texts, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mquery\u001b[39m(\u001b[39m*\u001b[39mtexts, field\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, where\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, collection\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Collection:\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m DEFAULT_SESSION\u001b[39m.\u001b[39;49mquery(\n\u001b[1;32m     55\u001b[0m         \u001b[39m*\u001b[39;49mtexts, field\u001b[39m=\u001b[39;49mfield, where\u001b[39m=\u001b[39;49mwhere, collection\u001b[39m=\u001b[39;49mcollection, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/promptx/promptx/world.py:108\u001b[0m, in \u001b[0;36mSession.query\u001b[0;34m(self, field, ids, where, collection, limit, *texts)\u001b[0m\n\u001b[1;32m    106\u001b[0m c \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcollection(collection)\n\u001b[1;32m    107\u001b[0m \u001b[39mif\u001b[39;00m c \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 108\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNo collection found with name \u001b[39m\u001b[39m{\u001b[39;00mcollection\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    109\u001b[0m where \u001b[39m=\u001b[39m where \u001b[39mor\u001b[39;00m {}\n\u001b[1;32m    110\u001b[0m \u001b[39mif\u001b[39;00m field \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: No collection found with name arxiv-thoughts"
     ]
    }
   ],
   "source": [
    "thoughts = read([sentence.text for sentence in doc.sents], limit=100)\n",
    "[thought.value for thought in thoughts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BREAK\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def batch(generator, bs=1, limit=None):\n",
    "    b = []\n",
    "    i = 0\n",
    "    for item in generator:\n",
    "        if limit and i > limit:\n",
    "            break\n",
    "        b.append(item)\n",
    "        if len(b) == bs:\n",
    "            yield b\n",
    "            b = []\n",
    "        i += bs\n",
    "    if b and (limit and i <= limit):  # Yield any remaining items in the batch\n",
    "        yield b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptx import store, query\n",
    "\n",
    "collection_name = 'arxiv'\n",
    "store(paper, collection=collection_name)\n",
    "query(collection=collection_name)[['title', 'abstract', 'url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptx import query\n",
    "\n",
    "paper = query(collection=collection_name).query('type == \"document\"').sample().first\n",
    "paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = load_pdf(paper.url)\n",
    "print(f'Loaded pdf with {len(pdf)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptx import store, query\n",
    "\n",
    "for chunk in batch(doc.sents, bs=10, limit=1000):\n",
    "    store(\n",
    "        *[\n",
    "            Quote(\n",
    "                text=sentence.text,\n",
    "                source=paper,\n",
    "                start=sentence.start_char,\n",
    "                end=sentence.end_char,\n",
    "            ) \n",
    "            for sentence in chunk\n",
    "        ], \n",
    "        collection=collection_name\n",
    "    )\n",
    "\n",
    "query(collection=collection_name).query('type == \"quote\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptx import prompt\n",
    "\n",
    "def read_document(doc, bs=5, limit=1000, recall_limit=3, recent_limit=5):\n",
    "    sentences = doc.sents\n",
    "    recent_thoughts = []\n",
    "    previous_passage = None\n",
    "    for chunk in batch(sentences, bs=bs, limit=limit):\n",
    "        passage = [sentence.text for sentence in chunk]\n",
    "        recalled_thoughts = query(*passage, collection=collection_name, limit=recall_limit).query('type == \"thought\"').objects\n",
    "        \n",
    "        thoughts = prompt(\n",
    "            '''\n",
    "            Given a passage of text and some context, generate some new thoughts about the text.\n",
    "            Make sure to not repeat any existing thoughts too closely.\n",
    "            ''',\n",
    "            input=dict(\n",
    "                context=dict(\n",
    "                    previous_passage=previous_passage,\n",
    "                    recent_thoughts=recent_thoughts,\n",
    "                    recalled_thoughts=recalled_thoughts,\n",
    "                ),\n",
    "                passage=passage,\n",
    "            ),\n",
    "            output=[Thought],\n",
    "        )\n",
    "\n",
    "        thoughts = [Thought(**{**dict(thought), 'source': paper}) for thought in thoughts.objects]\n",
    "        recent_thoughts = (thoughts + recent_thoughts)[:recent_limit]\n",
    "        previous_passage = passage\n",
    "        \n",
    "        print(f'Generated {len(thoughts)} thoughts')\n",
    "        print([thought.value for thought in thoughts])\n",
    "\n",
    "        store(*thoughts, collection=collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_document(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thoughts = query(collection=collection_name).query('type == \"thought\"')\n",
    "thoughts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv-reader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
