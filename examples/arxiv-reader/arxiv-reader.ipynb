{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-31 03:58:06.959\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpromptx\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mloading local app from /home/rjl/promptx/examples/arxiv-reader\u001b[0m\n",
      "\u001b[32m2023-10-31 03:58:06.966\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpromptx\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mloaded environment variables from /home/rjl/promptx/examples/arxiv-reader/.env\u001b[0m\n",
      "\u001b[32m2023-10-31 03:58:06.968\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpromptx\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mAPI KEY wMeGC\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", None) \n",
    "\n",
    "from promptx import load\n",
    "\n",
    "load()\n",
    "\n",
    "collection_name = 'arxiv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import requests\n",
    "import uuid\n",
    "\n",
    "def load_pdf(filepath_or_url):\n",
    "    \"\"\"\n",
    "    Load content of a PDF from either a file path or a remote URL.\n",
    "    \n",
    "    :param filepath_or_url: File path or URL to fetch the PDF from.\n",
    "    :return: Content of the PDF as a string.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Handle remote URL\n",
    "    if filepath_or_url.startswith((\"http://\", \"https://\")):\n",
    "        response = requests.get(filepath_or_url)\n",
    "        response.raise_for_status()\n",
    "        id = str(uuid.uuid4())\n",
    "        filepath_or_url = f'./data/{id}.pdf'\n",
    "        with open(filepath_or_url, 'wb') as pdf:\n",
    "            pdf.write(response.content)\n",
    "    \n",
    "    with open(filepath_or_url, 'rb') as f:\n",
    "        pdf_reader = PyPDF2.PdfReader(f)\n",
    "        text_content = ''.join([page.extract_text() for page in pdf_reader.pages])\n",
    "    return text_content\n",
    "\n",
    "\n",
    "def batch(generator, bs=1, limit=None):\n",
    "    b = []\n",
    "    i = 0\n",
    "    for item in generator:\n",
    "        if limit and i > limit:\n",
    "            break\n",
    "        b.append(item)\n",
    "        if len(b) == bs:\n",
    "            yield b\n",
    "            b = []\n",
    "        i += bs\n",
    "    if b and (limit and i <= limit):  # Yield any remaining items in the batch\n",
    "        yield b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "from pydantic import Field\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from promptx.collection import Entity\n",
    "\n",
    "\n",
    "class Document(Entity):\n",
    "    title: str\n",
    "    abstract: str\n",
    "    url: str\n",
    "\n",
    "\n",
    "def get_arxiv_urls():\n",
    "    response = requests.get('https://arxiv.org/list/cs.AI/recent')\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    urls = [f\"https://arxiv.org{a.attrs['href']}\" for a in soup.find_all('a', title='Abstract')]\n",
    "    return urls\n",
    "\n",
    "\n",
    "def extract_whitepaper_from_arxiv(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    title = soup.find('h1', class_='title').text.replace('Title:', '')\n",
    "    abstract = soup.find('blockquote', class_='abstract').text.replace('Abstract:', '')\n",
    "    url = soup.find('a', class_='download-pdf').attrs['href']\n",
    "    url = f\"https://arxiv.org{url}\"\n",
    "\n",
    "    return Document(\n",
    "        title=title,\n",
    "        abstract=abstract,\n",
    "        url=url,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pprint import pprint\n",
    "\n",
    "try:\n",
    "    urls = get_arxiv_urls()\n",
    "    url = random.choice(urls)\n",
    "    paper = extract_whitepaper_from_arxiv(url)\n",
    "except Exception as e:\n",
    "    print(f'Error loading {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Learning to Search Feasible and Infeasible Regions of Routing Problems with Flexible Neural k-Opt</td>\n",
       "      <td>\\n  In this paper, we present Neural k-Opt (NeuOpt), a novel learning-to-search\\n(L2S) solver for routing problems. It learns to perform flexible k-opt\\nexchanges based on a tailored action factorization method and a customized\\nrecurrent dual-stream decoder. As a pioneering work to circumvent the pure\\nfeasibility masking scheme and enable the autonomous exploration of both\\nfeasible and infeasible regions, we then propose the Guided Infeasible Region\\nExploration (GIRE) scheme, which supplements the NeuOpt policy network with\\nfeasibility-related features and leverages reward shaping to steer\\nreinforcement learning more effectively. Additionally, we equip NeuOpt with\\nDynamic Data Augmentation (D2A) for more diverse searches during inference.\\nExtensive experiments on the Traveling Salesman Problem (TSP) and Capacitated\\nVehicle Routing Problem (CVRP) demonstrate that our NeuOpt not only\\nsignificantly outstrips existing (masking-based) L2S solvers, but also\\nshowcases superiority over the learning-to-construct (L2C) and\\nlearning-to-predict (L2P) solvers. Notably, we offer fresh perspectives on how\\nneural solvers can handle VRP constraints. Our code is available:\\nthis https URL.\\n\\n</td>\n",
       "      <td>https://arxiv.org/pdf/2310.18264.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Image Clustering Conditioned on Text Criteria</td>\n",
       "      <td>\\nClassical clustering methods do not provide users with direct control of the clustering results, and the clustering results may not be consistent with the relevant criterion that a user has in mind. In this work, we present a new methodology for performing image clustering based on user-specified text criteria by leveraging modern vision-language models and large language models. We call our method Image Clustering Conditioned on Text Criteria (IC$|$TC), and it represents a different paradigm of image clustering. IC$|$TC requires a minimal and practical degree of human intervention and grants the user significant control over the clustering results in return. Our experiments show that IC$|$TC can effectively cluster images with various criteria, such as human action, physical location, or the person's mood, while significantly outperforming baselines.\\n</td>\n",
       "      <td>https://arxiv.org/pdf/2310.18297.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Innovation-to-Occupations Ontology: Linking Business Transformation Initiatives to Occupations and Skills</td>\n",
       "      <td>\\n  The fast adoption of new technologies forces companies to continuously adapt\\ntheir operations making it harder to predict workforce requirements. Several\\nrecent studies have attempted to predict the emergence of new roles and skills\\nin the labour market from online job ads. This paper aims to present a novel\\nontology linking business transformation initiatives to occupations and an\\napproach to automatically populating it by leveraging embeddings extracted from\\njob ads and Wikipedia pages on business transformation and emerging\\ntechnologies topics. To our knowledge, no previous research explicitly links\\nbusiness transformation initiatives, like the adoption of new technologies or\\nthe entry into new markets, to the roles needed. Our approach successfully\\nmatches occupations to transformation initiatives under ten different\\nscenarios, five linked to technology adoption and five related to business.\\nThis framework presents an innovative approach to guide enterprises and\\neducational institutions on the workforce requirements for specific business\\ntransformation initiatives.\\n\\n</td>\n",
       "      <td>https://arxiv.org/pdf/2310.17909.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is Scaling Learned Optimizers Worth It? Evaluating The Value of VeLO's 4000 TPU Months</td>\n",
       "      <td>\\n  We analyze VeLO (versatile learned optimizer), the largest scale attempt to\\ntrain a general purpose \"foundational\" optimizer to date. VeLO was trained on\\nthousands of machine learning tasks using over 4000 TPU months with the goal of\\nproducing an optimizer capable of generalizing to new problems while being\\nhyperparameter free, and outperforming industry standards such as Adam. We\\nindependently evaluate VeLO on the MLCommons optimizer benchmark suite. We find\\nthat, contrary to initial claims: (1) VeLO has a critical hyperparameter that\\nneeds problem-specific tuning, (2) VeLO does not necessarily outperform\\ncompetitors in quality of solution found, and (3) VeLO is not faster than\\ncompeting optimizers at reducing the training loss. These observations call\\ninto question VeLO's generality and the value of the investment in training it.\\n\\n</td>\n",
       "      <td>https://arxiv.org/pdf/2310.18191.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Moments for Perceptive Narration Analysis Through the Emotional Attachment of Audience to Discourse and Story</td>\n",
       "      <td>\\nIn this work, our goal is to develop a theoretical framework that can eventually be used for analyzing the effectiveness of visual stories such as feature films to comic books. To develop this theoretical framework, we introduce a new story element called moments. Our conjecture is that any linear story such as the story of a feature film can be decomposed into a set of moments that follow each other. Moments are defined as the perception of the actions, interactions, and expressions of all characters or a single character during a given time period. We categorize the moments into two major types: story moments and discourse moments. Each type of moment can further be classified into three types, which we call universal storytelling moments. We believe these universal moments foster or deteriorate the emotional attachment of the audience to a particular character or the story. We present a methodology to catalog the occurrences of these universal moments as they are found in the story. The cataloged moments can be represented using curves or color strips. Therefore, we can visualize a character's journey through the story as either a 3D curve or a color strip. We also demonstrated that both story and discourse moments can be transformed into one lump-sum attraction parameter. The attraction parameter in time provides a function that can be plotted graphically onto a timeline illustrating changes in the emotional attachment of audience to a character or the story. By inspecting these functions the story analyst can analytically decipher the moments in the story where the attachment is being established, maintained, strengthened, or conversely where it is languishing.\\n</td>\n",
       "      <td>https://arxiv.org/pdf/2310.18273.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>550 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "\n",
       "                                                                                                             title  \\\n",
       "\u001b[1;36m0\u001b[0m                Learning to Search Feasible and Infeasible Regions of Routing Problems with Flexible Neural k-Opt \n",
       "\u001b[1;36m1\u001b[0m                                                                    Image Clustering Conditioned on Text Criteria \n",
       "\u001b[1;36m2\u001b[0m    The Innovation-to-Occupations Ontology: Linking Business Transformation Initiatives to Occupations and Skills \n",
       "\u001b[1;36m3\u001b[0m                           Is Scaling Learned Optimizers Worth It? Evaluating The Value of VeLO's \u001b[1;36m4000\u001b[0m TPU Months \n",
       "\u001b[1;36m4\u001b[0m    Moments for Perceptive Narration Analysis Through the Emotional Attachment of Audience to Discourse and Story \n",
       "..                                                                                                             \u001b[33m...\u001b[0m \n",
       "\u001b[1;36m545\u001b[0m                                                                                                            NaN \n",
       "\u001b[1;36m546\u001b[0m                                                                                                            NaN \n",
       "\u001b[1;36m547\u001b[0m                                                                                                            NaN \n",
       "\u001b[1;36m548\u001b[0m                                                                                                            NaN \n",
       "\u001b[1;36m549\u001b[0m                                                                                                            NaN \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   abstract  \\\n",
       "\u001b[1;36m0\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \\n  In this paper, we present Neural k-Opt \u001b[1m(\u001b[0mNeuOpt\u001b[1m)\u001b[0m, a novel learning-to-search\\\u001b[1;35mn\u001b[0m\u001b[1m(\u001b[0mL2S\u001b[1m)\u001b[0m solver for routing problems. It learns to perform flexible k-opt\\nexchanges based on a tailored action factorization method and a customized\\nrecurrent dual-stream decoder. As a pioneering work to circumvent the pure\\nfeasibility masking scheme and enable the autonomous exploration of both\\nfeasible and infeasible regions, we then propose the Guided Infeasible Region\\nExploration \u001b[1m(\u001b[0mGIRE\u001b[1m)\u001b[0m scheme, which supplements the NeuOpt policy network with\\nfeasibility-related features and leverages reward shaping to steer\\nreinforcement learning more effectively. Additionally, we equip NeuOpt with\\nDynamic Data Augmentation \u001b[1m(\u001b[0mD2A\u001b[1m)\u001b[0m for more diverse searches during inference.\\nExtensive experiments on the Traveling Salesman Problem \u001b[1m(\u001b[0mTSP\u001b[1m)\u001b[0m and Capacitated\\nVehicle Routing Problem \u001b[1m(\u001b[0mCVRP\u001b[1m)\u001b[0m demonstrate that our NeuOpt not only\\nsignificantly outstrips existing \u001b[1m(\u001b[0mmasking-based\u001b[1m)\u001b[0m L2S solvers, but also\\nshowcases superiority over the learning-to-construct \u001b[1m(\u001b[0mL2C\u001b[1m)\u001b[0m and\\nlearning-to-predict \u001b[1m(\u001b[0mL2P\u001b[1m)\u001b[0m solvers. Notably, we offer fresh perspectives on how\\nneural solvers can handle VRP constraints. Our code is available:\\nthis https URL.\\n\\n\n",
       "\u001b[1;36m1\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\nClassical clustering methods do not provide users with direct control of the clustering results, and the clustering results may not be consistent with the relevant criterion that a user has in mind. In this work, we present a new methodology for performing image clustering based on user-specified text criteria by leveraging modern vision-language models and large language models. We call our method Image Clustering Conditioned on Text Criteria \u001b[1m(\u001b[0mIC$|$TC\u001b[1m)\u001b[0m, and it represents a different paradigm of image clustering. IC$|$TC requires a minimal and practical degree of human intervention and grants the user significant control over the clustering results in return. Our experiments show that IC$|$TC can effectively cluster images with various criteria, such as human action, physical location, or the person's mood, while significantly outperforming baselines.\\n\n",
       "\u001b[1;36m2\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n  The fast adoption of new technologies forces companies to continuously adapt\\ntheir operations making it harder to predict workforce requirements. Several\\nrecent studies have attempted to predict the emergence of new roles and skills\\nin the labour market from online job ads. This paper aims to present a novel\\nontology linking business transformation initiatives to occupations and an\\napproach to automatically populating it by leveraging embeddings extracted from\\njob ads and Wikipedia pages on business transformation and emerging\\ntechnologies topics. To our knowledge, no previous research explicitly links\\nbusiness transformation initiatives, like the adoption of new technologies or\\nthe entry into new markets, to the roles needed. Our approach successfully\\nmatches occupations to transformation initiatives under ten different\\nscenarios, five linked to technology adoption and five related to business.\\nThis framework presents an innovative approach to guide enterprises and\\neducational institutions on the workforce requirements for specific business\\ntransformation initiatives.\\n\\n\n",
       "\u001b[1;36m3\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\n  We analyze VeLO \u001b[1m(\u001b[0mversatile learned optimizer\u001b[1m)\u001b[0m, the largest scale attempt to\\ntrain a general purpose \u001b[32m\"foundational\"\u001b[0m optimizer to date. VeLO was trained on\\nthousands of machine learning tasks using over \u001b[1;36m4000\u001b[0m TPU months with the goal of\\nproducing an optimizer capable of generalizing to new problems while being\\nhyperparameter free, and outperforming industry standards such as Adam. We\\nindependently evaluate VeLO on the MLCommons optimizer benchmark suite. We find\\nthat, contrary to initial claims: \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m VeLO has a critical hyperparameter that\\nneeds problem-specific tuning, \u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m VeLO does not necessarily outperform\\ncompetitors in quality of solution found, and \u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m VeLO is not faster than\\ncompeting optimizers at reducing the training loss. These observations call\\ninto question VeLO's generality and the value of the investment in training it.\\n\\n\n",
       "\u001b[1;36m4\u001b[0m    \\nIn this work, our goal is to develop a theoretical framework that can eventually be used for analyzing the effectiveness of visual stories such as feature films to comic books. To develop this theoretical framework, we introduce a new story element called moments. Our conjecture is that any linear story such as the story of a feature film can be decomposed into a set of moments that follow each other. Moments are defined as the perception of the actions, interactions, and expressions of all characters or a single character during a given time period. We categorize the moments into two major types: story moments and discourse moments. Each type of moment can further be classified into three types, which we call universal storytelling moments. We believe these universal moments foster or deteriorate the emotional attachment of the audience to a particular character or the story. We present a methodology to catalog the occurrences of these universal moments as they are found in the story. The cataloged moments can be represented using curves or color strips. Therefore, we can visualize a character's journey through the story as either a 3D curve or a color strip. We also demonstrated that both story and discourse moments can be transformed into one lump-sum attraction parameter. The attraction parameter in time provides a function that can be plotted graphically onto a timeline illustrating changes in the emotional attachment of audience to a character or the story. By inspecting these functions the story analyst can analytically decipher the moments in the story where the attachment is being established, maintained, strengthened, or conversely where it is languishing.\\n\n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[33m...\u001b[0m\n",
       "\u001b[1;36m545\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     NaN\n",
       "\u001b[1;36m546\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     NaN\n",
       "\u001b[1;36m547\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     NaN\n",
       "\u001b[1;36m548\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     NaN\n",
       "\u001b[1;36m549\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     NaN\n",
       "\n",
       "                                      url  \n",
       "\u001b[1;36m0\u001b[0m    \u001b[4;94mhttps://arxiv.org/pdf/2310.18264.pdf\u001b[0m  \n",
       "\u001b[1;36m1\u001b[0m    \u001b[4;94mhttps://arxiv.org/pdf/2310.18297.pdf\u001b[0m  \n",
       "\u001b[1;36m2\u001b[0m    \u001b[4;94mhttps://arxiv.org/pdf/2310.17909.pdf\u001b[0m  \n",
       "\u001b[1;36m3\u001b[0m    \u001b[4;94mhttps://arxiv.org/pdf/2310.18191.pdf\u001b[0m  \n",
       "\u001b[1;36m4\u001b[0m    \u001b[4;94mhttps://arxiv.org/pdf/2310.18273.pdf\u001b[0m  \n",
       "..                                    \u001b[33m...\u001b[0m  \n",
       "\u001b[1;36m545\u001b[0m                                   NaN  \n",
       "\u001b[1;36m546\u001b[0m                                   NaN  \n",
       "\u001b[1;36m547\u001b[0m                                   NaN  \n",
       "\u001b[1;36m548\u001b[0m                                   NaN  \n",
       "\u001b[1;36m549\u001b[0m                                   NaN  \n",
       "\n",
       "\u001b[1m[\u001b[0m\u001b[1;36m550\u001b[0m rows x \u001b[1;36m3\u001b[0m columns\u001b[1m]\u001b[0m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from promptx import store, query\n",
    "\n",
    "store(paper, collection=collection_name)\n",
    "query(collection=collection_name)[['title', 'abstract', 'url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m'ab51e267-273b-4329-9d6e-d7c5ab3e8471'\u001b[0m,\n",
       "    \u001b[33mtype\u001b[0m=\u001b[32m'document'\u001b[0m,\n",
       "    \u001b[33mtitle\u001b[0m=\u001b[32m'Image Clustering Conditioned on Text Criteria'\u001b[0m,\n",
       "    \u001b[33mabstract\u001b[0m=\u001b[32m\"\\nClassical clustering methods do not provide users with direct control of the clustering results, and the clustering results may not be consistent with the relevant criterion that a user has in mind. In this work, we present a new methodology for performing image clustering based on user-specified text criteria by leveraging modern vision-language models and large language models. We call our method Image Clustering Conditioned on Text Criteria \u001b[0m\u001b[32m(\u001b[0m\u001b[32mIC$|$TC\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, and it represents a different paradigm of image clustering. IC$|$TC requires a minimal and practical degree of human intervention and grants the user significant control over the clustering results in return. Our experiments show that IC$|$TC can effectively cluster images with various criteria, such as human action, physical location, or the person's mood, while significantly outperforming baselines.\\n    \"\u001b[0m,\n",
       "    \u001b[33murl\u001b[0m=\u001b[32m'https://arxiv.org/pdf/2310.18297.pdf'\u001b[0m,\n",
       "    \u001b[33mbody\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from promptx import query\n",
    "\n",
    "paper = query(collection=collection_name).query('type == \"document\"').sample().first\n",
    "paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pdf with 89386 characters\n"
     ]
    }
   ],
   "source": [
    "pdf = load_pdf(paper.url)\n",
    "print(f'Loaded pdf with {len(pdf)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>body</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>value</th>\n",
       "      <th>category</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>e59b876a-76f3-4554-8bf0-b9f2577f7ad2</td>\n",
       "      <td>quote</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Under Review\\nIMAGE CLUSTERING CONDITIONED ON TEXT CRITERIA\\nSehyun Kwon†♢, Jaeseung Park†♢, Minkyu Kim♢, Jaewoong Cho♢, Ernest K. Ryu†∗, Kangwook Lee♢♣∗\\n†Seoul National University,♢KRAFTON,♣University of Wisconsin–Madison,∗Co-senior authors\\nABSTRACT\\nClassical clustering methods do not provide users with direct control of the clus-\\ntering results, and the clustering results may not be consistent with the relevant\\ncriterion that a user has in mind.</td>\n",
       "      <td>{'ids': ['ab51e267-273b-4329-9d6e-d7c5ab3e8471'], 'collection': 'arxiv', 'limit': 1}</td>\n",
       "      <td>0.0</td>\n",
       "      <td>449.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>5c85156c-d763-485f-8e2a-44b41948d0bc</td>\n",
       "      <td>quote</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In this work, we present a new methodology for\\nperforming image clustering based on user-specified text criteria by leveraging\\nmodern vision-language models and large language models.</td>\n",
       "      <td>{'ids': ['ab51e267-273b-4329-9d6e-d7c5ab3e8471'], 'collection': 'arxiv', 'limit': 1}</td>\n",
       "      <td>450.0</td>\n",
       "      <td>633.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>3c372465-b53f-4b08-a071-607d1c54bf5a</td>\n",
       "      <td>quote</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We call our method\\nImage Clustering Conditioned on TextCriteria (IC |TC), and it represents a differ-\\nent paradigm of image clustering.</td>\n",
       "      <td>{'ids': ['ab51e267-273b-4329-9d6e-d7c5ab3e8471'], 'collection': 'arxiv', 'limit': 1}</td>\n",
       "      <td>634.0</td>\n",
       "      <td>769.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>a7472a5e-3c94-4870-80b8-6ddf37e93d50</td>\n",
       "      <td>quote</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IC |TC requires a minimal and practical degree\\nof human intervention and grants the user significant control over the clustering\\nresults in return.</td>\n",
       "      <td>{'ids': ['ab51e267-273b-4329-9d6e-d7c5ab3e8471'], 'collection': 'arxiv', 'limit': 1}</td>\n",
       "      <td>770.0</td>\n",
       "      <td>917.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>c86deabc-0ec4-4b6b-9c28-c884df333017</td>\n",
       "      <td>quote</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our experiments show that IC |TC can effectively cluster im-\\nages with various criteria, such as human action, physical location, or the person’s\\nmood, while significantly outperforming baselines.1\\n1 I NTRODUCTION\\nImage clustering has been studied as a prototypical unsupervised learning task, and it has been\\nused to organize large volumes of visual data (Platt et al., 2003), to reduce the cost of labeling an\\nunlabeled image dataset (Russell et al., 2008; Schmarje et al., 2022), and to enhance image retrieval\\nsystems (Wu et al., 2000; J ´egou and Chum, 2012).</td>\n",
       "      <td>{'ids': ['ab51e267-273b-4329-9d6e-d7c5ab3e8471'], 'collection': 'arxiv', 'limit': 1}</td>\n",
       "      <td>918.0</td>\n",
       "      <td>1482.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>f3f4ce82-7ce1-4e75-a69c-f52f29a517dd</td>\n",
       "      <td>quote</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CIFAR-10\\nSTL-10\\nCIFAR-100LLAVA</td>\n",
       "      <td>{'ids': ['ab51e267-273b-4329-9d6e-d7c5ab3e8471'], 'collection': 'arxiv', 'limit': 1}</td>\n",
       "      <td>14694.0</td>\n",
       "      <td>14724.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>757ee605-9373-4bf6-a697-2933ad57fe3e</td>\n",
       "      <td>quote</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>only\\nLlama 2 (7B)\\nLlama 2 (13B)\\n</td>\n",
       "      <td>{'ids': ['ab51e267-273b-4329-9d6e-d7c5ab3e8471'], 'collection': 'arxiv', 'limit': 1}</td>\n",
       "      <td>14725.0</td>\n",
       "      <td>14757.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>e0471c26-f9a9-4c8f-88c4-7958902831e3</td>\n",
       "      <td>quote</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Llama 2 (70B)\\nGPT-3.5\\nGPT-4Figure 3: Effect of LLM selection.\\n</td>\n",
       "      <td>{'ids': ['ab51e267-273b-4329-9d6e-d7c5ab3e8471'], 'collection': 'arxiv', 'limit': 1}</td>\n",
       "      <td>14757.0</td>\n",
       "      <td>14819.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>f8d9ed1f-f122-493c-8fc4-6f770cda21ed</td>\n",
       "      <td>quote</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.5 P RODUCING CLUSTER LABELS\\nClassically, the unsupervised clustering task does not require the method to produce labels or de-\\nscriptions of the output clusters.</td>\n",
       "      <td>{'ids': ['ab51e267-273b-4329-9d6e-d7c5ab3e8471'], 'collection': 'arxiv', 'limit': 1}</td>\n",
       "      <td>14819.0</td>\n",
       "      <td>14982.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>2dc08e7e-dd2d-465c-900a-704a40f6a6f2</td>\n",
       "      <td>quote</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Notably, however, IC |TC produces names describing the clusters.\\n</td>\n",
       "      <td>{'ids': ['ab51e267-273b-4329-9d6e-d7c5ab3e8471'], 'collection': 'arxiv', 'limit': 1}</td>\n",
       "      <td>14983.0</td>\n",
       "      <td>15048.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "\n",
       "                                       id   type title abstract  url  body  \\\n",
       "\u001b[1;36m50\u001b[0m   \u001b[93me59b876a-76f3-4554-8bf0-b9f2577f7ad2\u001b[0m  quote   NaN      NaN  NaN   NaN   \n",
       "\u001b[1;36m51\u001b[0m   \u001b[93m5c85156c-d763-485f-8e2a-44b41948d0bc\u001b[0m  quote   NaN      NaN  NaN   NaN   \n",
       "\u001b[1;36m52\u001b[0m   \u001b[93m3c372465-b53f-4b08-a071-607d1c54bf5a\u001b[0m  quote   NaN      NaN  NaN   NaN   \n",
       "\u001b[1;36m53\u001b[0m   \u001b[93ma7472a5e-3c94-4870-80b8-6ddf37e93d50\u001b[0m  quote   NaN      NaN  NaN   NaN   \n",
       "\u001b[1;36m54\u001b[0m   \u001b[93mc86deabc-0ec4-4b6b-9c28-c884df333017\u001b[0m  quote   NaN      NaN  NaN   NaN   \n",
       "..                                    \u001b[33m...\u001b[0m    \u001b[33m...\u001b[0m   \u001b[33m...\u001b[0m      \u001b[33m...\u001b[0m  \u001b[33m...\u001b[0m   \u001b[33m...\u001b[0m   \n",
       "\u001b[1;36m645\u001b[0m  \u001b[93mf3f4ce82-7ce1-4e75-a69c-f52f29a517dd\u001b[0m  quote   NaN      NaN  NaN   NaN   \n",
       "\u001b[1;36m646\u001b[0m  \u001b[93m757ee605-9373-4bf6-a697-2933ad57fe3e\u001b[0m  quote   NaN      NaN  NaN   NaN   \n",
       "\u001b[1;36m647\u001b[0m  \u001b[93me0471c26-f9a9-4c8f-88c4-7958902831e3\u001b[0m  quote   NaN      NaN  NaN   NaN   \n",
       "\u001b[1;36m648\u001b[0m  \u001b[93mf8d9ed1f-f122-493c-8fc4-6f770cda21ed\u001b[0m  quote   NaN      NaN  NaN   NaN   \n",
       "\u001b[1;36m649\u001b[0m  \u001b[93m2dc08e7e-dd2d-465c-900a-704a40f6a6f2\u001b[0m  quote   NaN      NaN  NaN   NaN   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            text  \\\n",
       "\u001b[1;36m50\u001b[0m                                                                                                                      Under Review\\nIMAGE CLUSTERING CONDITIONED ON TEXT CRITERIA\\nSehyun Kwon†♢, Jaeseung Park†♢, Minkyu Kim♢, Jaewoong Cho♢, Ernest K. Ryu†∗, Kangwook Lee♢♣∗\\n†Seoul National University,♢KRAFTON,♣University of Wisconsin–Madison,∗Co-senior authors\\nABSTRACT\\nClassical clustering methods do not provide users with direct control of the clus-\\ntering results, and the clustering results may not be consistent with the relevant\\ncriterion that a user has in mind.\n",
       "\u001b[1;36m51\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                     In this work, we present a new methodology for\\nperforming image clustering based on user-specified text criteria by leveraging\\nmodern vision-language models and large language models.\n",
       "\u001b[1;36m52\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                                     We call our method\\nImage Clustering Conditioned on TextCriteria \u001b[1m(\u001b[0mIC |TC\u001b[1m)\u001b[0m, and it represents a differ-\\nent paradigm of image clustering.\n",
       "\u001b[1;36m53\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                         IC |TC requires a minimal and practical degree\\nof human intervention and grants the user significant control over the clustering\\nresults in return.\n",
       "\u001b[1;36m54\u001b[0m   Our experiments show that IC |TC can effectively cluster im-\\nages with various criteria, such as human action, physical location, or the person’s\\nmood, while significantly outperforming baselines.\u001b[1;36m1\u001b[0m\\n1 I NTRODUCTION\\nImage clustering has been studied as a prototypical unsupervised learning task, and it has been\\nused to organize large volumes of visual data \u001b[1m(\u001b[0mPlatt et al., \u001b[1;36m2003\u001b[0m\u001b[1m)\u001b[0m, to reduce the cost of labeling an\\nunlabeled image dataset \u001b[1m(\u001b[0mRussell et al., \u001b[1;36m2008\u001b[0m; Schmarje et al., \u001b[1;36m2022\u001b[0m\u001b[1m)\u001b[0m, and to enhance image retrieval\\nsystems \u001b[1m(\u001b[0mWu et al., \u001b[1;36m2000\u001b[0m; J ´egou and Chum, \u001b[1;36m2012\u001b[0m\u001b[1m)\u001b[0m.\n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \u001b[33m...\u001b[0m\n",
       "\u001b[1;36m645\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             CIFAR-\u001b[1;36m10\u001b[0m\\nSTL-\u001b[1;36m10\u001b[0m\\nCIFAR-100LLAVA\n",
       "\u001b[1;36m646\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          only\\nLlama \u001b[1;36m2\u001b[0m \u001b[1m(\u001b[0m7B\u001b[1m)\u001b[0m\\nLlama \u001b[1;36m2\u001b[0m \u001b[1m(\u001b[0m13B\u001b[1m)\u001b[0m\\n\n",
       "\u001b[1;36m647\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Llama \u001b[1;36m2\u001b[0m \u001b[1m(\u001b[0m70B\u001b[1m)\u001b[0m\\nGPT-\u001b[1;36m3.5\u001b[0m\\nGPT-4Figure \u001b[1;36m3\u001b[0m: Effect of LLM selection.\\n\n",
       "\u001b[1;36m648\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[1;36m3.5\u001b[0m P RODUCING CLUSTER LABELS\\nClassically, the unsupervised clustering task does not require the method to produce labels or de-\\nscriptions of the output clusters.\n",
       "\u001b[1;36m649\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Notably, however, IC |TC produces names describing the clusters.\\n\n",
       "\n",
       "                                                                                   source  \\\n",
       "\u001b[1;36m50\u001b[0m   \u001b[1m{\u001b[0m\u001b[32m'ids'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'ab51e267-273b-4329-9d6e-d7c5ab3e8471'\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'collection'\u001b[0m: \u001b[32m'arxiv'\u001b[0m, \u001b[32m'limit'\u001b[0m: \u001b[1;36m1\u001b[0m\u001b[1m}\u001b[0m   \n",
       "\u001b[1;36m51\u001b[0m   \u001b[1m{\u001b[0m\u001b[32m'ids'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'ab51e267-273b-4329-9d6e-d7c5ab3e8471'\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'collection'\u001b[0m: \u001b[32m'arxiv'\u001b[0m, \u001b[32m'limit'\u001b[0m: \u001b[1;36m1\u001b[0m\u001b[1m}\u001b[0m   \n",
       "\u001b[1;36m52\u001b[0m   \u001b[1m{\u001b[0m\u001b[32m'ids'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'ab51e267-273b-4329-9d6e-d7c5ab3e8471'\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'collection'\u001b[0m: \u001b[32m'arxiv'\u001b[0m, \u001b[32m'limit'\u001b[0m: \u001b[1;36m1\u001b[0m\u001b[1m}\u001b[0m   \n",
       "\u001b[1;36m53\u001b[0m   \u001b[1m{\u001b[0m\u001b[32m'ids'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'ab51e267-273b-4329-9d6e-d7c5ab3e8471'\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'collection'\u001b[0m: \u001b[32m'arxiv'\u001b[0m, \u001b[32m'limit'\u001b[0m: \u001b[1;36m1\u001b[0m\u001b[1m}\u001b[0m   \n",
       "\u001b[1;36m54\u001b[0m   \u001b[1m{\u001b[0m\u001b[32m'ids'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'ab51e267-273b-4329-9d6e-d7c5ab3e8471'\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'collection'\u001b[0m: \u001b[32m'arxiv'\u001b[0m, \u001b[32m'limit'\u001b[0m: \u001b[1;36m1\u001b[0m\u001b[1m}\u001b[0m   \n",
       "..                                                                                    \u001b[33m...\u001b[0m   \n",
       "\u001b[1;36m645\u001b[0m  \u001b[1m{\u001b[0m\u001b[32m'ids'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'ab51e267-273b-4329-9d6e-d7c5ab3e8471'\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'collection'\u001b[0m: \u001b[32m'arxiv'\u001b[0m, \u001b[32m'limit'\u001b[0m: \u001b[1;36m1\u001b[0m\u001b[1m}\u001b[0m   \n",
       "\u001b[1;36m646\u001b[0m  \u001b[1m{\u001b[0m\u001b[32m'ids'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'ab51e267-273b-4329-9d6e-d7c5ab3e8471'\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'collection'\u001b[0m: \u001b[32m'arxiv'\u001b[0m, \u001b[32m'limit'\u001b[0m: \u001b[1;36m1\u001b[0m\u001b[1m}\u001b[0m   \n",
       "\u001b[1;36m647\u001b[0m  \u001b[1m{\u001b[0m\u001b[32m'ids'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'ab51e267-273b-4329-9d6e-d7c5ab3e8471'\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'collection'\u001b[0m: \u001b[32m'arxiv'\u001b[0m, \u001b[32m'limit'\u001b[0m: \u001b[1;36m1\u001b[0m\u001b[1m}\u001b[0m   \n",
       "\u001b[1;36m648\u001b[0m  \u001b[1m{\u001b[0m\u001b[32m'ids'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'ab51e267-273b-4329-9d6e-d7c5ab3e8471'\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'collection'\u001b[0m: \u001b[32m'arxiv'\u001b[0m, \u001b[32m'limit'\u001b[0m: \u001b[1;36m1\u001b[0m\u001b[1m}\u001b[0m   \n",
       "\u001b[1;36m649\u001b[0m  \u001b[1m{\u001b[0m\u001b[32m'ids'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'ab51e267-273b-4329-9d6e-d7c5ab3e8471'\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'collection'\u001b[0m: \u001b[32m'arxiv'\u001b[0m, \u001b[32m'limit'\u001b[0m: \u001b[1;36m1\u001b[0m\u001b[1m}\u001b[0m   \n",
       "\n",
       "       start      end value category  confidence  \n",
       "\u001b[1;36m50\u001b[0m       \u001b[1;36m0.0\u001b[0m    \u001b[1;36m449.0\u001b[0m   NaN      NaN         NaN  \n",
       "\u001b[1;36m51\u001b[0m     \u001b[1;36m450.0\u001b[0m    \u001b[1;36m633.0\u001b[0m   NaN      NaN         NaN  \n",
       "\u001b[1;36m52\u001b[0m     \u001b[1;36m634.0\u001b[0m    \u001b[1;36m769.0\u001b[0m   NaN      NaN         NaN  \n",
       "\u001b[1;36m53\u001b[0m     \u001b[1;36m770.0\u001b[0m    \u001b[1;36m917.0\u001b[0m   NaN      NaN         NaN  \n",
       "\u001b[1;36m54\u001b[0m     \u001b[1;36m918.0\u001b[0m   \u001b[1;36m1482.0\u001b[0m   NaN      NaN         NaN  \n",
       "..       \u001b[33m...\u001b[0m      \u001b[33m...\u001b[0m   \u001b[33m...\u001b[0m      \u001b[33m...\u001b[0m         \u001b[33m...\u001b[0m  \n",
       "\u001b[1;36m645\u001b[0m  \u001b[1;36m14694.0\u001b[0m  \u001b[1;36m14724.0\u001b[0m   NaN      NaN         NaN  \n",
       "\u001b[1;36m646\u001b[0m  \u001b[1;36m14725.0\u001b[0m  \u001b[1;36m14757.0\u001b[0m   NaN      NaN         NaN  \n",
       "\u001b[1;36m647\u001b[0m  \u001b[1;36m14757.0\u001b[0m  \u001b[1;36m14819.0\u001b[0m   NaN      NaN         NaN  \n",
       "\u001b[1;36m648\u001b[0m  \u001b[1;36m14819.0\u001b[0m  \u001b[1;36m14982.0\u001b[0m   NaN      NaN         NaN  \n",
       "\u001b[1;36m649\u001b[0m  \u001b[1;36m14983.0\u001b[0m  \u001b[1;36m15048.0\u001b[0m   NaN      NaN         NaN  \n",
       "\n",
       "\u001b[1m[\u001b[0m\u001b[1;36m600\u001b[0m rows x \u001b[1;36m13\u001b[0m columns\u001b[1m]\u001b[0m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from promptx import store, query\n",
    "\n",
    "class Quote(Entity):\n",
    "    text: str\n",
    "    source: Document\n",
    "    start: int\n",
    "    end: int\n",
    "\n",
    "for chunk in batch(doc.sents, bs=10, limit=1000):\n",
    "    store(\n",
    "        *[\n",
    "            Quote(\n",
    "                text=sentence.text,\n",
    "                source=paper,\n",
    "                start=sentence.start_char,\n",
    "                end=sentence.end_char,\n",
    "            ) \n",
    "            for sentence in chunk\n",
    "        ], \n",
    "        collection=collection_name\n",
    "    )\n",
    "\n",
    "query(collection=collection_name).query('type == \"quote\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class ThoughtCategory(str, Enum):\n",
    "    fact = 'fact'\n",
    "    opinion = 'opinion'\n",
    "    idea = 'idea'\n",
    "    connection = 'connection'\n",
    "    belief = 'belief'\n",
    "\n",
    "\n",
    "class Thought(Entity):\n",
    "    value: str\n",
    "    category: ThoughtCategory\n",
    "    confidence: float\n",
    "    source: Entity = Field(None, generate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_document(doc, bs=5, limit=1000, recall_limit=3, recent_limit=5):\n",
    "    sentences = doc.sents\n",
    "    recent_thoughts = []\n",
    "    previous_passage = None\n",
    "    for chunk in batch(sentences, bs=bs, limit=limit):\n",
    "        passage = [sentence.text for sentence in chunk]\n",
    "        recalled_thoughts = query(text, collection=collection_name, limit=recall_limit).query('type == \"thought\"').objects\n",
    "        \n",
    "        thoughts = prompt(\n",
    "            '''\n",
    "            Given a passage of text and some context, generate some new thoughts about the text.\n",
    "            Make sure to not repeat any existing thoughts too closely.\n",
    "            ''',\n",
    "            input=dict(\n",
    "                context=dict(\n",
    "                    previous_passage=previous_passage,\n",
    "                    recent_thoughts=recent_thoughts,\n",
    "                    recalled_thoughts=recalled_thoughts,\n",
    "                ),\n",
    "                passage=passage,\n",
    "            ),\n",
    "            output=[Thought],\n",
    "        )\n",
    "\n",
    "        thoughts = [Thought(**{**dict(thought), 'source': paper}) for thought in thoughts.objects]\n",
    "        recent_thoughts = (thoughts + recent_thoughts)[:recent_limit]\n",
    "        previous_passage = passage\n",
    "        \n",
    "        print(f'Generated {len(thoughts)} thoughts')\n",
    "        print([thought.value for thought in thoughts])\n",
    "\n",
    "        store(*thoughts, collection=collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_document(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>body</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>value</th>\n",
       "      <th>category</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>56f0181f-083f-42ef-9e91-f2b0412ac9a6</td>\n",
       "      <td>thought</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'ids': ['633eb8b2-7cd3-4154-86ad-172d76416dd5'], 'collection': 'arxiv', 'limit': 1}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The goal of this work is to develop a theoretical framework for analyzing the effectiveness of visual stories.</td>\n",
       "      <td>fact</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>b8241343-55b3-4690-8e41-1e1b7c978418</td>\n",
       "      <td>thought</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'ids': ['633eb8b2-7cd3-4154-86ad-172d76416dd5'], 'collection': 'arxiv', 'limit': 1}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The theoretical framework includes a new story element called moments.</td>\n",
       "      <td>fact</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>c77e00b8-9a34-4af7-a058-0e16a381e24e</td>\n",
       "      <td>thought</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'ids': ['633eb8b2-7cd3-4154-86ad-172d76416dd5'], 'collection': 'arxiv', 'limit': 1}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Linear stories, like feature films, can be decomposed into a set of moments.</td>\n",
       "      <td>fact</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>79a260ea-90e0-422d-b5a8-503963788cd2</td>\n",
       "      <td>thought</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'ids': ['633eb8b2-7cd3-4154-86ad-172d76416dd5'], 'collection': 'arxiv', 'limit': 1}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The authors believe that moments can be used to analyze the effectiveness of visual stories.</td>\n",
       "      <td>belief</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0bd4905c-1b39-4986-abea-c53b5174a5c8</td>\n",
       "      <td>thought</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'ids': ['633eb8b2-7cd3-4154-86ad-172d76416dd5'], 'collection': 'arxiv', 'limit': 1}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Visual stories include feature films and comic books.</td>\n",
       "      <td>fact</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>de074dee-d67a-49ef-9197-107b51276571</td>\n",
       "      <td>thought</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'ids': ['ab51e267-273b-4329-9d6e-d7c5ab3e8471'], 'collection': 'arxiv', 'limit': 1}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The user specifies a criterion expressed in natural language to guide the image clustering process.</td>\n",
       "      <td>fact</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>16b297de-5051-476a-aa19-892fb5158790</td>\n",
       "      <td>thought</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'ids': ['ab51e267-273b-4329-9d6e-d7c5ab3e8471'], 'collection': 'arxiv', 'limit': 1}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Recent image clustering methods find clusters that agree with pre-defined class labels for datasets such as CIFAR-10.</td>\n",
       "      <td>fact</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0c0422d2-bad3-41a5-8b5e-ac3f8bea1a08</td>\n",
       "      <td>thought</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'ids': ['ab51e267-273b-4329-9d6e-d7c5ab3e8471'], 'collection': 'arxiv', 'limit': 1}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The inductive biases of the neural networks and the loss function, data augmentations, and feature extractors used within the method influence the choice of clusters.</td>\n",
       "      <td>fact</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>9ae91878-be4b-483c-997f-37772120cc75</td>\n",
       "      <td>thought</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'ids': ['ab51e267-273b-4329-9d6e-d7c5ab3e8471'], 'collection': 'arxiv', 'limit': 1}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Classical clustering methods may not be consistent with the relevant criterion that a user has in mind.</td>\n",
       "      <td>opinion</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>30b94e86-7c85-4038-8352-2b55486f3c50</td>\n",
       "      <td>thought</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'ids': ['ab51e267-273b-4329-9d6e-d7c5ab3e8471'], 'collection': 'arxiv', 'limit': 1}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Iterative refinement of text criteria allows the user to choose the text criterion through an iterative process.</td>\n",
       "      <td>fact</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "\n",
       "                                       id     type title abstract  url  body  \\\n",
       "\u001b[1;36m7\u001b[0m    \u001b[93m56f0181f-083f-42ef-9e91-f2b0412ac9a6\u001b[0m  thought   NaN      NaN  NaN   NaN   \n",
       "\u001b[1;36m8\u001b[0m    \u001b[93mb8241343-55b3-4690-8e41-1e1b7c978418\u001b[0m  thought   NaN      NaN  NaN   NaN   \n",
       "\u001b[1;36m9\u001b[0m    \u001b[93mc77e00b8-9a34-4af7-a058-0e16a381e24e\u001b[0m  thought   NaN      NaN  NaN   NaN   \n",
       "\u001b[1;36m10\u001b[0m   \u001b[93m79a260ea-90e0-422d-b5a8-503963788cd2\u001b[0m  thought   NaN      NaN  NaN   NaN   \n",
       "\u001b[1;36m11\u001b[0m   \u001b[93m0bd4905c-1b39-4986-abea-c53b5174a5c8\u001b[0m  thought   NaN      NaN  NaN   NaN   \n",
       "..                                    \u001b[33m...\u001b[0m      \u001b[33m...\u001b[0m   \u001b[33m...\u001b[0m      \u001b[33m...\u001b[0m  \u001b[33m...\u001b[0m   \u001b[33m...\u001b[0m   \n",
       "\u001b[1;36m133\u001b[0m  \u001b[93mde074dee-d67a-49ef-9197-107b51276571\u001b[0m  thought   NaN      NaN  NaN   NaN   \n",
       "\u001b[1;36m134\u001b[0m  \u001b[93m16b297de-5051-476a-aa19-892fb5158790\u001b[0m  thought   NaN      NaN  NaN   NaN   \n",
       "\u001b[1;36m135\u001b[0m  \u001b[93m0c0422d2-bad3-41a5-8b5e-ac3f8bea1a08\u001b[0m  thought   NaN      NaN  NaN   NaN   \n",
       "\u001b[1;36m136\u001b[0m  \u001b[93m9ae91878-be4b-483c-997f-37772120cc75\u001b[0m  thought   NaN      NaN  NaN   NaN   \n",
       "\u001b[1;36m137\u001b[0m  \u001b[93m30b94e86-7c85-4038-8352-2b55486f3c50\u001b[0m  thought   NaN      NaN  NaN   NaN   \n",
       "\n",
       "    text  \\\n",
       "\u001b[1;36m7\u001b[0m    NaN   \n",
       "\u001b[1;36m8\u001b[0m    NaN   \n",
       "\u001b[1;36m9\u001b[0m    NaN   \n",
       "\u001b[1;36m10\u001b[0m   NaN   \n",
       "\u001b[1;36m11\u001b[0m   NaN   \n",
       "..   \u001b[33m...\u001b[0m   \n",
       "\u001b[1;36m133\u001b[0m  NaN   \n",
       "\u001b[1;36m134\u001b[0m  NaN   \n",
       "\u001b[1;36m135\u001b[0m  NaN   \n",
       "\u001b[1;36m136\u001b[0m  NaN   \n",
       "\u001b[1;36m137\u001b[0m  NaN   \n",
       "\n",
       "                                                                                   source  \\\n",
       "\u001b[1;36m7\u001b[0m    \u001b[1m{\u001b[0m\u001b[32m'ids'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'633eb8b2-7cd3-4154-86ad-172d76416dd5'\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'collection'\u001b[0m: \u001b[32m'arxiv'\u001b[0m, \u001b[32m'limit'\u001b[0m: \u001b[1;36m1\u001b[0m\u001b[1m}\u001b[0m   \n",
       "\u001b[1;36m8\u001b[0m    \u001b[1m{\u001b[0m\u001b[32m'ids'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'633eb8b2-7cd3-4154-86ad-172d76416dd5'\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'collection'\u001b[0m: \u001b[32m'arxiv'\u001b[0m, \u001b[32m'limit'\u001b[0m: \u001b[1;36m1\u001b[0m\u001b[1m}\u001b[0m   \n",
       "\u001b[1;36m9\u001b[0m    \u001b[1m{\u001b[0m\u001b[32m'ids'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'633eb8b2-7cd3-4154-86ad-172d76416dd5'\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'collection'\u001b[0m: \u001b[32m'arxiv'\u001b[0m, \u001b[32m'limit'\u001b[0m: \u001b[1;36m1\u001b[0m\u001b[1m}\u001b[0m   \n",
       "\u001b[1;36m10\u001b[0m   \u001b[1m{\u001b[0m\u001b[32m'ids'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'633eb8b2-7cd3-4154-86ad-172d76416dd5'\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'collection'\u001b[0m: \u001b[32m'arxiv'\u001b[0m, \u001b[32m'limit'\u001b[0m: \u001b[1;36m1\u001b[0m\u001b[1m}\u001b[0m   \n",
       "\u001b[1;36m11\u001b[0m   \u001b[1m{\u001b[0m\u001b[32m'ids'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'633eb8b2-7cd3-4154-86ad-172d76416dd5'\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'collection'\u001b[0m: \u001b[32m'arxiv'\u001b[0m, \u001b[32m'limit'\u001b[0m: \u001b[1;36m1\u001b[0m\u001b[1m}\u001b[0m   \n",
       "..                                                                                    \u001b[33m...\u001b[0m   \n",
       "\u001b[1;36m133\u001b[0m  \u001b[1m{\u001b[0m\u001b[32m'ids'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'ab51e267-273b-4329-9d6e-d7c5ab3e8471'\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'collection'\u001b[0m: \u001b[32m'arxiv'\u001b[0m, \u001b[32m'limit'\u001b[0m: \u001b[1;36m1\u001b[0m\u001b[1m}\u001b[0m   \n",
       "\u001b[1;36m134\u001b[0m  \u001b[1m{\u001b[0m\u001b[32m'ids'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'ab51e267-273b-4329-9d6e-d7c5ab3e8471'\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'collection'\u001b[0m: \u001b[32m'arxiv'\u001b[0m, \u001b[32m'limit'\u001b[0m: \u001b[1;36m1\u001b[0m\u001b[1m}\u001b[0m   \n",
       "\u001b[1;36m135\u001b[0m  \u001b[1m{\u001b[0m\u001b[32m'ids'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'ab51e267-273b-4329-9d6e-d7c5ab3e8471'\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'collection'\u001b[0m: \u001b[32m'arxiv'\u001b[0m, \u001b[32m'limit'\u001b[0m: \u001b[1;36m1\u001b[0m\u001b[1m}\u001b[0m   \n",
       "\u001b[1;36m136\u001b[0m  \u001b[1m{\u001b[0m\u001b[32m'ids'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'ab51e267-273b-4329-9d6e-d7c5ab3e8471'\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'collection'\u001b[0m: \u001b[32m'arxiv'\u001b[0m, \u001b[32m'limit'\u001b[0m: \u001b[1;36m1\u001b[0m\u001b[1m}\u001b[0m   \n",
       "\u001b[1;36m137\u001b[0m  \u001b[1m{\u001b[0m\u001b[32m'ids'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'ab51e267-273b-4329-9d6e-d7c5ab3e8471'\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'collection'\u001b[0m: \u001b[32m'arxiv'\u001b[0m, \u001b[32m'limit'\u001b[0m: \u001b[1;36m1\u001b[0m\u001b[1m}\u001b[0m   \n",
       "\n",
       "     start  end  \\\n",
       "\u001b[1;36m7\u001b[0m      NaN  NaN   \n",
       "\u001b[1;36m8\u001b[0m      NaN  NaN   \n",
       "\u001b[1;36m9\u001b[0m      NaN  NaN   \n",
       "\u001b[1;36m10\u001b[0m     NaN  NaN   \n",
       "\u001b[1;36m11\u001b[0m     NaN  NaN   \n",
       "..     \u001b[33m...\u001b[0m  \u001b[33m...\u001b[0m   \n",
       "\u001b[1;36m133\u001b[0m    NaN  NaN   \n",
       "\u001b[1;36m134\u001b[0m    NaN  NaN   \n",
       "\u001b[1;36m135\u001b[0m    NaN  NaN   \n",
       "\u001b[1;36m136\u001b[0m    NaN  NaN   \n",
       "\u001b[1;36m137\u001b[0m    NaN  NaN   \n",
       "\n",
       "                                                                                                                                                                      value  \\\n",
       "\u001b[1;36m7\u001b[0m                                                            The goal of this work is to develop a theoretical framework for analyzing the effectiveness of visual stories.\n",
       "\u001b[1;36m8\u001b[0m                                                                                                    The theoretical framework includes a new story element called moments.\n",
       "\u001b[1;36m9\u001b[0m                                                                                              Linear stories, like feature films, can be decomposed into a set of moments.\n",
       "\u001b[1;36m10\u001b[0m                                                                             The authors believe that moments can be used to analyze the effectiveness of visual stories.\n",
       "\u001b[1;36m11\u001b[0m                                                                                                                    Visual stories include feature films and comic books.\n",
       "..                                                                                                                                                                      \u001b[33m...\u001b[0m\n",
       "\u001b[1;36m133\u001b[0m                                                                     The user specifies a criterion expressed in natural language to guide the image clustering process.\n",
       "\u001b[1;36m134\u001b[0m                                                   Recent image clustering methods find clusters that agree with pre-defined class labels for datasets such as CIFAR-\u001b[1;36m10\u001b[0m.\n",
       "\u001b[1;36m135\u001b[0m  The inductive biases of the neural networks and the loss function, data augmentations, and feature extractors used within the method influence the choice of clusters.\n",
       "\u001b[1;36m136\u001b[0m                                                                 Classical clustering methods may not be consistent with the relevant criterion that a user has in mind.\n",
       "\u001b[1;36m137\u001b[0m                                                        Iterative refinement of text criteria allows the user to choose the text criterion through an iterative process.\n",
       "\n",
       "    category  confidence  \n",
       "\u001b[1;36m7\u001b[0m       fact         \u001b[1;36m0.9\u001b[0m  \n",
       "\u001b[1;36m8\u001b[0m       fact         \u001b[1;36m0.8\u001b[0m  \n",
       "\u001b[1;36m9\u001b[0m       fact         \u001b[1;36m0.7\u001b[0m  \n",
       "\u001b[1;36m10\u001b[0m    belief         \u001b[1;36m0.6\u001b[0m  \n",
       "\u001b[1;36m11\u001b[0m      fact         \u001b[1;36m0.8\u001b[0m  \n",
       "..       \u001b[33m...\u001b[0m         \u001b[33m...\u001b[0m  \n",
       "\u001b[1;36m133\u001b[0m     fact         \u001b[1;36m0.7\u001b[0m  \n",
       "\u001b[1;36m134\u001b[0m     fact         \u001b[1;36m0.6\u001b[0m  \n",
       "\u001b[1;36m135\u001b[0m     fact         \u001b[1;36m0.6\u001b[0m  \n",
       "\u001b[1;36m136\u001b[0m  opinion         \u001b[1;36m0.7\u001b[0m  \n",
       "\u001b[1;36m137\u001b[0m     fact         \u001b[1;36m0.6\u001b[0m  \n",
       "\n",
       "\u001b[1m[\u001b[0m\u001b[1;36m131\u001b[0m rows x \u001b[1;36m13\u001b[0m columns\u001b[1m]\u001b[0m"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "thoughts = query(collection=collection_name).query('type == \"thought\"')\n",
    "thoughts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv-reader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
