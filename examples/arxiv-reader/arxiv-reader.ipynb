{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import requests\n",
    "import uuid\n",
    "\n",
    "def load_pdf(filepath_or_url):\n",
    "    \"\"\"\n",
    "    Load content of a PDF from either a file path or a remote URL.\n",
    "    \n",
    "    :param filepath_or_url: File path or URL to fetch the PDF from.\n",
    "    :return: Content of the PDF as a string.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Handle remote URL\n",
    "    if filepath_or_url.startswith((\"http://\", \"https://\")):\n",
    "        response = requests.get(filepath_or_url)\n",
    "        response.raise_for_status()\n",
    "        id = str(uuid.uuid4())\n",
    "        filepath_or_url = f'./data/{id}.pdf'\n",
    "        with open(filepath_or_url, 'wb') as pdf:\n",
    "            pdf.write(response.content)\n",
    "    \n",
    "    with open(filepath_or_url, 'rb') as f:\n",
    "        pdf_reader = PyPDF2.PdfReader(f)\n",
    "        text_content = ''.join([page.extract_text() for page in pdf_reader.pages])\n",
    "    return text_content\n",
    "\n",
    "\n",
    "def batch(generator, bs=1, limit=None):\n",
    "    b = []\n",
    "    i = 0\n",
    "    for item in generator:\n",
    "        if limit and i > limit:\n",
    "            break\n",
    "        b.append(item)\n",
    "        if len(b) == bs:\n",
    "            yield b\n",
    "            b = []\n",
    "        i += bs\n",
    "    if b and (limit and i <= limit):  # Yield any remaining items in the batch\n",
    "        yield b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-11-02 00:13:02.600\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpromptx\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mloading local app from /home/rjl/promptx/examples/arxiv-reader\u001b[0m\n",
      "\u001b[32m2023-11-02 00:13:02.603\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpromptx\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mloaded environment variables from /home/rjl/promptx/examples/arxiv-reader/.env\u001b[0m\n",
      "\u001b[32m2023-11-02 00:13:02.604\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpromptx\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mAPI KEY wMeGC\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from typing import *\n",
    "from pydantic import Field\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from promptx.collection import Entity\n",
    "\n",
    "\n",
    "class Document(Entity):\n",
    "    title: str\n",
    "    abstract: str\n",
    "    url: str\n",
    "\n",
    "\n",
    "def get_arxiv_urls():\n",
    "    response = requests.get('https://arxiv.org/list/cs.AI/recent')\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    urls = [f\"https://arxiv.org{a.attrs['href']}\" for a in soup.find_all('a', title='Abstract')]\n",
    "    return urls\n",
    "\n",
    "\n",
    "def extract_whitepaper_from_arxiv(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    title = soup.find('h1', class_='title').text.replace('Title:', '')\n",
    "    abstract = soup.find('blockquote', class_='abstract').text.replace('Abstract:', '')\n",
    "    url = soup.find('a', class_='download-pdf').attrs['href']\n",
    "    url = f\"https://arxiv.org{url}\"\n",
    "\n",
    "    return Document(\n",
    "        title=title,\n",
    "        abstract=abstract,\n",
    "        url=url,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pprint import pprint\n",
    "\n",
    "try:\n",
    "    urls = get_arxiv_urls()\n",
    "    url = random.choice(urls)\n",
    "    paper = extract_whitepaper_from_arxiv(url)\n",
    "except Exception as e:\n",
    "    print(f'Error loading {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Image Clustering Conditioned on Text Criteria</td>\n",
       "      <td>\\nClassical clustering methods do not provide ...</td>\n",
       "      <td>https://arxiv.org/pdf/2310.18297.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Learning to Search Feasible and Infeasible Reg...</td>\n",
       "      <td>\\n  In this paper, we present Neural k-Opt (Ne...</td>\n",
       "      <td>https://arxiv.org/pdf/2310.18264.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Innovation-to-Occupations Ontology: Linkin...</td>\n",
       "      <td>\\n  The fast adoption of new technologies forc...</td>\n",
       "      <td>https://arxiv.org/pdf/2310.17909.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is Scaling Learned Optimizers Worth It? Evalua...</td>\n",
       "      <td>\\n  We analyze VeLO (versatile learned optimiz...</td>\n",
       "      <td>https://arxiv.org/pdf/2310.18191.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Moments for Perceptive Narration Analysis Thro...</td>\n",
       "      <td>\\nIn this work, our goal is to develop a theor...</td>\n",
       "      <td>https://arxiv.org/pdf/2310.18273.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>739 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "\n",
       "                                                 title  \\\n",
       "\u001b[1;36m0\u001b[0m        Image Clustering Conditioned on Text Criteria   \n",
       "\u001b[1;36m1\u001b[0m    Learning to Search Feasible and Infeasible Reg\u001b[33m...\u001b[0m   \n",
       "\u001b[1;36m2\u001b[0m    The Innovation-to-Occupations Ontology: Linkin\u001b[33m...\u001b[0m   \n",
       "\u001b[1;36m3\u001b[0m    Is Scaling Learned Optimizers Worth It? Evalua\u001b[33m...\u001b[0m   \n",
       "\u001b[1;36m4\u001b[0m    Moments for Perceptive Narration Analysis Thro\u001b[33m...\u001b[0m   \n",
       "..                                                 \u001b[33m...\u001b[0m   \n",
       "\u001b[1;36m734\u001b[0m                                                NaN   \n",
       "\u001b[1;36m735\u001b[0m                                                NaN   \n",
       "\u001b[1;36m736\u001b[0m                                                NaN   \n",
       "\u001b[1;36m737\u001b[0m                                                NaN   \n",
       "\u001b[1;36m738\u001b[0m                                                NaN   \n",
       "\n",
       "                                              abstract  \\\n",
       "\u001b[1;36m0\u001b[0m    \\nClassical clustering methods do not provide \u001b[33m...\u001b[0m   \n",
       "\u001b[1;36m1\u001b[0m    \\n  In this paper, we present Neural k-Opt \u001b[1m(\u001b[0mNe\u001b[33m...\u001b[0m   \n",
       "\u001b[1;36m2\u001b[0m    \\n  The fast adoption of new technologies forc\u001b[33m...\u001b[0m   \n",
       "\u001b[1;36m3\u001b[0m    \\n  We analyze VeLO \u001b[1m(\u001b[0mversatile learned optimiz\u001b[33m...\u001b[0m   \n",
       "\u001b[1;36m4\u001b[0m    \\nIn this work, our goal is to develop a theor\u001b[33m...\u001b[0m   \n",
       "..                                                 \u001b[33m...\u001b[0m   \n",
       "\u001b[1;36m734\u001b[0m                                                NaN   \n",
       "\u001b[1;36m735\u001b[0m                                                NaN   \n",
       "\u001b[1;36m736\u001b[0m                                                NaN   \n",
       "\u001b[1;36m737\u001b[0m                                                NaN   \n",
       "\u001b[1;36m738\u001b[0m                                                NaN   \n",
       "\n",
       "                                      url  \n",
       "\u001b[1;36m0\u001b[0m    \u001b[4;94mhttps://arxiv.org/pdf/2310.18297.pdf\u001b[0m  \n",
       "\u001b[1;36m1\u001b[0m    \u001b[4;94mhttps://arxiv.org/pdf/2310.18264.pdf\u001b[0m  \n",
       "\u001b[1;36m2\u001b[0m    \u001b[4;94mhttps://arxiv.org/pdf/2310.17909.pdf\u001b[0m  \n",
       "\u001b[1;36m3\u001b[0m    \u001b[4;94mhttps://arxiv.org/pdf/2310.18191.pdf\u001b[0m  \n",
       "\u001b[1;36m4\u001b[0m    \u001b[4;94mhttps://arxiv.org/pdf/2310.18273.pdf\u001b[0m  \n",
       "..                                    \u001b[33m...\u001b[0m  \n",
       "\u001b[1;36m734\u001b[0m                                   NaN  \n",
       "\u001b[1;36m735\u001b[0m                                   NaN  \n",
       "\u001b[1;36m736\u001b[0m                                   NaN  \n",
       "\u001b[1;36m737\u001b[0m                                   NaN  \n",
       "\u001b[1;36m738\u001b[0m                                   NaN  \n",
       "\n",
       "\u001b[1m[\u001b[0m\u001b[1;36m739\u001b[0m rows x \u001b[1;36m3\u001b[0m columns\u001b[1m]\u001b[0m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from promptx import store, query\n",
    "\n",
    "collection_name = 'arxiv'\n",
    "store(paper, collection=collection_name)\n",
    "query(collection=collection_name)[['title', 'abstract', 'url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m'90bd06ba-604a-4d00-8c0d-422278338ff4'\u001b[0m,\n",
       "    \u001b[33mtype\u001b[0m=\u001b[32m'document'\u001b[0m,\n",
       "    \u001b[33mtitle\u001b[0m=\u001b[32m\"Is\u001b[0m\u001b[32m Scaling Learned Optimizers Worth It? Evaluating The Value of VeLO's 4000 TPU Months\"\u001b[0m,\n",
       "    \u001b[33mabstract\u001b[0m=\u001b[32m'\\n  We analyze VeLO \u001b[0m\u001b[32m(\u001b[0m\u001b[32mversatile learned optimizer\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, the largest scale attempt to\\ntrain a general purpose \"foundational\" optimizer to date. VeLO was trained on\\nthousands of machine learning tasks using over 4000 TPU months with the goal of\\nproducing an optimizer capable of generalizing to new problems while being\\nhyperparameter free, and outperforming industry standards such as Adam. We\\nindependently evaluate VeLO on the MLCommons optimizer benchmark suite. We find\\nthat, contrary to initial claims: \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m VeLO has a critical hyperparameter that\\nneeds problem-specific tuning, \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m VeLO does not necessarily outperform\\ncompetitors in quality of solution found, and \u001b[0m\u001b[32m(\u001b[0m\u001b[32m3\u001b[0m\u001b[32m)\u001b[0m\u001b[32m VeLO is not faster than\\ncompeting optimizers at reducing the training loss. These observations call\\ninto question VeLO\\'s generality and the value of the investment in training it.\\n\\n    '\u001b[0m,\n",
       "    \u001b[33murl\u001b[0m=\u001b[32m'https://arxiv.org/pdf/2310.18191.pdf'\u001b[0m,\n",
       "    \u001b[33mbody\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from promptx import query\n",
    "\n",
    "paper = query(collection=collection_name).query('type == \"document\"').sample().first\n",
    "paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pdf with 46195 characters\n"
     ]
    }
   ],
   "source": [
    "pdf = load_pdf(paper.url)\n",
    "print(f'Loaded pdf with {len(pdf)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>body</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>value</th>\n",
       "      <th>category</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>e59b876a-76f3-4554-8bf0-b9f2577f7ad2</td>\n",
       "      <td>quote</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Under Review\\nIMAGE CLUSTERING CONDITIONED ON ...</td>\n",
       "      <td>{'ids': ['ab51e267-273b-4329-9d6e-d7c5ab3e8471...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>449.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>5c85156c-d763-485f-8e2a-44b41948d0bc</td>\n",
       "      <td>quote</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In this work, we present a new methodology for...</td>\n",
       "      <td>{'ids': ['ab51e267-273b-4329-9d6e-d7c5ab3e8471...</td>\n",
       "      <td>450.0</td>\n",
       "      <td>633.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>3c372465-b53f-4b08-a071-607d1c54bf5a</td>\n",
       "      <td>quote</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We call our method\\nImage Clustering Condition...</td>\n",
       "      <td>{'ids': ['ab51e267-273b-4329-9d6e-d7c5ab3e8471...</td>\n",
       "      <td>634.0</td>\n",
       "      <td>769.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>a7472a5e-3c94-4870-80b8-6ddf37e93d50</td>\n",
       "      <td>quote</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IC |TC requires a minimal and practical degree...</td>\n",
       "      <td>{'ids': ['ab51e267-273b-4329-9d6e-d7c5ab3e8471...</td>\n",
       "      <td>770.0</td>\n",
       "      <td>917.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>c86deabc-0ec4-4b6b-9c28-c884df333017</td>\n",
       "      <td>quote</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our experiments show that IC |TC can effective...</td>\n",
       "      <td>{'ids': ['ab51e267-273b-4329-9d6e-d7c5ab3e8471...</td>\n",
       "      <td>918.0</td>\n",
       "      <td>1482.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>0ee98c30-6423-4eb4-a827-433c0018f287</td>\n",
       "      <td>quote</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In summary, for individual benchmarks wand alg...</td>\n",
       "      <td>{'ids': ['90bd06ba-604a-4d00-8c0d-422278338ff4...</td>\n",
       "      <td>14811.0</td>\n",
       "      <td>14899.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>60d98342-05fb-4894-ba9e-8775ca4439d4</td>\n",
       "      <td>quote</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We measure\\nboth wall-clock-time to target (de...</td>\n",
       "      <td>{'ids': ['90bd06ba-604a-4d00-8c0d-422278338ff4...</td>\n",
       "      <td>14900.0</td>\n",
       "      <td>15022.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>89aa8c38-d067-4f2b-9d4e-607bd6f9c497</td>\n",
       "      <td>quote</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>To aggregate across benchmarks we report the a...</td>\n",
       "      <td>{'ids': ['90bd06ba-604a-4d00-8c0d-422278338ff4...</td>\n",
       "      <td>15022.0</td>\n",
       "      <td>15099.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>ea06325d-312d-4209-8228-e31e0bbd3e61</td>\n",
       "      <td>quote</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Measuring Training Quality While MLCommons\\nma...</td>\n",
       "      <td>{'ids': ['90bd06ba-604a-4d00-8c0d-422278338ff4...</td>\n",
       "      <td>15099.0</td>\n",
       "      <td>15269.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>24606539-1afc-463e-adff-e0fc92b3c685</td>\n",
       "      <td>quote</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>To this end, we also assess training\\nand vali...</td>\n",
       "      <td>{'ids': ['90bd06ba-604a-4d00-8c0d-422278338ff4...</td>\n",
       "      <td>15270.0</td>\n",
       "      <td>15458.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "\n",
       "                                       id   type title abstract  url  body  \\\n",
       "\u001b[1;36m139\u001b[0m  \u001b[93me59b876a-76f3-4554-8bf0-b9f2577f7ad2\u001b[0m  quote   NaN      NaN  NaN   NaN   \n",
       "\u001b[1;36m140\u001b[0m  \u001b[93m5c85156c-d763-485f-8e2a-44b41948d0bc\u001b[0m  quote   NaN      NaN  NaN   NaN   \n",
       "\u001b[1;36m141\u001b[0m  \u001b[93m3c372465-b53f-4b08-a071-607d1c54bf5a\u001b[0m  quote   NaN      NaN  NaN   NaN   \n",
       "\u001b[1;36m142\u001b[0m  \u001b[93ma7472a5e-3c94-4870-80b8-6ddf37e93d50\u001b[0m  quote   NaN      NaN  NaN   NaN   \n",
       "\u001b[1;36m143\u001b[0m  \u001b[93mc86deabc-0ec4-4b6b-9c28-c884df333017\u001b[0m  quote   NaN      NaN  NaN   NaN   \n",
       "..                                    \u001b[33m...\u001b[0m    \u001b[33m...\u001b[0m   \u001b[33m...\u001b[0m      \u001b[33m...\u001b[0m  \u001b[33m...\u001b[0m   \u001b[33m...\u001b[0m   \n",
       "\u001b[1;36m834\u001b[0m  \u001b[93m0ee98c30-6423-4eb4-a827-433c0018f287\u001b[0m  quote   NaN      NaN  NaN   NaN   \n",
       "\u001b[1;36m835\u001b[0m  \u001b[93m60d98342-05fb-4894-ba9e-8775ca4439d4\u001b[0m  quote   NaN      NaN  NaN   NaN   \n",
       "\u001b[1;36m836\u001b[0m  \u001b[93m89aa8c38-d067-4f2b-9d4e-607bd6f9c497\u001b[0m  quote   NaN      NaN  NaN   NaN   \n",
       "\u001b[1;36m837\u001b[0m  \u001b[93mea06325d-312d-4209-8228-e31e0bbd3e61\u001b[0m  quote   NaN      NaN  NaN   NaN   \n",
       "\u001b[1;36m838\u001b[0m  \u001b[93m24606539-1afc-463e-adff-e0fc92b3c685\u001b[0m  quote   NaN      NaN  NaN   NaN   \n",
       "\n",
       "                                                  text  \\\n",
       "\u001b[1;36m139\u001b[0m  Under Review\\nIMAGE CLUSTERING CONDITIONED ON \u001b[33m...\u001b[0m   \n",
       "\u001b[1;36m140\u001b[0m  In this work, we present a new methodology for\u001b[33m...\u001b[0m   \n",
       "\u001b[1;36m141\u001b[0m  We call our method\\nImage Clustering Condition\u001b[33m...\u001b[0m   \n",
       "\u001b[1;36m142\u001b[0m  IC |TC requires a minimal and practical degree\u001b[33m...\u001b[0m   \n",
       "\u001b[1;36m143\u001b[0m  Our experiments show that IC |TC can effective\u001b[33m...\u001b[0m   \n",
       "..                                                 \u001b[33m...\u001b[0m   \n",
       "\u001b[1;36m834\u001b[0m  In summary, for individual benchmarks wand alg\u001b[33m...\u001b[0m   \n",
       "\u001b[1;36m835\u001b[0m  We measure\\nboth wall-clock-time to target \u001b[1m(\u001b[0mde\u001b[33m...\u001b[0m   \n",
       "\u001b[1;36m836\u001b[0m  To aggregate across benchmarks we report the a\u001b[33m...\u001b[0m   \n",
       "\u001b[1;36m837\u001b[0m  Measuring Training Quality While MLCommons\\nma\u001b[33m...\u001b[0m   \n",
       "\u001b[1;36m838\u001b[0m  To this end, we also assess training\\nand vali\u001b[33m...\u001b[0m   \n",
       "\n",
       "                                                source    start      end  \\\n",
       "\u001b[1;36m139\u001b[0m  \u001b[1m{\u001b[0m\u001b[32m'ids'\u001b[0m: \u001b[1m[\u001b[0m'\u001b[93mab51e267-273b-4329-9d6e-d7c5ab3e8471\u001b[0m\u001b[33m...\u001b[0m      \u001b[1;36m0.0\u001b[0m    \u001b[1;36m449.0\u001b[0m   \n",
       "\u001b[1;36m140\u001b[0m  \u001b[1m{\u001b[0m\u001b[32m'ids'\u001b[0m: \u001b[1m[\u001b[0m'\u001b[93mab51e267-273b-4329-9d6e-d7c5ab3e8471\u001b[0m\u001b[33m...\u001b[0m    \u001b[1;36m450.0\u001b[0m    \u001b[1;36m633.0\u001b[0m   \n",
       "\u001b[1;36m141\u001b[0m  \u001b[1m{\u001b[0m\u001b[32m'ids'\u001b[0m: \u001b[1m[\u001b[0m'\u001b[93mab51e267-273b-4329-9d6e-d7c5ab3e8471\u001b[0m\u001b[33m...\u001b[0m    \u001b[1;36m634.0\u001b[0m    \u001b[1;36m769.0\u001b[0m   \n",
       "\u001b[1;36m142\u001b[0m  \u001b[1m{\u001b[0m\u001b[32m'ids'\u001b[0m: \u001b[1m[\u001b[0m'\u001b[93mab51e267-273b-4329-9d6e-d7c5ab3e8471\u001b[0m\u001b[33m...\u001b[0m    \u001b[1;36m770.0\u001b[0m    \u001b[1;36m917.0\u001b[0m   \n",
       "\u001b[1;36m143\u001b[0m  \u001b[1m{\u001b[0m\u001b[32m'ids'\u001b[0m: \u001b[1m[\u001b[0m'\u001b[93mab51e267-273b-4329-9d6e-d7c5ab3e8471\u001b[0m\u001b[33m...\u001b[0m    \u001b[1;36m918.0\u001b[0m   \u001b[1;36m1482.0\u001b[0m   \n",
       "..                                                 \u001b[33m...\u001b[0m      \u001b[33m...\u001b[0m      \u001b[33m...\u001b[0m   \n",
       "\u001b[1;36m834\u001b[0m  \u001b[1m{\u001b[0m\u001b[32m'ids'\u001b[0m: \u001b[1m[\u001b[0m'\u001b[93m90bd06ba-604a-4d00-8c0d-422278338ff4\u001b[0m\u001b[33m...\u001b[0m  \u001b[1;36m14811.0\u001b[0m  \u001b[1;36m14899.0\u001b[0m   \n",
       "\u001b[1;36m835\u001b[0m  \u001b[1m{\u001b[0m\u001b[32m'ids'\u001b[0m: \u001b[1m[\u001b[0m'\u001b[93m90bd06ba-604a-4d00-8c0d-422278338ff4\u001b[0m\u001b[33m...\u001b[0m  \u001b[1;36m14900.0\u001b[0m  \u001b[1;36m15022.0\u001b[0m   \n",
       "\u001b[1;36m836\u001b[0m  \u001b[1m{\u001b[0m\u001b[32m'ids'\u001b[0m: \u001b[1m[\u001b[0m'\u001b[93m90bd06ba-604a-4d00-8c0d-422278338ff4\u001b[0m\u001b[33m...\u001b[0m  \u001b[1;36m15022.0\u001b[0m  \u001b[1;36m15099.0\u001b[0m   \n",
       "\u001b[1;36m837\u001b[0m  \u001b[1m{\u001b[0m\u001b[32m'ids'\u001b[0m: \u001b[1m[\u001b[0m'\u001b[93m90bd06ba-604a-4d00-8c0d-422278338ff4\u001b[0m\u001b[33m...\u001b[0m  \u001b[1;36m15099.0\u001b[0m  \u001b[1;36m15269.0\u001b[0m   \n",
       "\u001b[1;36m838\u001b[0m  \u001b[1m{\u001b[0m\u001b[32m'ids'\u001b[0m: \u001b[1m[\u001b[0m'\u001b[93m90bd06ba-604a-4d00-8c0d-422278338ff4\u001b[0m\u001b[33m...\u001b[0m  \u001b[1;36m15270.0\u001b[0m  \u001b[1;36m15458.0\u001b[0m   \n",
       "\n",
       "    value category  confidence  \n",
       "\u001b[1;36m139\u001b[0m   NaN      NaN         NaN  \n",
       "\u001b[1;36m140\u001b[0m   NaN      NaN         NaN  \n",
       "\u001b[1;36m141\u001b[0m   NaN      NaN         NaN  \n",
       "\u001b[1;36m142\u001b[0m   NaN      NaN         NaN  \n",
       "\u001b[1;36m143\u001b[0m   NaN      NaN         NaN  \n",
       "..    \u001b[33m...\u001b[0m      \u001b[33m...\u001b[0m         \u001b[33m...\u001b[0m  \n",
       "\u001b[1;36m834\u001b[0m   NaN      NaN         NaN  \n",
       "\u001b[1;36m835\u001b[0m   NaN      NaN         NaN  \n",
       "\u001b[1;36m836\u001b[0m   NaN      NaN         NaN  \n",
       "\u001b[1;36m837\u001b[0m   NaN      NaN         NaN  \n",
       "\u001b[1;36m838\u001b[0m   NaN      NaN         NaN  \n",
       "\n",
       "\u001b[1m[\u001b[0m\u001b[1;36m700\u001b[0m rows x \u001b[1;36m13\u001b[0m columns\u001b[1m]\u001b[0m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from promptx import store, query\n",
    "\n",
    "class Quote(Entity):\n",
    "    text: str\n",
    "    source: Document\n",
    "    start: int\n",
    "    end: int\n",
    "\n",
    "for chunk in batch(doc.sents, bs=10, limit=1000):\n",
    "    store(\n",
    "        *[\n",
    "            Quote(\n",
    "                text=sentence.text,\n",
    "                source=paper,\n",
    "                start=sentence.start_char,\n",
    "                end=sentence.end_char,\n",
    "            ) \n",
    "            for sentence in chunk\n",
    "        ], \n",
    "        collection=collection_name\n",
    "    )\n",
    "\n",
    "query(collection=collection_name).query('type == \"quote\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class ThoughtCategory(str, Enum):\n",
    "    fact = 'fact'\n",
    "    opinion = 'opinion'\n",
    "    idea = 'idea'\n",
    "    connection = 'connection'\n",
    "    belief = 'belief'\n",
    "\n",
    "\n",
    "class Thought(Entity):\n",
    "    value: str\n",
    "    category: ThoughtCategory\n",
    "    confidence: float\n",
    "    source: Entity = Field(None, generate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_document(doc, bs=5, limit=1000, recall_limit=3, recent_limit=5):\n",
    "    sentences = doc.sents\n",
    "    recent_thoughts = []\n",
    "    previous_passage = None\n",
    "    for chunk in batch(sentences, bs=bs, limit=limit):\n",
    "        passage = [sentence.text for sentence in chunk]\n",
    "        recalled_thoughts = query(*passage, collection=collection_name, limit=recall_limit).query('type == \"thought\"').objects\n",
    "        \n",
    "        thoughts = prompt(\n",
    "            '''\n",
    "            Given a passage of text and some context, generate some new thoughts about the text.\n",
    "            Make sure to not repeat any existing thoughts too closely.\n",
    "            ''',\n",
    "            input=dict(\n",
    "                context=dict(\n",
    "                    previous_passage=previous_passage,\n",
    "                    recent_thoughts=recent_thoughts,\n",
    "                    recalled_thoughts=recalled_thoughts,\n",
    "                ),\n",
    "                passage=passage,\n",
    "            ),\n",
    "            output=[Thought],\n",
    "        )\n",
    "\n",
    "        thoughts = [Thought(**{**dict(thought), 'source': paper}) for thought in thoughts.objects]\n",
    "        recent_thoughts = (thoughts + recent_thoughts)[:recent_limit]\n",
    "        previous_passage = passage\n",
    "        \n",
    "        print(f'Generated {len(thoughts)} thoughts')\n",
    "        print([thought.value for thought in thoughts])\n",
    "\n",
    "        store(*thoughts, collection=collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TextInputSequence must be str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/rjl/promptx/examples/arxiv-reader/arxiv-reader.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/rjl/promptx/examples/arxiv-reader/arxiv-reader.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m read_document(doc)\n",
      "\u001b[1;32m/home/rjl/promptx/examples/arxiv-reader/arxiv-reader.ipynb Cell 11\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/rjl/promptx/examples/arxiv-reader/arxiv-reader.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m batch(sentences, bs\u001b[39m=\u001b[39mbs, limit\u001b[39m=\u001b[39mlimit):\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/rjl/promptx/examples/arxiv-reader/arxiv-reader.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     passage \u001b[39m=\u001b[39m [sentence\u001b[39m.\u001b[39mtext \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m chunk]\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/rjl/promptx/examples/arxiv-reader/arxiv-reader.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     recalled_thoughts \u001b[39m=\u001b[39m query(passage, collection\u001b[39m=\u001b[39;49mcollection_name, limit\u001b[39m=\u001b[39;49mrecall_limit)\u001b[39m.\u001b[39mquery(\u001b[39m'\u001b[39m\u001b[39mtype == \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthought\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mobjects\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/rjl/promptx/examples/arxiv-reader/arxiv-reader.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     thoughts \u001b[39m=\u001b[39m prompt(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/rjl/promptx/examples/arxiv-reader/arxiv-reader.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m        \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/rjl/promptx/examples/arxiv-reader/arxiv-reader.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m        Given a passage of text and some context, generate some new thoughts about the text.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/rjl/promptx/examples/arxiv-reader/arxiv-reader.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m         output\u001b[39m=\u001b[39m[Thought],\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/rjl/promptx/examples/arxiv-reader/arxiv-reader.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/rjl/promptx/examples/arxiv-reader/arxiv-reader.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     thoughts \u001b[39m=\u001b[39m [Thought(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mdict\u001b[39m(thought), \u001b[39m'\u001b[39m\u001b[39msource\u001b[39m\u001b[39m'\u001b[39m: paper}) \u001b[39mfor\u001b[39;00m thought \u001b[39min\u001b[39;00m thoughts\u001b[39m.\u001b[39mobjects]\n",
      "File \u001b[0;32m~/promptx/promptx/__init__.py:54\u001b[0m, in \u001b[0;36mquery\u001b[0;34m(field, where, collection, *texts, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mquery\u001b[39m(\u001b[39m*\u001b[39mtexts, field\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, where\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, collection\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Collection:\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m DEFAULT_SESSION\u001b[39m.\u001b[39;49mquery(\n\u001b[1;32m     55\u001b[0m         \u001b[39m*\u001b[39;49mtexts, field\u001b[39m=\u001b[39;49mfield, where\u001b[39m=\u001b[39;49mwhere, collection\u001b[39m=\u001b[39;49mcollection, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/promptx/promptx/world.py:112\u001b[0m, in \u001b[0;36mSession.query\u001b[0;34m(self, field, ids, where, collection, limit, *texts)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39mif\u001b[39;00m field \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     where[\u001b[39m'\u001b[39m\u001b[39mfield\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m field\n\u001b[0;32m--> 112\u001b[0m r \u001b[39m=\u001b[39m  c(\u001b[39m*\u001b[39;49mtexts, ids\u001b[39m=\u001b[39;49mids, where\u001b[39m=\u001b[39;49mwhere, limit\u001b[39m=\u001b[39;49mlimit)\n\u001b[1;32m    113\u001b[0m \u001b[39mif\u001b[39;00m r \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/promptx/promptx/collection.py:711\u001b[0m, in \u001b[0;36mCollection.__call__\u001b[0;34m(self, where, *texts, **kwargs)\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mtexts, where\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 711\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding_query(\u001b[39m*\u001b[39;49mtexts, where\u001b[39m=\u001b[39;49mwhere, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/promptx/promptx/collection.py:687\u001b[0m, in \u001b[0;36mCollection.embedding_query\u001b[0;34m(self, ids, where, threshold, limit, *texts, **kwargs)\u001b[0m\n\u001b[1;32m    685\u001b[0m             scores[\u001b[39mid\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    686\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 687\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdb\u001b[39m.\u001b[39;49mquery(query_texts\u001b[39m=\u001b[39;49mtexts, where\u001b[39m=\u001b[39;49mwhere, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    688\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(results[\u001b[39m'\u001b[39m\u001b[39mids\u001b[39m\u001b[39m'\u001b[39m])):\n\u001b[1;32m    689\u001b[0m         \u001b[39mfor\u001b[39;00m \u001b[39mid\u001b[39m, d, m \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(results[\u001b[39m'\u001b[39m\u001b[39mids\u001b[39m\u001b[39m'\u001b[39m][i], results[\u001b[39m'\u001b[39m\u001b[39mdistances\u001b[39m\u001b[39m'\u001b[39m][i], results[\u001b[39m'\u001b[39m\u001b[39mmetadatas\u001b[39m\u001b[39m'\u001b[39m][i]):\n",
      "File \u001b[0;32m~/promptx/examples/arxiv-reader/venv-arxiv-reader/lib/python3.10/site-packages/chromadb/api/models/Collection.py:211\u001b[0m, in \u001b[0;36mCollection.query\u001b[0;34m(self, query_embeddings, query_texts, n_results, where, where_document, include)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou must provide embeddings or a function to compute them\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     \u001b[39m# We know query texts is not None at this point, cast for the typechecker\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m     query_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_embedding_function(\n\u001b[1;32m    212\u001b[0m         cast(List[Document], query_texts)\n\u001b[1;32m    213\u001b[0m     )\n\u001b[1;32m    215\u001b[0m \u001b[39mif\u001b[39;00m where \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     where \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/promptx/examples/arxiv-reader/venv-arxiv-reader/lib/python3.10/site-packages/chromadb/utils/embedding_functions.py:381\u001b[0m, in \u001b[0;36mONNXMiniLM_L6_V2.__call__\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download_model_if_not_exists()\n\u001b[1;32m    380\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_model_and_tokenizer()\n\u001b[0;32m--> 381\u001b[0m res \u001b[39m=\u001b[39m cast(Embeddings, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(texts)\u001b[39m.\u001b[39mtolist())\n\u001b[1;32m    382\u001b[0m \u001b[39mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/promptx/examples/arxiv-reader/venv-arxiv-reader/lib/python3.10/site-packages/chromadb/utils/embedding_functions.py:319\u001b[0m, in \u001b[0;36mONNXMiniLM_L6_V2._forward\u001b[0;34m(self, documents, batch_size)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(documents), batch_size):\n\u001b[1;32m    318\u001b[0m     batch \u001b[39m=\u001b[39m documents[i : i \u001b[39m+\u001b[39m batch_size]\n\u001b[0;32m--> 319\u001b[0m     encoded \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mencode(d) \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m batch]\n\u001b[1;32m    320\u001b[0m     input_ids \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([e\u001b[39m.\u001b[39mids \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m encoded])\n\u001b[1;32m    321\u001b[0m     attention_mask \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([e\u001b[39m.\u001b[39mattention_mask \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m encoded])\n",
      "File \u001b[0;32m~/promptx/examples/arxiv-reader/venv-arxiv-reader/lib/python3.10/site-packages/chromadb/utils/embedding_functions.py:319\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(documents), batch_size):\n\u001b[1;32m    318\u001b[0m     batch \u001b[39m=\u001b[39m documents[i : i \u001b[39m+\u001b[39m batch_size]\n\u001b[0;32m--> 319\u001b[0m     encoded \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mencode(d) \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m batch]\n\u001b[1;32m    320\u001b[0m     input_ids \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([e\u001b[39m.\u001b[39mids \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m encoded])\n\u001b[1;32m    321\u001b[0m     attention_mask \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([e\u001b[39m.\u001b[39mattention_mask \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m encoded])\n",
      "\u001b[0;31mTypeError\u001b[0m: TextInputSequence must be str"
     ]
    }
   ],
   "source": [
    "read_document(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "thoughts = query(collection=collection_name).query('type == \"thought\"')\n",
    "thoughts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv-reader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
